{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "872c2ed4-a888-4da2-91ec-788f511ffdf1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import mlflow\n",
    "mlflow.set_tracking_uri(\"http://127.0.0.1:5000\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d468d6c-b322-45cc-8dfa-85b4d0aa3e15",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-01-09 21:02:09.272895: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-01-09 21:02:09.283978: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "2025-01-09 21:02:09.298284: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "2025-01-09 21:02:09.298318: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "2025-01-09 21:02:09.308197: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-01-09 21:02:10.144356: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "/home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2025-01-09 21:02:10,930\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-01-09 21:02:11,042\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n",
      "2025-01-09 21:02:11,136\tINFO util.py:154 -- Missing packages: ['ipywidgets']. Run `pip install -U ipywidgets`, then restart the notebook server for rich notebook output.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from ray import train, tune\n",
    "from ray.tune.schedulers import ASHAScheduler\n",
    "from ray.train import RunConfig\n",
    "from ray.air.integrations.keras import ReportCheckpointCallback\n",
    "from ray.air.integrations.mlflow import MLflowLoggerCallback\n",
    "from joblib import load\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e5841d2-fde8-49ac-b8d9-1c7b6d83c4e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset():\n",
    "    import pandas as pd\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "    \n",
    "    # Load the CSV file\n",
    "    # df = pd.read_csv(\"/mnt/e/Python projects/Movie Recomender/normalized_user_movie_data.csv\")\n",
    "\n",
    "    item_train=pd.read_csv(\"/mnt/e/Python projects/Movie Recomender/data/content_item_train.csv\")\n",
    "    user_train=pd.read_csv(\"/mnt/e/Python projects/Movie Recomender/data/content_user_train.csv\")\n",
    "    y_train=pd.read_csv(\"/mnt/e/Python projects/Movie Recomender/data/content_y_train.csv\")\n",
    "    \n",
    "    scalerItem = StandardScaler()\n",
    "    scalerItem.fit(item_train)\n",
    "    item_train = scalerItem.transform(item_train)\n",
    "    \n",
    "    scalerUser = StandardScaler()\n",
    "    scalerUser.fit(user_train)\n",
    "    user_train = scalerUser.transform(user_train)\n",
    "    \n",
    "    scalerTarget = MinMaxScaler((-1, 1))\n",
    "    scalerTarget.fit(y_train)\n",
    "    y_train = scalerTarget.transform(y_train)\n",
    "\n",
    "\n",
    "    item_train, item_test = train_test_split(item_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "    user_train, user_test = train_test_split(user_train, train_size=0.80, shuffle=True, random_state=1)\n",
    "    y_train, y_test       = train_test_split(y_train,    train_size=0.80, shuffle=True, random_state=1)\n",
    "    \n",
    "    \n",
    "    \n",
    "   \n",
    "    # X=df\n",
    "    # # UserTrain=df.iloc[:,2:20]\n",
    "    # # MovieTrain=df.iloc[:,20:39]\n",
    "    # y=df['rating']\n",
    "    \n",
    "       \n",
    "    # Split the data into train (80%), temp (20%) split\n",
    "    # X_train, X_temp, y_train, y_temp = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    # Split the temp data into validation (10%) and test (10%) sets\n",
    "    # X_dev, X_test, y_dev, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=42)\n",
    "    # UserTrain=X_train.iloc[:,2:20]\n",
    "    # MovieTrain=X_train.iloc[:,20:39]\n",
    "    # UserDev=X_dev.iloc[:,2:20]\n",
    "    # MovieDev=X_dev.iloc[:,20:39]\n",
    "    # UserTest=X_test.iloc[:,2:20]\n",
    "    # MovieTest=X_test.iloc[:,20:39]\n",
    "    return [user_train[:,3:],item_train[:,2:],y_train,user_test[:,3:],item_test[:,2:],y_test]\n",
    "    #Display the sizes of each set\n",
    "    # print(f\"Training set: {UserTrain.info()},{MovieTrain.info()}, {y_train.shape}\")\n",
    "    # print(f\"Validation (Dev) set: {UserDev.shape},{MovieDev.shape}, {y_dev.shape}\")\n",
    "    # print(f\"Test set: {UserTest.shape},{MovieTest.shape}, {y_test.shape}\")\n",
    "    #print(X_train)\n",
    "    # UserTest.to_csv(\"UserTest.csv\", index=False)\n",
    "    # MovieTest.to_csv(\"MovieTest.csv\",index=False)\n",
    "    # y_test.to_csv(\"y_test.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "00b78b68-568d-4212-aff6-7c7b9268e6a7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[-0.75002545, -0.71201645,  0.05438833, ..., -0.63795668,\n",
       "         -0.65475297, -0.73380805],\n",
       "        [-0.4964236 , -0.74715933, -0.08397947, ..., -0.55450152,\n",
       "         -0.46625411, -0.3946172 ],\n",
       "        [ 0.3489159 , -0.37815909,  0.38434231, ...,  0.0018662 ,\n",
       "          0.43274043, -0.02312246],\n",
       "        ...,\n",
       "        [ 0.02768689,  0.18412699, -0.03076109, ..., -0.10940734,\n",
       "          0.21524175, -0.2169458 ],\n",
       "        [ 1.29569614,  1.20327051,  0.79944571, ...,  1.46233149,\n",
       "          0.91123753,  1.1398176 ],\n",
       "        [ 0.0107801 ,  0.09626979,  0.29919289, ...,  0.07141217,\n",
       "          0.31674114,  0.1060931 ]]),\n",
       " array([[-0.74706775, -0.71224094, -0.69696856, ..., -0.39919489,\n",
       "         -0.50822894, -0.65974585],\n",
       "        [-1.02211109, -0.71224094,  1.43478495, ..., -0.39919489,\n",
       "         -0.50822894, -0.65974585],\n",
       "        [ 1.16171783,  1.40401926, -0.69696856, ..., -0.39919489,\n",
       "          1.96761721,  1.51573519],\n",
       "        ...,\n",
       "        [-0.09735632, -0.71224094, -0.69696856, ..., -0.39919489,\n",
       "         -0.50822894, -0.65974585],\n",
       "        [ 0.92432343,  1.40401926, -0.69696856, ..., -0.39919489,\n",
       "          1.96761721, -0.65974585],\n",
       "        [-2.49987536,  1.40401926, -0.69696856, ..., -0.39919489,\n",
       "         -0.50822894,  1.51573519]]),\n",
       " array([[0.11111111],\n",
       "        [0.33333333],\n",
       "        [0.55555556],\n",
       "        ...,\n",
       "        [0.33333333],\n",
       "        [0.77777778],\n",
       "        [0.33333333]]),\n",
       " array([[ 1.71836589,  1.74798515,  1.55514677, ...,  1.39278553,\n",
       "          1.25923542,  2.06047847],\n",
       "        [ 1.32950972,  1.41412779,  0.6185032 , ...,  1.30933037,\n",
       "          1.1142363 ,  1.30133705],\n",
       "        [ 1.85362021,  2.02912819, -3.76669169, ...,  0.69732587,\n",
       "          1.52023383,  1.65667984],\n",
       "        ...,\n",
       "        [-0.12447422, -0.07944461,  1.02296293, ...,  0.69732587,\n",
       "          0.21524175,  0.38067617],\n",
       "        [ 0.09531405,  0.14898411,  0.26726186, ...,  0.34959603,\n",
       "          0.24424158,  0.34837228],\n",
       "        [ 0.43344985,  0.44769859,  0.756871  , ...,  0.02968459,\n",
       "          0.59223946,  0.21915672]]),\n",
       " array([[ 1.26364288, -0.71224094,  1.43478495, ..., -0.39919489,\n",
       "         -0.50822894, -0.65974585],\n",
       "        [-0.09950691, -0.71224094,  1.43478495, ...,  2.5050421 ,\n",
       "         -0.50822894, -0.65974585],\n",
       "        [ 0.44212698,  1.40401926, -0.69696856, ..., -0.39919489,\n",
       "         -0.50822894,  1.51573519],\n",
       "        ...,\n",
       "        [ 1.40316624, -0.71224094, -0.69696856, ...,  2.5050421 ,\n",
       "          1.96761721, -0.65974585],\n",
       "        [-0.53634601,  1.40401926,  1.43478495, ..., -0.39919489,\n",
       "         -0.50822894,  1.51573519],\n",
       "        [ 0.59172318,  1.40401926, -0.69696856, ..., -0.39919489,\n",
       "         -0.50822894,  1.51573519]]),\n",
       " array([[1.        ],\n",
       "        [0.77777778],\n",
       "        [0.77777778],\n",
       "        ...,\n",
       "        [1.        ],\n",
       "        [0.11111111],\n",
       "        [1.        ]])]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "39459e60-569e-42b1-bf3d-3afd3b29b6ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import Layer\n",
    "class L2Normalization(Layer):\n",
    "    def call(self, inputs):\n",
    "        return tf.linalg.l2_normalize(inputs, axis=1)\n",
    "\n",
    "\n",
    "\n",
    "def train_model(config):\n",
    "    # Build the model based on the Ray Tune configuration\n",
    "     # Define the input layer\n",
    "    input1 = tf.keras.Input(shape=(14,))\n",
    "    input2 = tf.keras.Input(shape=(15,))\n",
    "\n",
    "    # First dense layer\n",
    "    x1 = tf.keras.layers.Dense(config['units_u1'], activation=\"relu\", kernel_initializer=\"he_normal\")(input1)\n",
    "\n",
    "    # Additional dense layers based on the config\n",
    "    for i in range(2, config['num_layersu'] + 2):\n",
    "        x1 = tf.keras.layers.Dense(config[f'unitsu_{i}'], activation=\"relu\", kernel_initializer=\"he_normal\")(x1)\n",
    "\n",
    "    # Output layer\n",
    "    embedding1 = tf.keras.layers.Dense(config['emb_unit'], activation=\"relu\")(x1)\n",
    "\n",
    "     # First dense layer\n",
    "    x2 = tf.keras.layers.Dense(config['units_m1'], activation=\"relu\", kernel_initializer=\"he_normal\")(input2)\n",
    "\n",
    "    # Additional dense layers based on the config\n",
    "    for i in range(2, config['num_layersm'] + 2):\n",
    "        x2 = tf.keras.layers.Dense(config[f'unitsm_{i}'], activation=\"relu\", kernel_initializer=\"he_normal\")(x2)\n",
    "\n",
    "    # Output layer\n",
    "    embedding2 = tf.keras.layers.Dense(config['emb_unit'], activation=\"relu\")(x2)\n",
    "\n",
    "\n",
    "    emb1 = L2Normalization()(embedding1)\n",
    "    emb2 = L2Normalization()(embedding2)\n",
    "    #emb1=tf.linalg.l2_normalize(embedding1, axis=1)\n",
    "    #emb2=tf.linalg.l2_normalize(embedding2, axis=1)\n",
    "\n",
    "    output = tf.keras.layers.Dot(axes=1)([emb1, emb2])\n",
    "\n",
    "    # Create the model\n",
    "    model = tf.keras.Model(inputs=[input1,input2], outputs=output)\n",
    "\n",
    "    # Compile the model\n",
    "    model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=config['learning_rate']),\n",
    "        loss=tf.keras.losses.mse,\n",
    "        metrics=['r2_score','mean_squared_error']\n",
    "    )\n",
    "\n",
    "    # Load the dataset\n",
    "    #(train_data, train_labels), (val_data, val_labels) = tf.keras.datasets.mnist.load_data()\n",
    "    UserTrain,MovieTrain,y_train,UserDev,MovieDev,y_dev=load_dataset()\n",
    "    # UserTrain = np.array(UserTrain, dtype=np.float32)\n",
    "    # MovieTrain = np.array(MovieTrain, dtype=np.float32)\n",
    "    # y_train = np.array(y_train, dtype=np.float32)\n",
    "    # UserDev = np.array(UserDev, dtype=np.float32)\n",
    "    # MovieDev = np.array(MovieDev, dtype=np.float32)\n",
    "    # y_dev = np.array(y_dev, dtype=np.float32)\n",
    "    #train_data, val_data = train_data / 255.0, val_data / 255.0\n",
    "    #train_data, val_data = train_data.reshape(-1, 784), val_data.reshape(-1, 784)\n",
    "    #train_labels, val_labels = train_labels % 3, val_labels % 3  # Reduce classes for example\n",
    "\n",
    "    # Train the model with Ray Tune's ReportCheckpointCallback\n",
    "    model.fit(\n",
    "        [UserTrain,MovieTrain], y_train,\n",
    "        epochs=20,\n",
    "        validation_data=([UserDev,MovieDev], y_dev),\n",
    "        verbose=0,\n",
    "        callbacks=[ReportCheckpointCallback(metrics={\"r2_score\": \"r2_score\",\"mse\":\"mean_squared_error\"})]\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "53af99b8-33f8-4550-82b9-aaa3add5506f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-01-09 20:29:03</td></tr>\n",
       "<tr><td>Running for: </td><td>00:17:43.33        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.8/7.6 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using AsyncHyperBand: num_stopped=18<br>Bracket: Iter 40.000: None | Iter 20.000: -0.09879686683416367 | Iter 10.000: -0.1067267581820488 | Iter 5.000: -0.11830015107989311<br>Logical resource usage: 10.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    \n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name             </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  emb_unit</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layersm</th><th style=\"text-align: right;\">  num_layersu</th><th style=\"text-align: right;\">  units_m1</th><th style=\"text-align: right;\">  units_u1</th><th style=\"text-align: right;\">  unitsm_2</th><th style=\"text-align: right;\">  unitsm_3</th><th style=\"text-align: right;\">  unitsm_4</th><th style=\"text-align: right;\">  unitsu_2</th><th style=\"text-align: right;\">  unitsu_3</th><th style=\"text-align: right;\">  unitsu_4</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  r2_score</th><th style=\"text-align: right;\">      mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_ca4cc_00000</td><td>TERMINATED</td><td>172.21.168.162:173793</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">    0.00228821 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        14</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        18</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">        37</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         69.3384</td><td style=\"text-align: right;\">  0.46312 </td><td style=\"text-align: right;\">0.0969465</td></tr>\n",
       "<tr><td>train_model_ca4cc_00001</td><td>TERMINATED</td><td>172.21.168.162:175208</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">    0.00381236 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        17</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         22.2579</td><td style=\"text-align: right;\">  0.33535 </td><td style=\"text-align: right;\">0.120018 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00002</td><td>TERMINATED</td><td>172.21.168.162:175793</td><td style=\"text-align: right;\">        17</td><td style=\"text-align: right;\">    0.00226427 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        47</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        23</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        31</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         41.4027</td><td style=\"text-align: right;\">  0.404433</td><td style=\"text-align: right;\">0.107544 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00003</td><td>TERMINATED</td><td>172.21.168.162:176665</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">    0.00168036 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">        35</td><td style=\"text-align: right;\">        35</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">         2</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         27.8073</td><td style=\"text-align: right;\">  0.338981</td><td style=\"text-align: right;\">0.119363 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00004</td><td>TERMINATED</td><td>172.21.168.162:177378</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">    0.00256835 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        17</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         24.9046</td><td style=\"text-align: right;\">  0.294794</td><td style=\"text-align: right;\">0.127342 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00005</td><td>TERMINATED</td><td>172.21.168.162:178006</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    0.000393346</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        43</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         22.7167</td><td style=\"text-align: right;\">  0.317669</td><td style=\"text-align: right;\">0.123212 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00006</td><td>TERMINATED</td><td>172.21.168.162:178597</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    0.00590638 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        47</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        37</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         41.8986</td><td style=\"text-align: right;\">  0.410794</td><td style=\"text-align: right;\">0.106395 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00007</td><td>TERMINATED</td><td>172.21.168.162:179556</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">    0.000650006</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">        17</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        17</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         73.1836</td><td style=\"text-align: right;\">  0.44116 </td><td style=\"text-align: right;\">0.100912 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00008</td><td>TERMINATED</td><td>172.21.168.162:180952</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">    0.000240878</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         29.3342</td><td style=\"text-align: right;\">  0.30721 </td><td style=\"text-align: right;\">0.1251   </td></tr>\n",
       "<tr><td>train_model_ca4cc_00009</td><td>TERMINATED</td><td>172.21.168.162:181638</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">    0.00497764 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">        35</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         25.265 </td><td style=\"text-align: right;\">  0.30462 </td><td style=\"text-align: right;\">0.125568 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00010</td><td>TERMINATED</td><td>172.21.168.162:182237</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">    0.000959362</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">        37</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         45.1066</td><td style=\"text-align: right;\">  0.389586</td><td style=\"text-align: right;\">0.110225 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00011</td><td>TERMINATED</td><td>172.21.168.162:183171</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">    0.000616122</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         43.2627</td><td style=\"text-align: right;\">  0.402111</td><td style=\"text-align: right;\">0.107963 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00012</td><td>TERMINATED</td><td>172.21.168.162:184071</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">    0.00223824 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">        14</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">        41</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         79.2738</td><td style=\"text-align: right;\">  0.452873</td><td style=\"text-align: right;\">0.0987969</td></tr>\n",
       "<tr><td>train_model_ca4cc_00013</td><td>TERMINATED</td><td>172.21.168.162:185510</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">    0.00433734 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        43</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         21.9406</td><td style=\"text-align: right;\">  0.342698</td><td style=\"text-align: right;\">0.118692 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00014</td><td>TERMINATED</td><td>172.21.168.162:186076</td><td style=\"text-align: right;\">        31</td><td style=\"text-align: right;\">    0.00601631 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        37</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">        26</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         22.6281</td><td style=\"text-align: right;\">  0.340033</td><td style=\"text-align: right;\">0.119173 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00015</td><td>TERMINATED</td><td>172.21.168.162:186669</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    0.00185043 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        23</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        41</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         42.3976</td><td style=\"text-align: right;\">  0.408959</td><td style=\"text-align: right;\">0.106727 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00016</td><td>TERMINATED</td><td>172.21.168.162:187590</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">    0.00195149 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         22.6881</td><td style=\"text-align: right;\">  0.329812</td><td style=\"text-align: right;\">0.121019 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00017</td><td>TERMINATED</td><td>172.21.168.162:188162</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">    0.00504336 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        37</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">    10</td><td style=\"text-align: right;\">         38.1264</td><td style=\"text-align: right;\">  0.398335</td><td style=\"text-align: right;\">0.108645 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00018</td><td>TERMINATED</td><td>172.21.168.162:189011</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    0.00541418 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        23</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">        23</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         24.1785</td><td style=\"text-align: right;\">  0.347035</td><td style=\"text-align: right;\">0.117908 </td></tr>\n",
       "<tr><td>train_model_ca4cc_00019</td><td>TERMINATED</td><td>172.21.168.162:189633</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">    0.000392063</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">        42</td><td style=\"text-align: right;\">        30</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">     5</td><td style=\"text-align: right;\">         22.9121</td><td style=\"text-align: right;\">  0.313873</td><td style=\"text-align: right;\">0.123897 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=173793)\u001b[0m 2025-01-09 20:11:22.039679: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=173793)\u001b[0m 2025-01-09 20:11:22.055937: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=173793)\u001b[0m 2025-01-09 20:11:22.074929: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=173793)\u001b[0m 2025-01-09 20:11:22.074975: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=173793)\u001b[0m 2025-01-09 20:11:22.089057: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=173793)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=173793)\u001b[0m 2025-01-09 20:11:23.085823: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.466807: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.518803: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.518858: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.521685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.521734: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.521750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.617915: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.617996: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.618021: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.618068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:25.618112: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m I0000 00:00:1736433687.409137  173911 service.cc:145] XLA service 0x7f2b3800f590 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m I0000 00:00:1736433687.409219  173911 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:27.465821: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m 2025-01-09 20:11:27.704685: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m I0000 00:00:1736433690.516107  173911 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:11:38,394\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:38,396\tWARNING util.py:201 -- The `process_trial_result` operation took 1.370 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:38,396\tWARNING util.py:201 -- Processing trial results took 1.370 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:11:38,397\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:11:42,966\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.290 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:42,968\tWARNING util.py:201 -- The `process_trial_result` operation took 1.292 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:42,969\tWARNING util.py:201 -- Processing trial results took 1.294 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:11:42,970\tWARNING util.py:201 -- The `process_trial_result` operation took 1.294 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:11:47,445\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:47,447\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:47,447\tWARNING util.py:201 -- Processing trial results took 1.354 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:11:47,448\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:11:51,806\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:51,807\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:51,808\tWARNING util.py:201 -- Processing trial results took 1.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:11:51,808\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:11:56,272\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.284 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:56,274\tWARNING util.py:201 -- The `process_trial_result` operation took 1.286 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:11:56,275\tWARNING util.py:201 -- Processing trial results took 1.286 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:11:56,276\tWARNING util.py:201 -- The `process_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:12:00,824\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:00,825\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:00,827\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:00,828\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:12:05,195\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:05,196\tWARNING util.py:201 -- The `process_trial_result` operation took 1.369 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:05,197\tWARNING util.py:201 -- Processing trial results took 1.370 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:05,198\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:12:09,692\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:09,694\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:09,695\tWARNING util.py:201 -- Processing trial results took 1.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:09,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:12:14,113\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:14,115\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:14,115\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:14,116\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:12:18,404\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:18,405\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:18,406\tWARNING util.py:201 -- Processing trial results took 1.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:18,406\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000010)\n",
      "2025-01-09 20:12:22,615\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:22,617\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:22,617\tWARNING util.py:201 -- Processing trial results took 1.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:22,618\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000011)\n",
      "2025-01-09 20:12:26,693\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.220 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:26,695\tWARNING util.py:201 -- The `process_trial_result` operation took 1.222 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:26,695\tWARNING util.py:201 -- Processing trial results took 1.223 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:26,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.223 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000012)\n",
      "2025-01-09 20:12:30,876\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:30,878\tWARNING util.py:201 -- The `process_trial_result` operation took 1.289 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:30,878\tWARNING util.py:201 -- Processing trial results took 1.289 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:30,879\tWARNING util.py:201 -- The `process_trial_result` operation took 1.290 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000013)\n",
      "2025-01-09 20:12:34,943\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.152 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:34,945\tWARNING util.py:201 -- The `process_trial_result` operation took 1.153 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:34,946\tWARNING util.py:201 -- Processing trial results took 1.154 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:34,946\tWARNING util.py:201 -- The `process_trial_result` operation took 1.155 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000014)\n",
      "2025-01-09 20:12:39,102\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.258 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:39,104\tWARNING util.py:201 -- The `process_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:39,104\tWARNING util.py:201 -- Processing trial results took 1.261 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:39,105\tWARNING util.py:201 -- The `process_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000015)\n",
      "2025-01-09 20:12:43,296\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.280 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:43,297\tWARNING util.py:201 -- The `process_trial_result` operation took 1.281 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:43,298\tWARNING util.py:201 -- Processing trial results took 1.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:43,299\tWARNING util.py:201 -- The `process_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000016)\n",
      "2025-01-09 20:12:47,960\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:47,962\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:47,962\tWARNING util.py:201 -- Processing trial results took 1.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:47,963\tWARNING util.py:201 -- The `process_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000017)\n",
      "2025-01-09 20:12:52,387\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:52,389\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:52,389\tWARNING util.py:201 -- Processing trial results took 1.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:52,390\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000018)\n",
      "2025-01-09 20:12:56,693\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:56,694\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:12:56,696\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:12:56,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=173793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00000_0_emb_unit=28,learning_rate=0.0023,num_layersm=2,num_layersu=2,units_m1=34,units_u1=14,unitsm_2=46,unitsm__2025-01-09_20-11-20/checkpoint_000019)\n",
      "2025-01-09 20:13:01,201\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.407 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:01,202\tWARNING util.py:201 -- The `process_trial_result` operation took 1.408 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:01,203\tWARNING util.py:201 -- Processing trial results took 1.408 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:01,203\tWARNING util.py:201 -- The `process_trial_result` operation took 1.409 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:13:01 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00000 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/58664e7ec26945b88205a8becff4c936.\n",
      "2025/01/09 20:13:01 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=175208)\u001b[0m 2025-01-09 20:13:03.497789: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=175208)\u001b[0m 2025-01-09 20:13:03.508958: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=175208)\u001b[0m 2025-01-09 20:13:03.523574: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=175208)\u001b[0m 2025-01-09 20:13:03.523623: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=175208)\u001b[0m 2025-01-09 20:13:03.533710: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=175208)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=175208)\u001b[0m 2025-01-09 20:13:04.092246: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.731526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.750910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.750967: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.753285: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.753355: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.753373: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.841950: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.842030: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.842036: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.842061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:05.842083: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m I0000 00:00:1736433787.481053  175319 service.cc:145] XLA service 0x7fad78010b70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m I0000 00:00:1736433787.481105  175319 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:07.516560: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m 2025-01-09 20:13:07.706470: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m I0000 00:00:1736433789.671688  175319 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00001_1_emb_unit=22,learning_rate=0.0038,num_layersm=3,num_layersu=2,units_m1=38,units_u1=21,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:13:16,654\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:16,655\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:16,656\tWARNING util.py:201 -- Processing trial results took 1.299 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:16,657\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00001_1_emb_unit=22,learning_rate=0.0038,num_layersm=3,num_layersu=2,units_m1=38,units_u1=21,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:13:21,237\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:21,238\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:21,239\tWARNING util.py:201 -- Processing trial results took 1.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:21,239\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00001_1_emb_unit=22,learning_rate=0.0038,num_layersm=3,num_layersu=2,units_m1=38,units_u1=21,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:13:25,761\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:25,763\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:25,764\tWARNING util.py:201 -- Processing trial results took 1.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:25,764\tWARNING util.py:201 -- The `process_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00001_1_emb_unit=22,learning_rate=0.0038,num_layersm=3,num_layersu=2,units_m1=38,units_u1=21,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:13:30,137\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:30,139\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:30,140\tWARNING util.py:201 -- Processing trial results took 1.321 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:30,141\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175208)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00001_1_emb_unit=22,learning_rate=0.0038,num_layersm=3,num_layersu=2,units_m1=38,units_u1=21,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:13:34,364\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.119 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:34,365\tWARNING util.py:201 -- The `process_trial_result` operation took 1.121 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:34,366\tWARNING util.py:201 -- Processing trial results took 1.121 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:34,366\tWARNING util.py:201 -- The `process_trial_result` operation took 1.122 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:13:34 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00001 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/0e37d5658e3f4a2babc31f8bf2a99d16.\n",
      "2025/01/09 20:13:34 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=175793)\u001b[0m 2025-01-09 20:13:36.481645: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=175793)\u001b[0m 2025-01-09 20:13:36.490667: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=175793)\u001b[0m 2025-01-09 20:13:36.503461: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=175793)\u001b[0m 2025-01-09 20:13:36.503498: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=175793)\u001b[0m 2025-01-09 20:13:36.511212: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=175793)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=175793)\u001b[0m 2025-01-09 20:13:37.103993: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.723165: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.752443: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.752571: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.756174: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.756274: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.756296: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.842764: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.842832: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.842839: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.842865: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:38.842887: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m I0000 00:00:1736433820.528439  175897 service.cc:145] XLA service 0x7fa930024520 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m I0000 00:00:1736433820.528488  175897 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:40.560149: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m 2025-01-09 20:13:40.740684: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m I0000 00:00:1736433823.705562  175897 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m I0000 00:00:1736433828.026825  175900 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m \n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:13:51,177\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:51,178\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:51,178\tWARNING util.py:201 -- Processing trial results took 1.321 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:51,179\tWARNING util.py:201 -- The `process_trial_result` operation took 1.322 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:13:55,984\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:55,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:13:55,986\tWARNING util.py:201 -- Processing trial results took 1.316 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:13:55,987\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:14:00,617\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:00,619\tWARNING util.py:201 -- The `process_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:00,619\tWARNING util.py:201 -- Processing trial results took 1.312 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:00,620\tWARNING util.py:201 -- The `process_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:14:05,236\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:05,238\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:05,239\tWARNING util.py:201 -- Processing trial results took 1.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:05,240\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:14:09,953\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:09,955\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:09,955\tWARNING util.py:201 -- Processing trial results took 1.338 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:09,956\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:14:14,427\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.229 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:14,429\tWARNING util.py:201 -- The `process_trial_result` operation took 1.230 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:14,430\tWARNING util.py:201 -- Processing trial results took 1.231 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:14,431\tWARNING util.py:201 -- The `process_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:14:19,044\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:19,045\tWARNING util.py:201 -- The `process_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:19,046\tWARNING util.py:201 -- Processing trial results took 1.323 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:19,046\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:14:23,863\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:23,865\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:23,865\tWARNING util.py:201 -- Processing trial results took 1.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:23,866\tWARNING util.py:201 -- The `process_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:14:28,540\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.313 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:28,541\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:28,542\tWARNING util.py:201 -- Processing trial results took 1.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:28,542\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=175793)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00002_2_emb_unit=17,learning_rate=0.0023,num_layersm=3,num_layersu=2,units_m1=10,units_u1=13,unitsm_2=47,unitsm__2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:14:33,256\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:33,258\tWARNING util.py:201 -- The `process_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:33,258\tWARNING util.py:201 -- Processing trial results took 1.313 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:33,259\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:14:33 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00002 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/d8e8d1fe853f4a81b69fc0e1303e0e99.\n",
      "2025/01/09 20:14:33 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=176665)\u001b[0m 2025-01-09 20:14:35.907480: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=176665)\u001b[0m 2025-01-09 20:14:35.917714: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=176665)\u001b[0m 2025-01-09 20:14:35.930826: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=176665)\u001b[0m 2025-01-09 20:14:35.930867: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=176665)\u001b[0m 2025-01-09 20:14:35.938984: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=176665)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=176665)\u001b[0m 2025-01-09 20:14:36.803120: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.799869: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.838451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.838525: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.842222: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.842287: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.842307: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.930778: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.930847: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.930854: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.930880: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:38.930901: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m I0000 00:00:1736433880.934029  176778 service.cc:145] XLA service 0x7f9c90006fa0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m I0000 00:00:1736433880.934083  176778 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:40.968877: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m 2025-01-09 20:14:41.174979: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m I0000 00:00:1736433885.315238  176778 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m I0000 00:00:1736433889.132452  176971 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_554', 8 bytes spill stores, 8 bytes spill loads\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m \n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00003_3_emb_unit=25,learning_rate=0.0017,num_layersm=3,num_layersu=3,units_m1=29,units_u1=36,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:14:55,021\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:55,023\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:55,024\tWARNING util.py:201 -- Processing trial results took 1.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:55,024\tWARNING util.py:201 -- The `process_trial_result` operation took 1.248 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00003_3_emb_unit=25,learning_rate=0.0017,num_layersm=3,num_layersu=3,units_m1=29,units_u1=36,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:14:59,740\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:59,742\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:14:59,742\tWARNING util.py:201 -- Processing trial results took 1.350 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:14:59,742\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00003_3_emb_unit=25,learning_rate=0.0017,num_layersm=3,num_layersu=3,units_m1=29,units_u1=36,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:15:04,193\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:04,196\tWARNING util.py:201 -- The `process_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:04,196\tWARNING util.py:201 -- Processing trial results took 1.301 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:04,197\tWARNING util.py:201 -- The `process_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00003_3_emb_unit=25,learning_rate=0.0017,num_layersm=3,num_layersu=3,units_m1=29,units_u1=36,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:15:08,620\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:08,622\tWARNING util.py:201 -- The `process_trial_result` operation took 1.275 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:08,622\tWARNING util.py:201 -- Processing trial results took 1.276 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:08,623\tWARNING util.py:201 -- The `process_trial_result` operation took 1.276 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=176665)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00003_3_emb_unit=25,learning_rate=0.0017,num_layersm=3,num_layersu=3,units_m1=29,units_u1=36,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:15:13,101\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:13,103\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:13,103\tWARNING util.py:201 -- Processing trial results took 1.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:13,104\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:15:13 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00003 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/d8756ba1539d48c790920f587be132af.\n",
      "2025/01/09 20:15:13 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=177378)\u001b[0m 2025-01-09 20:15:15.412703: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=177378)\u001b[0m 2025-01-09 20:15:15.423833: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=177378)\u001b[0m 2025-01-09 20:15:15.437522: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=177378)\u001b[0m 2025-01-09 20:15:15.437564: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=177378)\u001b[0m 2025-01-09 20:15:15.445493: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=177378)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=177378)\u001b[0m 2025-01-09 20:15:16.037545: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.716964: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.744791: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.744853: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.748418: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.748493: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.748517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.838543: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.838607: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.838614: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.838640: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:17.838658: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m I0000 00:00:1736433919.608646  177488 service.cc:145] XLA service 0x7f3958016dc0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m I0000 00:00:1736433919.608702  177488 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:19.641648: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m 2025-01-09 20:15:19.831010: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m I0000 00:00:1736433923.070703  177488 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00004_4_emb_unit=16,learning_rate=0.0026,num_layersm=3,num_layersu=3,units_m1=28,units_u1=1,unitsm_2=40,unitsm_3_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:15:30,694\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:30,696\tWARNING util.py:201 -- The `process_trial_result` operation took 1.233 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:30,697\tWARNING util.py:201 -- Processing trial results took 1.234 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:30,697\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00004_4_emb_unit=16,learning_rate=0.0026,num_layersm=3,num_layersu=3,units_m1=28,units_u1=1,unitsm_2=40,unitsm_3_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:15:35,178\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:35,179\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:35,180\tWARNING util.py:201 -- Processing trial results took 1.328 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:35,181\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00004_4_emb_unit=16,learning_rate=0.0026,num_layersm=3,num_layersu=3,units_m1=28,units_u1=1,unitsm_2=40,unitsm_3_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:15:39,887\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:39,889\tWARNING util.py:201 -- The `process_trial_result` operation took 1.366 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:39,890\tWARNING util.py:201 -- Processing trial results took 1.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:39,890\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00004_4_emb_unit=16,learning_rate=0.0026,num_layersm=3,num_layersu=3,units_m1=28,units_u1=1,unitsm_2=40,unitsm_3_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:15:44,643\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:44,644\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:44,645\tWARNING util.py:201 -- Processing trial results took 1.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:44,646\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=177378)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00004_4_emb_unit=16,learning_rate=0.0026,num_layersm=3,num_layersu=3,units_m1=28,units_u1=1,unitsm_2=40,unitsm_3_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:15:49,227\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:49,229\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:15:49,230\tWARNING util.py:201 -- Processing trial results took 1.335 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:15:49,230\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:15:49 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00004 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/9c9c97b29e1c4f6e8fcd66908a096f13.\n",
      "2025/01/09 20:15:49 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=178006)\u001b[0m 2025-01-09 20:15:51.490208: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=178006)\u001b[0m 2025-01-09 20:15:51.499629: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=178006)\u001b[0m 2025-01-09 20:15:51.512767: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=178006)\u001b[0m 2025-01-09 20:15:51.512812: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=178006)\u001b[0m 2025-01-09 20:15:51.520661: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=178006)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=178006)\u001b[0m 2025-01-09 20:15:52.079540: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.788903: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.809342: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.809418: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.812303: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.812427: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.812460: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.899914: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.899978: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.899985: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.900011: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:53.900033: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m I0000 00:00:1736433955.590811  178113 service.cc:145] XLA service 0x7fc9ec0150b0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m I0000 00:00:1736433955.590863  178113 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:55.621287: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m 2025-01-09 20:15:55.821901: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m I0000 00:00:1736433958.661018  178113 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00005_5_emb_unit=24,learning_rate=0.0004,num_layersm=2,num_layersu=3,units_m1=43,units_u1=33,unitsm_2=21,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:16:05,619\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.171 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:05,620\tWARNING util.py:201 -- The `process_trial_result` operation took 1.172 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:05,620\tWARNING util.py:201 -- Processing trial results took 1.173 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:05,621\tWARNING util.py:201 -- The `process_trial_result` operation took 1.173 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00005_5_emb_unit=24,learning_rate=0.0004,num_layersm=2,num_layersu=3,units_m1=43,units_u1=33,unitsm_2=21,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:16:09,871\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:09,873\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:09,874\tWARNING util.py:201 -- Processing trial results took 1.308 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:09,874\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00005_5_emb_unit=24,learning_rate=0.0004,num_layersm=2,num_layersu=3,units_m1=43,units_u1=33,unitsm_2=21,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:16:14,449\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:14,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:14,451\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:14,451\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00005_5_emb_unit=24,learning_rate=0.0004,num_layersm=2,num_layersu=3,units_m1=43,units_u1=33,unitsm_2=21,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:16:18,623\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.238 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:18,625\tWARNING util.py:201 -- The `process_trial_result` operation took 1.240 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:18,627\tWARNING util.py:201 -- Processing trial results took 1.241 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:18,627\tWARNING util.py:201 -- The `process_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178006)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00005_5_emb_unit=24,learning_rate=0.0004,num_layersm=2,num_layersu=3,units_m1=43,units_u1=33,unitsm_2=21,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:16:23,015\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.471 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:23,016\tWARNING util.py:201 -- The `process_trial_result` operation took 1.472 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:23,017\tWARNING util.py:201 -- Processing trial results took 1.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:23,018\tWARNING util.py:201 -- The `process_trial_result` operation took 1.474 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:16:23 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00005 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/b1e48bcd5aac45ac80341857efd3fcfc.\n",
      "2025/01/09 20:16:23 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=178597)\u001b[0m 2025-01-09 20:16:24.935378: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=178597)\u001b[0m 2025-01-09 20:16:24.945217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=178597)\u001b[0m 2025-01-09 20:16:24.957858: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=178597)\u001b[0m 2025-01-09 20:16:24.957898: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=178597)\u001b[0m 2025-01-09 20:16:24.965797: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=178597)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=178597)\u001b[0m 2025-01-09 20:16:25.783048: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.834881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.873347: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.873401: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.876451: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.876505: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.876521: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.966368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.966434: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.966440: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.966464: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:27.966487: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m I0000 00:00:1736433989.817317  178713 service.cc:145] XLA service 0x7fcd8c011eb0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m I0000 00:00:1736433989.817368  178713 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:29.849679: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m 2025-01-09 20:16:30.044305: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m I0000 00:00:1736433993.661787  178713 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:16:41,994\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.062 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:41,995\tWARNING util.py:201 -- The `process_trial_result` operation took 1.064 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:41,996\tWARNING util.py:201 -- Processing trial results took 1.065 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:41,997\tWARNING util.py:201 -- The `process_trial_result` operation took 1.066 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:16:46,543\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:46,545\tWARNING util.py:201 -- The `process_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:46,546\tWARNING util.py:201 -- Processing trial results took 1.318 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:46,546\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:16:51,012\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:51,013\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:51,014\tWARNING util.py:201 -- Processing trial results took 1.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:51,015\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:16:55,639\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.313 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:55,640\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:55,641\tWARNING util.py:201 -- Processing trial results took 1.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:55,641\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:16:59,903\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:59,905\tWARNING util.py:201 -- The `process_trial_result` operation took 1.193 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:16:59,906\tWARNING util.py:201 -- Processing trial results took 1.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:16:59,906\tWARNING util.py:201 -- The `process_trial_result` operation took 1.195 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:17:04,358\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:04,359\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:04,360\tWARNING util.py:201 -- Processing trial results took 1.300 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:04,360\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:17:09,052\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:09,053\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:09,054\tWARNING util.py:201 -- Processing trial results took 1.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:09,054\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:17:13,377\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.239 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:13,378\tWARNING util.py:201 -- The `process_trial_result` operation took 1.241 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:13,379\tWARNING util.py:201 -- Processing trial results took 1.241 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:13,379\tWARNING util.py:201 -- The `process_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:17:17,674\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.237 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:17,675\tWARNING util.py:201 -- The `process_trial_result` operation took 1.238 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:17,676\tWARNING util.py:201 -- Processing trial results took 1.239 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:17,676\tWARNING util.py:201 -- The `process_trial_result` operation took 1.240 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=178597)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00006_6_emb_unit=24,learning_rate=0.0059,num_layersm=2,num_layersu=3,units_m1=47,units_u1=16,unitsm_2=49,unitsm__2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:17:22,256\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.201 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:22,257\tWARNING util.py:201 -- The `process_trial_result` operation took 1.202 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:22,258\tWARNING util.py:201 -- Processing trial results took 1.203 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:22,258\tWARNING util.py:201 -- The `process_trial_result` operation took 1.203 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:17:22 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00006 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/01dde90a3abe4f12bfa1499c7162987e.\n",
      "2025/01/09 20:17:22 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=179556)\u001b[0m 2025-01-09 20:17:24.887033: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=179556)\u001b[0m 2025-01-09 20:17:24.897787: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=179556)\u001b[0m 2025-01-09 20:17:24.913190: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=179556)\u001b[0m 2025-01-09 20:17:24.913228: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=179556)\u001b[0m 2025-01-09 20:17:24.922889: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=179556)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=179556)\u001b[0m 2025-01-09 20:17:25.785094: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:27.941101: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:27.991587: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:27.991652: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:27.993967: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:27.994045: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:27.994079: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:28.086763: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:28.086837: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:28.086844: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:28.086868: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:28.086890: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m I0000 00:00:1736434050.190617  179669 service.cc:145] XLA service 0x7f48f8006250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m I0000 00:00:1736434050.190671  179669 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:30.230460: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m 2025-01-09 20:17:30.453031: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m I0000 00:00:1736434053.915319  179669 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:17:42,109\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:42,110\tWARNING util.py:201 -- The `process_trial_result` operation took 1.279 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:42,111\tWARNING util.py:201 -- Processing trial results took 1.279 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:42,111\tWARNING util.py:201 -- The `process_trial_result` operation took 1.280 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:17:46,684\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:46,686\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:46,686\tWARNING util.py:201 -- Processing trial results took 1.318 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:46,687\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:17:51,258\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:51,259\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:51,260\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:51,260\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:17:55,777\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:55,779\tWARNING util.py:201 -- The `process_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:55,779\tWARNING util.py:201 -- Processing trial results took 1.312 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:55,779\tWARNING util.py:201 -- The `process_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:17:59,915\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.018 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:59,917\tWARNING util.py:201 -- The `process_trial_result` operation took 1.019 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:17:59,918\tWARNING util.py:201 -- Processing trial results took 1.020 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:17:59,918\tWARNING util.py:201 -- The `process_trial_result` operation took 1.021 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:18:04,554\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:04,555\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:04,556\tWARNING util.py:201 -- Processing trial results took 1.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:04,556\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:18:08,904\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.313 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:08,907\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:08,907\tWARNING util.py:201 -- Processing trial results took 1.317 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:08,907\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:18:13,268\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.281 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:13,269\tWARNING util.py:201 -- The `process_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:13,270\tWARNING util.py:201 -- Processing trial results took 1.283 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:13,271\tWARNING util.py:201 -- The `process_trial_result` operation took 1.284 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:18:17,984\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:17,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:17,986\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:17,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:18:22,409\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:22,411\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:22,411\tWARNING util.py:201 -- Processing trial results took 1.299 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:22,412\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000010)\n",
      "2025-01-09 20:18:26,804\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:26,806\tWARNING util.py:201 -- The `process_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:26,806\tWARNING util.py:201 -- Processing trial results took 1.313 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:26,807\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000011)\n",
      "2025-01-09 20:18:31,326\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.238 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:31,328\tWARNING util.py:201 -- The `process_trial_result` operation took 1.240 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:31,328\tWARNING util.py:201 -- Processing trial results took 1.241 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:31,329\tWARNING util.py:201 -- The `process_trial_result` operation took 1.241 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000012)\n",
      "2025-01-09 20:18:35,541\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.185 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:35,543\tWARNING util.py:201 -- The `process_trial_result` operation took 1.186 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:35,543\tWARNING util.py:201 -- Processing trial results took 1.187 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:35,544\tWARNING util.py:201 -- The `process_trial_result` operation took 1.188 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000013)\n",
      "2025-01-09 20:18:39,872\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.254 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:39,873\tWARNING util.py:201 -- The `process_trial_result` operation took 1.256 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:39,874\tWARNING util.py:201 -- Processing trial results took 1.257 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:39,874\tWARNING util.py:201 -- The `process_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000014)\n",
      "2025-01-09 20:18:44,269\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:44,272\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:44,273\tWARNING util.py:201 -- Processing trial results took 1.304 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:44,274\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000015)\n",
      "2025-01-09 20:18:48,839\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.270 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:48,840\tWARNING util.py:201 -- The `process_trial_result` operation took 1.272 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:48,841\tWARNING util.py:201 -- Processing trial results took 1.273 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:48,842\tWARNING util.py:201 -- The `process_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000016)\n",
      "2025-01-09 20:18:53,401\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:53,402\tWARNING util.py:201 -- The `process_trial_result` operation took 1.306 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:53,403\tWARNING util.py:201 -- Processing trial results took 1.307 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:53,403\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000017)\n",
      "2025-01-09 20:18:57,799\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:57,801\tWARNING util.py:201 -- The `process_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:18:57,801\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:18:57,801\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000018)\n",
      "2025-01-09 20:19:02,299\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:02,300\tWARNING util.py:201 -- The `process_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:02,301\tWARNING util.py:201 -- Processing trial results took 1.302 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:02,301\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=179556)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00007_7_emb_unit=29,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=32,units_u1=30,unitsm_2=17,unitsm__2025-01-09_20-11-20/checkpoint_000019)\n",
      "2025-01-09 20:19:06,752\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.275 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:06,754\tWARNING util.py:201 -- The `process_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:06,754\tWARNING util.py:201 -- Processing trial results took 1.278 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:06,754\tWARNING util.py:201 -- The `process_trial_result` operation took 1.278 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:19:07 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00007 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/7f831d90731443adbbd06706810e2561.\n",
      "2025/01/09 20:19:07 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=180952)\u001b[0m 2025-01-09 20:19:08.968297: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=180952)\u001b[0m 2025-01-09 20:19:08.979217: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=180952)\u001b[0m 2025-01-09 20:19:08.994276: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=180952)\u001b[0m 2025-01-09 20:19:08.994319: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=180952)\u001b[0m 2025-01-09 20:19:09.004098: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=180952)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=180952)\u001b[0m 2025-01-09 20:19:09.892557: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.000900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.037350: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.037405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.039836: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.039910: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.039931: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.122740: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.122817: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.122824: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.122855: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:12.122879: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m I0000 00:00:1736434154.139516  181066 service.cc:145] XLA service 0x7fcf2c017510 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m I0000 00:00:1736434154.139608  181066 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:14.177894: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m 2025-01-09 20:19:14.432814: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m I0000 00:00:1736434158.273297  181066 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00008_8_emb_unit=16,learning_rate=0.0002,num_layersm=3,num_layersu=3,units_m1=7,units_u1=4,unitsm_2=39,unitsm_3=_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:19:28,077\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:28,079\tWARNING util.py:201 -- The `process_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:28,080\tWARNING util.py:201 -- Processing trial results took 1.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:28,080\tWARNING util.py:201 -- The `process_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00008_8_emb_unit=16,learning_rate=0.0002,num_layersm=3,num_layersu=3,units_m1=7,units_u1=4,unitsm_2=39,unitsm_3=_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:19:33,364\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:33,366\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:33,366\tWARNING util.py:201 -- Processing trial results took 1.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:33,367\tWARNING util.py:201 -- The `process_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00008_8_emb_unit=16,learning_rate=0.0002,num_layersm=3,num_layersu=3,units_m1=7,units_u1=4,unitsm_2=39,unitsm_3=_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:19:38,126\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:38,127\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:38,128\tWARNING util.py:201 -- Processing trial results took 1.318 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:38,129\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00008_8_emb_unit=16,learning_rate=0.0002,num_layersm=3,num_layersu=3,units_m1=7,units_u1=4,unitsm_2=39,unitsm_3=_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:19:42,959\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:42,961\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:42,961\tWARNING util.py:201 -- Processing trial results took 1.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:42,962\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=180952)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00008_8_emb_unit=16,learning_rate=0.0002,num_layersm=3,num_layersu=3,units_m1=7,units_u1=4,unitsm_2=39,unitsm_3=_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:19:47,676\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.167 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:47,677\tWARNING util.py:201 -- The `process_trial_result` operation took 1.169 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:19:47,678\tWARNING util.py:201 -- Processing trial results took 1.170 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:19:47,679\tWARNING util.py:201 -- The `process_trial_result` operation took 1.171 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:19:48 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00008 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/3e07ad537c3742b59ac36168f208828d.\n",
      "2025/01/09 20:19:48 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=181638)\u001b[0m 2025-01-09 20:19:49.565081: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=181638)\u001b[0m 2025-01-09 20:19:49.577482: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=181638)\u001b[0m 2025-01-09 20:19:49.594660: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=181638)\u001b[0m 2025-01-09 20:19:49.594698: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=181638)\u001b[0m 2025-01-09 20:19:49.606285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=181638)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=181638)\u001b[0m 2025-01-09 20:19:50.204520: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.846694: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.880169: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.880224: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.883944: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.884041: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.884062: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.979068: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.979145: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.979152: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.979177: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:51.979199: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m I0000 00:00:1736434194.060253  181748 service.cc:145] XLA service 0x7f0ea4017de0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m I0000 00:00:1736434194.060307  181748 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:54.098200: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m 2025-01-09 20:19:54.309073: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m I0000 00:00:1736434196.818056  181748 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00009_9_emb_unit=21,learning_rate=0.0050,num_layersm=3,num_layersu=3,units_m1=21,units_u1=3,unitsm_2=8,unitsm_3=_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:20:04,487\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:04,488\tWARNING util.py:201 -- The `process_trial_result` operation took 1.267 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:04,489\tWARNING util.py:201 -- Processing trial results took 1.267 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:04,489\tWARNING util.py:201 -- The `process_trial_result` operation took 1.268 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00009_9_emb_unit=21,learning_rate=0.0050,num_layersm=3,num_layersu=3,units_m1=21,units_u1=3,unitsm_2=8,unitsm_3=_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:20:09,190\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.216 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:09,192\tWARNING util.py:201 -- The `process_trial_result` operation took 1.218 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:09,193\tWARNING util.py:201 -- Processing trial results took 1.219 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:09,194\tWARNING util.py:201 -- The `process_trial_result` operation took 1.220 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00009_9_emb_unit=21,learning_rate=0.0050,num_layersm=3,num_layersu=3,units_m1=21,units_u1=3,unitsm_2=8,unitsm_3=_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:20:14,113\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:14,115\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:14,115\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:14,115\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00009_9_emb_unit=21,learning_rate=0.0050,num_layersm=3,num_layersu=3,units_m1=21,units_u1=3,unitsm_2=8,unitsm_3=_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:20:18,934\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:18,935\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:18,936\tWARNING util.py:201 -- Processing trial results took 1.350 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:18,936\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=181638)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00009_9_emb_unit=21,learning_rate=0.0050,num_layersm=3,num_layersu=3,units_m1=21,units_u1=3,unitsm_2=8,unitsm_3=_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:20:23,566\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:23,567\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:23,567\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:23,569\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:20:23 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00009 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/5dc74f070435487c8ca96f8492937ce4.\n",
      "2025/01/09 20:20:23 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=182237)\u001b[0m 2025-01-09 20:20:25.983727: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=182237)\u001b[0m 2025-01-09 20:20:25.996610: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=182237)\u001b[0m 2025-01-09 20:20:26.011082: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=182237)\u001b[0m 2025-01-09 20:20:26.011122: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=182237)\u001b[0m 2025-01-09 20:20:26.020305: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=182237)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=182237)\u001b[0m 2025-01-09 20:20:26.853891: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.859202: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.898865: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.898919: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.902577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.902630: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.902646: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.993268: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.993336: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.993343: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.993368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:28.993396: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m I0000 00:00:1736434230.988221  182349 service.cc:145] XLA service 0x7f26d00176f0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m I0000 00:00:1736434230.988273  182349 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:31.026386: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m 2025-01-09 20:20:31.244418: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m I0000 00:00:1736434235.245812  182349 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_4', 4 bytes spill stores, 4 bytes spill loads\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m \n",
      "\u001b[36m(train_model pid=182237)\u001b[0m I0000 00:00:1736434235.249874  182349 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:20:44,245\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:44,247\tWARNING util.py:201 -- The `process_trial_result` operation took 1.276 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:44,248\tWARNING util.py:201 -- Processing trial results took 1.277 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:44,248\tWARNING util.py:201 -- The `process_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:20:48,933\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:48,935\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:48,935\tWARNING util.py:201 -- Processing trial results took 1.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:48,935\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:20:53,709\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:53,710\tWARNING util.py:201 -- The `process_trial_result` operation took 1.275 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:53,711\tWARNING util.py:201 -- Processing trial results took 1.275 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:53,713\tWARNING util.py:201 -- The `process_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:20:58,272\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.296 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:58,274\tWARNING util.py:201 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:20:58,275\tWARNING util.py:201 -- Processing trial results took 1.299 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:20:58,275\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:21:03,016\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:03,017\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:03,018\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:03,019\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:21:08,157\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.451 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:08,158\tWARNING util.py:201 -- The `process_trial_result` operation took 1.452 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:08,159\tWARNING util.py:201 -- Processing trial results took 1.453 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:08,159\tWARNING util.py:201 -- The `process_trial_result` operation took 1.453 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:21:13,044\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:13,046\tWARNING util.py:201 -- The `process_trial_result` operation took 1.284 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:13,047\tWARNING util.py:201 -- Processing trial results took 1.285 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:13,047\tWARNING util.py:201 -- The `process_trial_result` operation took 1.286 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:21:17,626\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:17,628\tWARNING util.py:201 -- The `process_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:17,629\tWARNING util.py:201 -- Processing trial results took 1.258 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:17,630\tWARNING util.py:201 -- The `process_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:21:22,458\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:22,460\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:22,460\tWARNING util.py:201 -- Processing trial results took 1.333 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:22,461\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=182237)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00010_10_emb_unit=20,learning_rate=0.0010,num_layersm=3,num_layersu=3,units_m1=19,units_u1=37,unitsm_2=45,unitsm_2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:21:26,801\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.016 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:26,802\tWARNING util.py:201 -- The `process_trial_result` operation took 1.018 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:26,803\tWARNING util.py:201 -- Processing trial results took 1.019 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:26,803\tWARNING util.py:201 -- The `process_trial_result` operation took 1.019 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:21:27 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00010 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/9df8449b3b6b4703975c8386472b9c36.\n",
      "2025/01/09 20:21:27 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=183171)\u001b[0m 2025-01-09 20:21:28.536237: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=183171)\u001b[0m 2025-01-09 20:21:28.546396: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=183171)\u001b[0m 2025-01-09 20:21:28.560136: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=183171)\u001b[0m 2025-01-09 20:21:28.560173: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=183171)\u001b[0m 2025-01-09 20:21:28.568586: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=183171)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=183171)\u001b[0m 2025-01-09 20:21:29.148270: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.771316: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.790063: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.790118: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.792504: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.792555: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.792570: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.873436: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.873507: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.873513: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.873539: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:30.873559: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m I0000 00:00:1736434292.710600  183279 service.cc:145] XLA service 0x7f976c009a00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m I0000 00:00:1736434292.710655  183279 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:32.745094: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m 2025-01-09 20:21:32.948709: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m I0000 00:00:1736434296.859799  183279 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:21:45,244\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:45,246\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:45,247\tWARNING util.py:201 -- Processing trial results took 1.305 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:45,247\tWARNING util.py:201 -- The `process_trial_result` operation took 1.306 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:21:49,670\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:49,673\tWARNING util.py:201 -- The `process_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:49,674\tWARNING util.py:201 -- Processing trial results took 1.262 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:49,674\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:21:54,441\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:54,442\tWARNING util.py:201 -- The `process_trial_result` operation took 1.235 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:54,443\tWARNING util.py:201 -- Processing trial results took 1.236 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:54,444\tWARNING util.py:201 -- The `process_trial_result` operation took 1.237 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:21:58,884\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.284 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:58,885\tWARNING util.py:201 -- The `process_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:21:58,886\tWARNING util.py:201 -- Processing trial results took 1.286 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:21:58,886\tWARNING util.py:201 -- The `process_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:22:03,630\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:03,632\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:03,632\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:03,633\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:22:08,147\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:08,148\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:08,149\tWARNING util.py:201 -- Processing trial results took 1.180 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:08,150\tWARNING util.py:201 -- The `process_trial_result` operation took 1.180 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:22:13,182\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:13,184\tWARNING util.py:201 -- The `process_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:13,185\tWARNING util.py:201 -- Processing trial results took 1.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:13,185\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:22:17,834\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.434 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:17,836\tWARNING util.py:201 -- The `process_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:17,837\tWARNING util.py:201 -- Processing trial results took 1.436 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:17,838\tWARNING util.py:201 -- The `process_trial_result` operation took 1.437 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:22:22,054\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.974 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:22,055\tWARNING util.py:201 -- The `process_trial_result` operation took 0.976 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:22,056\tWARNING util.py:201 -- Processing trial results took 0.977 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:22,057\tWARNING util.py:201 -- The `process_trial_result` operation took 0.978 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=183171)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00011_11_emb_unit=19,learning_rate=0.0006,num_layersm=3,num_layersu=3,units_m1=44,units_u1=39,unitsm_2=19,unitsm_2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:22:26,724\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:26,726\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:26,726\tWARNING util.py:201 -- Processing trial results took 1.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:26,727\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:22:27 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00011 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/a1baafbcb40b45cdadf67abb542d7809.\n",
      "2025/01/09 20:22:27 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=184071)\u001b[0m 2025-01-09 20:22:29.013897: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=184071)\u001b[0m 2025-01-09 20:22:29.023605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=184071)\u001b[0m 2025-01-09 20:22:29.035991: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=184071)\u001b[0m 2025-01-09 20:22:29.036030: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=184071)\u001b[0m 2025-01-09 20:22:29.044139: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=184071)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=184071)\u001b[0m 2025-01-09 20:22:29.911064: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:31.969337: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.005293: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.005343: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.007836: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.007889: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.007905: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.098445: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.098517: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.098525: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.098552: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:32.098575: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m I0000 00:00:1736434354.049348  184182 service.cc:145] XLA service 0x7fcefc005ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m I0000 00:00:1736434354.049402  184182 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:34.086203: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m 2025-01-09 20:22:34.292322: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m I0000 00:00:1736434358.670032  184182 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:22:47,187\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.244 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:47,189\tWARNING util.py:201 -- The `process_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:47,190\tWARNING util.py:201 -- Processing trial results took 1.246 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:47,190\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:22:52,279\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:52,280\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:52,282\tWARNING util.py:201 -- Processing trial results took 1.304 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:52,282\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:22:57,657\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.413 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:57,660\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:22:57,660\tWARNING util.py:201 -- Processing trial results took 1.416 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:22:57,660\tWARNING util.py:201 -- The `process_trial_result` operation took 1.417 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:23:02,986\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.189 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:02,988\tWARNING util.py:201 -- The `process_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:02,991\tWARNING util.py:201 -- Processing trial results took 1.194 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:02,992\tWARNING util.py:201 -- The `process_trial_result` operation took 1.195 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:23:08,055\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:08,056\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:08,057\tWARNING util.py:201 -- Processing trial results took 1.318 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:08,057\tWARNING util.py:201 -- The `process_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:23:12,642\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:12,644\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:12,644\tWARNING util.py:201 -- Processing trial results took 1.350 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:12,644\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:23:16,973\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.084 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:16,975\tWARNING util.py:201 -- The `process_trial_result` operation took 1.085 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:16,976\tWARNING util.py:201 -- Processing trial results took 1.087 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:16,977\tWARNING util.py:201 -- The `process_trial_result` operation took 1.087 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:23:21,794\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:21,795\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:21,796\tWARNING util.py:201 -- Processing trial results took 1.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:21,797\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:23:26,446\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:26,448\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:26,449\tWARNING util.py:201 -- Processing trial results took 1.322 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:26,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:23:31,176\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:31,178\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:31,179\tWARNING util.py:201 -- Processing trial results took 1.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:31,179\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000010)\n",
      "2025-01-09 20:23:35,894\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.409 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:35,895\tWARNING util.py:201 -- The `process_trial_result` operation took 1.410 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:35,896\tWARNING util.py:201 -- Processing trial results took 1.411 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:35,897\tWARNING util.py:201 -- The `process_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000011)\n",
      "2025-01-09 20:23:40,404\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:40,406\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:40,406\tWARNING util.py:201 -- Processing trial results took 1.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:40,406\tWARNING util.py:201 -- The `process_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000012)\n",
      "2025-01-09 20:23:45,200\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.391 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:45,201\tWARNING util.py:201 -- The `process_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:45,202\tWARNING util.py:201 -- Processing trial results took 1.393 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:45,203\tWARNING util.py:201 -- The `process_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000013)\n",
      "2025-01-09 20:23:49,878\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:49,880\tWARNING util.py:201 -- The `process_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:49,880\tWARNING util.py:201 -- Processing trial results took 1.384 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:49,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.385 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000014)\n",
      "2025-01-09 20:23:54,395\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.196 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:54,396\tWARNING util.py:201 -- The `process_trial_result` operation took 1.197 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:54,397\tWARNING util.py:201 -- Processing trial results took 1.198 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:54,397\tWARNING util.py:201 -- The `process_trial_result` operation took 1.199 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000015)\n",
      "2025-01-09 20:23:59,042\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.280 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:59,044\tWARNING util.py:201 -- The `process_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:23:59,045\tWARNING util.py:201 -- Processing trial results took 1.283 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:23:59,046\tWARNING util.py:201 -- The `process_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000016)\n",
      "2025-01-09 20:24:03,412\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.076 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:03,414\tWARNING util.py:201 -- The `process_trial_result` operation took 1.077 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:03,414\tWARNING util.py:201 -- Processing trial results took 1.078 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:03,415\tWARNING util.py:201 -- The `process_trial_result` operation took 1.078 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000017)\n",
      "2025-01-09 20:24:07,879\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:07,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:07,881\tWARNING util.py:201 -- Processing trial results took 1.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:07,882\tWARNING util.py:201 -- The `process_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000018)\n",
      "2025-01-09 20:24:12,549\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.269 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:12,551\tWARNING util.py:201 -- The `process_trial_result` operation took 1.270 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:12,551\tWARNING util.py:201 -- Processing trial results took 1.271 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:12,552\tWARNING util.py:201 -- The `process_trial_result` operation took 1.272 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=184071)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00012_12_emb_unit=30,learning_rate=0.0022,num_layersm=3,num_layersu=3,units_m1=13,units_u1=24,unitsm_2=34,unitsm_2025-01-09_20-11-20/checkpoint_000019)\n",
      "2025-01-09 20:24:17,063\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:17,064\tWARNING util.py:201 -- The `process_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:17,065\tWARNING util.py:201 -- Processing trial results took 1.285 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:17,066\tWARNING util.py:201 -- The `process_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:24:17 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00012 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/9bd42eaed2644fc4b2c5d671ff92ac43.\n",
      "2025/01/09 20:24:17 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=185510)\u001b[0m 2025-01-09 20:24:18.970486: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=185510)\u001b[0m 2025-01-09 20:24:18.981255: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=185510)\u001b[0m 2025-01-09 20:24:18.996519: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=185510)\u001b[0m 2025-01-09 20:24:18.996568: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=185510)\u001b[0m 2025-01-09 20:24:19.006106: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=185510)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=185510)\u001b[0m 2025-01-09 20:24:19.943686: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.017976: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.054467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.054526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.057801: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.057861: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.057877: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.152497: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.152577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.152584: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.152609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:22.152635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m I0000 00:00:1736434464.219456  185625 service.cc:145] XLA service 0x7fd270012600 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m I0000 00:00:1736434464.219508  185625 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:24.254816: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m 2025-01-09 20:24:24.450792: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m I0000 00:00:1736434466.686131  185625 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00013_13_emb_unit=21,learning_rate=0.0043,num_layersm=2,num_layersu=2,units_m1=9,units_u1=8,unitsm_2=16,unitsm_3_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:24:33,379\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:33,381\tWARNING util.py:201 -- The `process_trial_result` operation took 1.438 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:33,382\tWARNING util.py:201 -- Processing trial results took 1.438 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:33,382\tWARNING util.py:201 -- The `process_trial_result` operation took 1.439 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00013_13_emb_unit=21,learning_rate=0.0043,num_layersm=2,num_layersu=2,units_m1=9,units_u1=8,unitsm_2=16,unitsm_3_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:24:37,756\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:37,757\tWARNING util.py:201 -- The `process_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:37,759\tWARNING util.py:201 -- Processing trial results took 1.302 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:37,759\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00013_13_emb_unit=21,learning_rate=0.0043,num_layersm=2,num_layersu=2,units_m1=9,units_u1=8,unitsm_2=16,unitsm_3_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:24:41,777\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.984 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:41,778\tWARNING util.py:201 -- The `process_trial_result` operation took 0.985 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:41,780\tWARNING util.py:201 -- Processing trial results took 0.987 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:41,780\tWARNING util.py:201 -- The `process_trial_result` operation took 0.987 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00013_13_emb_unit=21,learning_rate=0.0043,num_layersm=2,num_layersu=2,units_m1=9,units_u1=8,unitsm_2=16,unitsm_3_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:24:46,118\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:46,120\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:46,121\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:46,122\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=185510)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00013_13_emb_unit=21,learning_rate=0.0043,num_layersm=2,num_layersu=2,units_m1=9,units_u1=8,unitsm_2=16,unitsm_3_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:24:50,421\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.443 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:50,424\tWARNING util.py:201 -- The `process_trial_result` operation took 1.446 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:24:50,424\tWARNING util.py:201 -- Processing trial results took 1.446 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:24:50,425\tWARNING util.py:201 -- The `process_trial_result` operation took 1.447 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:24:50 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00013 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/e59fb5ecf9cf4e648467a69c56d0cc35.\n",
      "2025/01/09 20:24:50 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=186076)\u001b[0m 2025-01-09 20:24:53.012445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=186076)\u001b[0m 2025-01-09 20:24:53.023403: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=186076)\u001b[0m 2025-01-09 20:24:53.039223: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=186076)\u001b[0m 2025-01-09 20:24:53.039265: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=186076)\u001b[0m 2025-01-09 20:24:53.048940: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=186076)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=186076)\u001b[0m 2025-01-09 20:24:53.886387: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.669054: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.702907: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.702970: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.705031: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.705083: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.705099: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.788742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.788815: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.788822: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.788848: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:55.788872: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m I0000 00:00:1736434497.748122  186189 service.cc:145] XLA service 0x7f6b7c016780 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m I0000 00:00:1736434497.748173  186189 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:57.803519: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m 2025-01-09 20:24:58.040539: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m I0000 00:00:1736434500.164122  186189 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00014_14_emb_unit=31,learning_rate=0.0060,num_layersm=2,num_layersu=3,units_m1=20,units_u1=44,unitsm_2=1,unitsm__2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:25:07,445\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:07,447\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:07,447\tWARNING util.py:201 -- Processing trial results took 1.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:07,447\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00014_14_emb_unit=31,learning_rate=0.0060,num_layersm=2,num_layersu=3,units_m1=20,units_u1=44,unitsm_2=1,unitsm__2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:25:11,542\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.100 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:11,544\tWARNING util.py:201 -- The `process_trial_result` operation took 1.102 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:11,545\tWARNING util.py:201 -- Processing trial results took 1.103 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:11,546\tWARNING util.py:201 -- The `process_trial_result` operation took 1.104 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00014_14_emb_unit=31,learning_rate=0.0060,num_layersm=2,num_layersu=3,units_m1=20,units_u1=44,unitsm_2=1,unitsm__2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:25:15,878\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:15,880\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:15,880\tWARNING util.py:201 -- Processing trial results took 1.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:15,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00014_14_emb_unit=31,learning_rate=0.0060,num_layersm=2,num_layersu=3,units_m1=20,units_u1=44,unitsm_2=1,unitsm__2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:25:20,309\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.313 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:20,310\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:20,311\tWARNING util.py:201 -- Processing trial results took 1.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:20,311\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186076)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00014_14_emb_unit=31,learning_rate=0.0060,num_layersm=2,num_layersu=3,units_m1=20,units_u1=44,unitsm_2=1,unitsm__2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:25:24,598\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:24,599\tWARNING util.py:201 -- The `process_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:24,600\tWARNING util.py:201 -- Processing trial results took 1.262 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:24,601\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:25:24 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00014 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/4eea8432a2d845be8e12ca010902b177.\n",
      "2025/01/09 20:25:24 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=186669)\u001b[0m 2025-01-09 20:25:26.525775: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=186669)\u001b[0m 2025-01-09 20:25:26.536601: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=186669)\u001b[0m 2025-01-09 20:25:26.552770: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=186669)\u001b[0m 2025-01-09 20:25:26.552811: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=186669)\u001b[0m 2025-01-09 20:25:26.562958: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=186669)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=186669)\u001b[0m 2025-01-09 20:25:27.128147: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.787042: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.813668: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.813724: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.816180: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.816231: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.816251: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.910882: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.910963: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.910970: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.910996: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:28.911019: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m I0000 00:00:1736434530.698594  186780 service.cc:145] XLA service 0x7f7644005000 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m I0000 00:00:1736434530.698649  186780 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:30.730929: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m 2025-01-09 20:25:30.910291: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m I0000 00:00:1736434534.486029  186780 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_5', 4 bytes spill stores, 4 bytes spill loads\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m \n",
      "\u001b[36m(train_model pid=186669)\u001b[0m I0000 00:00:1736434534.489532  186780 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m I0000 00:00:1736434539.454253  186777 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m \n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:25:42,348\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.009 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:42,349\tWARNING util.py:201 -- The `process_trial_result` operation took 1.011 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:42,350\tWARNING util.py:201 -- Processing trial results took 1.011 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:42,350\tWARNING util.py:201 -- The `process_trial_result` operation took 1.012 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:25:47,051\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:47,052\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:47,053\tWARNING util.py:201 -- Processing trial results took 1.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:47,054\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:25:51,623\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:51,625\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:51,626\tWARNING util.py:201 -- Processing trial results took 1.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:51,626\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:25:55,969\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.995 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:55,971\tWARNING util.py:201 -- The `process_trial_result` operation took 0.996 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:25:55,972\tWARNING util.py:201 -- Processing trial results took 0.997 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:25:55,972\tWARNING util.py:201 -- The `process_trial_result` operation took 0.998 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:26:00,659\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.418 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:00,661\tWARNING util.py:201 -- The `process_trial_result` operation took 1.420 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:00,662\tWARNING util.py:201 -- Processing trial results took 1.421 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:00,663\tWARNING util.py:201 -- The `process_trial_result` operation took 1.423 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:26:05,061\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.322 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:05,062\tWARNING util.py:201 -- The `process_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:05,063\tWARNING util.py:201 -- Processing trial results took 1.324 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:05,064\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:26:09,757\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:09,759\tWARNING util.py:201 -- The `process_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:09,760\tWARNING util.py:201 -- Processing trial results took 1.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:09,760\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:26:14,396\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.023 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:14,398\tWARNING util.py:201 -- The `process_trial_result` operation took 1.025 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:14,399\tWARNING util.py:201 -- Processing trial results took 1.027 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:14,400\tWARNING util.py:201 -- The `process_trial_result` operation took 1.027 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:26:19,198\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.459 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:19,200\tWARNING util.py:201 -- The `process_trial_result` operation took 1.461 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:19,201\tWARNING util.py:201 -- Processing trial results took 1.462 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:19,202\tWARNING util.py:201 -- The `process_trial_result` operation took 1.463 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=186669)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00015_15_emb_unit=27,learning_rate=0.0019,num_layersm=3,num_layersu=2,units_m1=25,units_u1=34,unitsm_2=23,unitsm_2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:26:23,901\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.443 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:23,902\tWARNING util.py:201 -- The `process_trial_result` operation took 1.445 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:23,903\tWARNING util.py:201 -- Processing trial results took 1.446 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:23,903\tWARNING util.py:201 -- The `process_trial_result` operation took 1.446 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:26:24 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00015 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/a5ed8429dade464f9308f73e08d05c65.\n",
      "2025/01/09 20:26:24 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=187590)\u001b[0m 2025-01-09 20:26:25.974429: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=187590)\u001b[0m 2025-01-09 20:26:25.984343: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=187590)\u001b[0m 2025-01-09 20:26:25.999010: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=187590)\u001b[0m 2025-01-09 20:26:25.999048: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=187590)\u001b[0m 2025-01-09 20:26:26.007980: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=187590)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=187590)\u001b[0m 2025-01-09 20:26:26.831735: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:28.873823: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:28.913716: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:28.913768: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:28.916802: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:28.916864: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:28.916881: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:29.001490: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:29.001574: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:29.001582: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:29.001612: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:29.001635: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m I0000 00:00:1736434590.909714  187700 service.cc:145] XLA service 0x7fd3b0012180 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m I0000 00:00:1736434590.909760  187700 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:30.943473: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m 2025-01-09 20:26:31.148021: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m I0000 00:00:1736434593.712061  187700 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00016_16_emb_unit=19,learning_rate=0.0020,num_layersm=2,num_layersu=3,units_m1=21,units_u1=15,unitsm_2=33,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:26:40,519\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:40,520\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:40,521\tWARNING util.py:201 -- Processing trial results took 1.316 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:40,521\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00016_16_emb_unit=19,learning_rate=0.0020,num_layersm=2,num_layersu=3,units_m1=21,units_u1=15,unitsm_2=33,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:26:44,900\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.129 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:44,901\tWARNING util.py:201 -- The `process_trial_result` operation took 1.131 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:44,902\tWARNING util.py:201 -- Processing trial results took 1.131 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:44,902\tWARNING util.py:201 -- The `process_trial_result` operation took 1.132 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00016_16_emb_unit=19,learning_rate=0.0020,num_layersm=2,num_layersu=3,units_m1=21,units_u1=15,unitsm_2=33,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:26:49,378\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.439 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:49,379\tWARNING util.py:201 -- The `process_trial_result` operation took 1.441 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:49,380\tWARNING util.py:201 -- Processing trial results took 1.442 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:49,381\tWARNING util.py:201 -- The `process_trial_result` operation took 1.442 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00016_16_emb_unit=19,learning_rate=0.0020,num_layersm=2,num_layersu=3,units_m1=21,units_u1=15,unitsm_2=33,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:26:53,794\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:53,795\tWARNING util.py:201 -- The `process_trial_result` operation took 1.414 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:53,796\tWARNING util.py:201 -- Processing trial results took 1.414 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:53,797\tWARNING util.py:201 -- The `process_trial_result` operation took 1.415 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=187590)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00016_16_emb_unit=19,learning_rate=0.0020,num_layersm=2,num_layersu=3,units_m1=21,units_u1=15,unitsm_2=33,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:26:58,190\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:58,191\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:26:58,192\tWARNING util.py:201 -- Processing trial results took 1.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:26:58,193\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:26:58 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00016 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/e87ef23889274a228306bb25ddd12055.\n",
      "2025/01/09 20:26:58 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=188162)\u001b[0m 2025-01-09 20:27:00.555204: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=188162)\u001b[0m 2025-01-09 20:27:00.566682: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=188162)\u001b[0m 2025-01-09 20:27:00.583730: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=188162)\u001b[0m 2025-01-09 20:27:00.583771: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=188162)\u001b[0m 2025-01-09 20:27:00.592553: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=188162)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=188162)\u001b[0m 2025-01-09 20:27:01.202284: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.758060: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.786390: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.786450: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.789930: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.790005: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.790026: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.882321: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.882391: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.882432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.882473: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:02.882493: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m I0000 00:00:1736434624.549003  188272 service.cc:145] XLA service 0x7fabec002e30 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m I0000 00:00:1736434624.549052  188272 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:04.576875: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m 2025-01-09 20:27:04.737613: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m I0000 00:00:1736434627.524882  188272 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:27:14,396\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:14,398\tWARNING util.py:201 -- The `process_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:14,398\tWARNING util.py:201 -- Processing trial results took 1.262 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:14,399\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:27:19,149\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:19,151\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:19,151\tWARNING util.py:201 -- Processing trial results took 1.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:19,152\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:27:23,503\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:23,504\tWARNING util.py:201 -- The `process_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:23,505\tWARNING util.py:201 -- Processing trial results took 1.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:23,506\tWARNING util.py:201 -- The `process_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:27:28,006\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.427 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:28,007\tWARNING util.py:201 -- The `process_trial_result` operation took 1.428 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:28,008\tWARNING util.py:201 -- Processing trial results took 1.430 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:28,009\tWARNING util.py:201 -- The `process_trial_result` operation took 1.430 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:27:32,181\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.081 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:32,183\tWARNING util.py:201 -- The `process_trial_result` operation took 1.083 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:32,183\tWARNING util.py:201 -- Processing trial results took 1.084 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:32,184\tWARNING util.py:201 -- The `process_trial_result` operation took 1.084 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000005)\n",
      "2025-01-09 20:27:36,549\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.420 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:36,551\tWARNING util.py:201 -- The `process_trial_result` operation took 1.422 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:36,552\tWARNING util.py:201 -- Processing trial results took 1.423 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:36,552\tWARNING util.py:201 -- The `process_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000006)\n",
      "2025-01-09 20:27:41,035\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.442 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:41,037\tWARNING util.py:201 -- The `process_trial_result` operation took 1.443 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:41,038\tWARNING util.py:201 -- Processing trial results took 1.444 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:41,038\tWARNING util.py:201 -- The `process_trial_result` operation took 1.445 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000007)\n",
      "2025-01-09 20:27:45,317\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.183 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:45,319\tWARNING util.py:201 -- The `process_trial_result` operation took 1.185 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:45,320\tWARNING util.py:201 -- Processing trial results took 1.185 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:45,320\tWARNING util.py:201 -- The `process_trial_result` operation took 1.186 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000008)\n",
      "2025-01-09 20:27:49,727\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:49,728\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:49,728\tWARNING util.py:201 -- Processing trial results took 1.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:49,729\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=188162)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00017_17_emb_unit=21,learning_rate=0.0050,num_layersm=2,num_layersu=2,units_m1=38,units_u1=38,unitsm_2=25,unitsm_2025-01-09_20-11-20/checkpoint_000009)\n",
      "2025-01-09 20:27:54,258\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.447 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:54,260\tWARNING util.py:201 -- The `process_trial_result` operation took 1.450 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:27:54,262\tWARNING util.py:201 -- Processing trial results took 1.451 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:27:54,263\tWARNING util.py:201 -- The `process_trial_result` operation took 1.452 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:27:54 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00017 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/20f0f16d74d0440ca6980fd1377545c6.\n",
      "2025/01/09 20:27:54 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=189011)\u001b[0m 2025-01-09 20:27:56.992054: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=189011)\u001b[0m 2025-01-09 20:27:57.004186: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=189011)\u001b[0m 2025-01-09 20:27:57.021459: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=189011)\u001b[0m 2025-01-09 20:27:57.021505: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=189011)\u001b[0m 2025-01-09 20:27:57.031950: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=189011)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=189011)\u001b[0m 2025-01-09 20:27:57.886484: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.853102: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.887005: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.887061: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.889549: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.889604: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.889623: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.976052: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.976120: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.976127: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.976153: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:27:59.976173: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m I0000 00:00:1736434681.809543  189124 service.cc:145] XLA service 0x7f5050005ce0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m I0000 00:00:1736434681.809591  189124 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:28:01.840584: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m 2025-01-09 20:28:02.030546: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m I0000 00:00:1736434684.833829  189124 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00018_18_emb_unit=27,learning_rate=0.0054,num_layersm=2,num_layersu=2,units_m1=9,units_u1=36,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:28:12,401\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.184 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:12,402\tWARNING util.py:201 -- The `process_trial_result` operation took 1.186 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:12,403\tWARNING util.py:201 -- Processing trial results took 1.186 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:12,403\tWARNING util.py:201 -- The `process_trial_result` operation took 1.187 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00018_18_emb_unit=27,learning_rate=0.0054,num_layersm=2,num_layersu=2,units_m1=9,units_u1=36,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:28:16,700\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.986 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:16,701\tWARNING util.py:201 -- The `process_trial_result` operation took 0.988 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:16,702\tWARNING util.py:201 -- Processing trial results took 0.989 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:16,703\tWARNING util.py:201 -- The `process_trial_result` operation took 0.990 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00018_18_emb_unit=27,learning_rate=0.0054,num_layersm=2,num_layersu=2,units_m1=9,units_u1=36,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:28:21,207\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:21,209\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:21,209\tWARNING util.py:201 -- Processing trial results took 1.336 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:21,210\tWARNING util.py:201 -- The `process_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00018_18_emb_unit=27,learning_rate=0.0054,num_layersm=2,num_layersu=2,units_m1=9,units_u1=36,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:28:25,729\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:25,731\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:25,732\tWARNING util.py:201 -- Processing trial results took 1.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:25,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189011)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00018_18_emb_unit=27,learning_rate=0.0054,num_layersm=2,num_layersu=2,units_m1=9,units_u1=36,unitsm_2=1,unitsm_3_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:28:30,021\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.193 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:30,023\tWARNING util.py:201 -- The `process_trial_result` operation took 1.196 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:30,023\tWARNING util.py:201 -- Processing trial results took 1.196 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:30,024\tWARNING util.py:201 -- The `process_trial_result` operation took 1.197 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:28:30 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00018 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/7011fdb1d65042d9acfb6a30ca2cb50a.\n",
      "2025/01/09 20:28:30 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=189633)\u001b[0m 2025-01-09 20:28:31.689197: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=189633)\u001b[0m 2025-01-09 20:28:31.698968: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=189633)\u001b[0m 2025-01-09 20:28:31.712971: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=189633)\u001b[0m 2025-01-09 20:28:31.713011: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=189633)\u001b[0m 2025-01-09 20:28:31.721885: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=189633)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=189633)\u001b[0m 2025-01-09 20:28:32.376076: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.166213: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.191146: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.191204: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.193700: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.193750: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.193768: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.282143: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.282217: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.282224: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.282250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:34.282272: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m I0000 00:00:1736434716.111285  189744 service.cc:145] XLA service 0x7fc128004e40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m I0000 00:00:1736434716.111332  189744 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:36.143336: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m 2025-01-09 20:28:36.334006: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m I0000 00:00:1736434718.436981  189744 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00019_19_emb_unit=16,learning_rate=0.0004,num_layersm=3,num_layersu=2,units_m1=36,units_u1=42,unitsm_2=30,unitsm_2025-01-09_20-11-20/checkpoint_000000)\n",
      "2025-01-09 20:28:44,982\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.187 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:44,984\tWARNING util.py:201 -- The `process_trial_result` operation took 1.188 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:44,984\tWARNING util.py:201 -- Processing trial results took 1.189 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:44,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.190 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00019_19_emb_unit=16,learning_rate=0.0004,num_layersm=3,num_layersu=2,units_m1=36,units_u1=42,unitsm_2=30,unitsm_2025-01-09_20-11-20/checkpoint_000001)\n",
      "2025-01-09 20:28:49,719\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:49,720\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:49,721\tWARNING util.py:201 -- Processing trial results took 1.349 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:49,721\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00019_19_emb_unit=16,learning_rate=0.0004,num_layersm=3,num_layersu=2,units_m1=36,units_u1=42,unitsm_2=30,unitsm_2025-01-09_20-11-20/checkpoint_000002)\n",
      "2025-01-09 20:28:54,323\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.320 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:54,324\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:54,326\tWARNING util.py:201 -- Processing trial results took 1.323 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:54,327\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00019_19_emb_unit=16,learning_rate=0.0004,num_layersm=3,num_layersu=2,units_m1=36,units_u1=42,unitsm_2=30,unitsm_2025-01-09_20-11-20/checkpoint_000003)\n",
      "2025-01-09 20:28:58,759\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:58,761\tWARNING util.py:201 -- The `process_trial_result` operation took 1.236 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:28:58,762\tWARNING util.py:201 -- Processing trial results took 1.237 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:28:58,762\tWARNING util.py:201 -- The `process_trial_result` operation took 1.237 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=189633)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah/train_model_ca4cc_00019_19_emb_unit=16,learning_rate=0.0004,num_layersm=3,num_layersu=2,units_m1=36,units_u1=42,unitsm_2=30,unitsm_2025-01-09_20-11-20/checkpoint_000004)\n",
      "2025-01-09 20:29:03,306\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.160 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:29:03,307\tWARNING util.py:201 -- The `process_trial_result` operation took 1.161 s, which may be a performance bottleneck.\n",
      "2025-01-09 20:29:03,308\tWARNING util.py:201 -- Processing trial results took 1.162 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 20:29:03,309\tWARNING util.py:201 -- The `process_trial_result` operation took 1.163 s, which may be a performance bottleneck.\n",
      "2025/01/09 20:29:03 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_ca4cc_00019 at: http://127.0.0.1:5000/#/experiments/576481206930531660/runs/9395196c906344d69525284944f8ebbd.\n",
      "2025/01/09 20:29:03 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/576481206930531660.\n",
      "2025-01-09 20:29:03,742\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/agah' in 0.0053s.\n",
      "2025-01-09 20:29:03,749\tINFO tune.py:1041 -- Total run time: 1063.64 seconds (1063.33 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    \"units_u1\": tune.randint(1, 50),\n",
    "    \"units_m1\": tune.randint(1, 50),\n",
    "    \"num_layersu\": tune.randint(2, 4),\n",
    "    \"num_layersm\": tune.randint(2, 4),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"emb_unit\":tune.randint(16,32)\n",
    "}\n",
    "for i in range(2, 5):  # Add units for layers dynamically based on max num_layers\n",
    "    search_space[f'unitsu_{i}'] = tune.randint(1, 50)\n",
    "    search_space[f'unitsm_{i}'] = tune.randint(1, 50)\n",
    "\n",
    "# Set up ASHA Scheduler\n",
    "scheduler = ASHAScheduler(\n",
    "    metric=\"mse\",\n",
    "    mode=\"min\",\n",
    "    max_t=50,\n",
    "    grace_period=5,\n",
    "    reduction_factor=2\n",
    ")\n",
    "\n",
    "# Configure the Tuner and resources\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {\"cpu\": 10, \"gpu\": 1}),\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=20,\n",
    "        scheduler=scheduler\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "            name=\"agah\",\n",
    "            callbacks=[\n",
    "                MLflowLoggerCallback(\n",
    "                    tracking_uri=\"http://127.0.0.1:5000\",\n",
    "                    experiment_name=\"movie_recommender_7\"  # Databricks MLflow experiment name\n",
    "                )\n",
    "            ]\n",
    "        )\n",
    ")\n",
    "\n",
    "# Start tuning and get results\n",
    "analysis = tuner.fit()\n",
    "#print(\"Best hyperparameters found were: \", analysis.get_best_result().config)\n",
    "#print(\"All trial results as a DataFrame:\\n\", analysis.get_dataframe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ff87d916-3db0-4600-9b2f-62a4c08010f3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div class=\"tuneStatus\">\n",
       "  <div style=\"display: flex;flex-direction: row\">\n",
       "    <div style=\"display: flex;flex-direction: column;\">\n",
       "      <h3>Tune Status</h3>\n",
       "      <table>\n",
       "<tbody>\n",
       "<tr><td>Current time:</td><td>2025-01-09 21:40:38</td></tr>\n",
       "<tr><td>Running for: </td><td>00:36:01.08        </td></tr>\n",
       "<tr><td>Memory:      </td><td>6.8/7.6 GiB        </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "    <div class=\"systemInfo\">\n",
       "      <h3>System Info</h3>\n",
       "      Using FIFO scheduling algorithm.<br>Logical resource usage: 10.0/12 CPUs, 1.0/1 GPUs (0.0/1.0 accelerator_type:G)\n",
       "    </div>\n",
       "    <div class=\"vDivider\"></div>\n",
       "<div class=\"messages\">\n",
       "  <h3>Messages</h3>\n",
       "  : ***LOW MEMORY*** less than 10% of the memory on this node is available for use. This can cause unexpected crashes. Consider reducing the memory used by your application or reducing the Ray object store size by setting `object_store_memory` when calling `ray.init`.\n",
       "  \n",
       "  \n",
       "</div>\n",
       "<style>\n",
       ".messages {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  padding-left: 1em;\n",
       "  overflow-y: auto;\n",
       "}\n",
       ".messages h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n",
       "\n",
       "  </div>\n",
       "  <div class=\"hDivider\"></div>\n",
       "  <div class=\"trialStatus\">\n",
       "    <h3>Trial Status</h3>\n",
       "    <table>\n",
       "<thead>\n",
       "<tr><th>Trial name          </th><th>status    </th><th>loc                  </th><th style=\"text-align: right;\">  emb_unit</th><th style=\"text-align: right;\">  learning_rate</th><th style=\"text-align: right;\">  num_layersm</th><th style=\"text-align: right;\">  num_layersu</th><th style=\"text-align: right;\">  units_m1</th><th style=\"text-align: right;\">  units_u1</th><th style=\"text-align: right;\">  unitsm_2</th><th style=\"text-align: right;\">  unitsm_3</th><th style=\"text-align: right;\">  unitsm_4</th><th style=\"text-align: right;\">  unitsu_2</th><th style=\"text-align: right;\">  unitsu_3</th><th style=\"text-align: right;\">  unitsu_4</th><th style=\"text-align: right;\">  iter</th><th style=\"text-align: right;\">  total time (s)</th><th style=\"text-align: right;\">  r2_score</th><th style=\"text-align: right;\">      mse</th></tr>\n",
       "</thead>\n",
       "<tbody>\n",
       "<tr><td>train_model_4aa3b002</td><td>TERMINATED</td><td>172.21.168.162:202448</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    0.000215167</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">        41</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">        26</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         75.1225</td><td style=\"text-align: right;\">  0.354507</td><td style=\"text-align: right;\">0.116559 </td></tr>\n",
       "<tr><td>train_model_cd8a920e</td><td>TERMINATED</td><td>172.21.168.162:203996</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    0.00326712 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        14</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">        41</td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         74.3569</td><td style=\"text-align: right;\">  0.392103</td><td style=\"text-align: right;\">0.10977  </td></tr>\n",
       "<tr><td>train_model_8c7e937e</td><td>TERMINATED</td><td>172.21.168.162:205389</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">    0.000375496</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         71.9984</td><td style=\"text-align: right;\">  0.455692</td><td style=\"text-align: right;\">0.098288 </td></tr>\n",
       "<tr><td>train_model_14b90495</td><td>TERMINATED</td><td>172.21.168.162:206849</td><td style=\"text-align: right;\">        26</td><td style=\"text-align: right;\">    0.000384754</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        18</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">        23</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         72.6127</td><td style=\"text-align: right;\">  0.430326</td><td style=\"text-align: right;\">0.102868 </td></tr>\n",
       "<tr><td>train_model_f1fe9322</td><td>TERMINATED</td><td>172.21.168.162:208191</td><td style=\"text-align: right;\">        26</td><td style=\"text-align: right;\">    0.00407952 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        43</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        41</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         73.6457</td><td style=\"text-align: right;\">  0.411261</td><td style=\"text-align: right;\">0.106311 </td></tr>\n",
       "<tr><td>train_model_2a680cce</td><td>TERMINATED</td><td>172.21.168.162:209576</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">    0.00842283 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        31</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        35</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         78.1041</td><td style=\"text-align: right;\">  0.403728</td><td style=\"text-align: right;\">0.107671 </td></tr>\n",
       "<tr><td>train_model_8c364857</td><td>TERMINATED</td><td>172.21.168.162:211059</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    0.0035435  </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">         7</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         69.4158</td><td style=\"text-align: right;\">  0.348322</td><td style=\"text-align: right;\">0.117676 </td></tr>\n",
       "<tr><td>train_model_395fc625</td><td>TERMINATED</td><td>172.21.168.162:212437</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    0.00596157 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        16</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         74.989 </td><td style=\"text-align: right;\">  0.366439</td><td style=\"text-align: right;\">0.114405 </td></tr>\n",
       "<tr><td>train_model_3250a14a</td><td>TERMINATED</td><td>172.21.168.162:213792</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">    0.000489734</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">        42</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         77.7088</td><td style=\"text-align: right;\">  0.447989</td><td style=\"text-align: right;\">0.0996792</td></tr>\n",
       "<tr><td>train_model_b673c549</td><td>TERMINATED</td><td>172.21.168.162:215253</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">    0.000286694</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">         6</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         81.0001</td><td style=\"text-align: right;\">  0.392051</td><td style=\"text-align: right;\">0.10978  </td></tr>\n",
       "<tr><td>train_model_fe897ff4</td><td>TERMINATED</td><td>172.21.168.162:216718</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">    0.000187969</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        18</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        42</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">        37</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         77.8015</td><td style=\"text-align: right;\">  0.334178</td><td style=\"text-align: right;\">0.12023  </td></tr>\n",
       "<tr><td>train_model_da84f091</td><td>TERMINATED</td><td>172.21.168.162:218161</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">    0.00300595 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">        19</td><td style=\"text-align: right;\">        11</td><td style=\"text-align: right;\">         3</td><td style=\"text-align: right;\">        47</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        36</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         79.3214</td><td style=\"text-align: right;\">  0.344298</td><td style=\"text-align: right;\">0.118403 </td></tr>\n",
       "<tr><td>train_model_6aede8f1</td><td>TERMINATED</td><td>172.21.168.162:219591</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    0.00640891 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">        49</td><td style=\"text-align: right;\">         6</td><td style=\"text-align: right;\">        21</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">         1</td><td style=\"text-align: right;\">        38</td><td style=\"text-align: right;\">        42</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         70.1155</td><td style=\"text-align: right;\">  0.349599</td><td style=\"text-align: right;\">0.117446 </td></tr>\n",
       "<tr><td>train_model_a0675423</td><td>TERMINATED</td><td>172.21.168.162:220913</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">    0.000382932</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">        39</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         76.5494</td><td style=\"text-align: right;\">  0.429442</td><td style=\"text-align: right;\">0.103028 </td></tr>\n",
       "<tr><td>train_model_33d1ecd6</td><td>TERMINATED</td><td>172.21.168.162:222319</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    0.00394925 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        41</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">        20</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         71.5094</td><td style=\"text-align: right;\">  0.411787</td><td style=\"text-align: right;\">0.106216 </td></tr>\n",
       "<tr><td>train_model_a64c6e17</td><td>TERMINATED</td><td>172.21.168.162:223657</td><td style=\"text-align: right;\">        28</td><td style=\"text-align: right;\">    0.000124099</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">         9</td><td style=\"text-align: right;\">        27</td><td style=\"text-align: right;\">        46</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        18</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         78.907 </td><td style=\"text-align: right;\">  0.324155</td><td style=\"text-align: right;\">0.12204  </td></tr>\n",
       "<tr><td>train_model_2bd8f9f3</td><td>TERMINATED</td><td>172.21.168.162:225116</td><td style=\"text-align: right;\">        17</td><td style=\"text-align: right;\">    0.000685301</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">        35</td><td style=\"text-align: right;\">         8</td><td style=\"text-align: right;\">         4</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">        40</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         78.0344</td><td style=\"text-align: right;\">  0.385599</td><td style=\"text-align: right;\">0.110945 </td></tr>\n",
       "<tr><td>train_model_fb38c247</td><td>TERMINATED</td><td>172.21.168.162:226560</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">    0.00776169 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        32</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">        43</td><td style=\"text-align: right;\">        44</td><td style=\"text-align: right;\">        42</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">        33</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         76.7461</td><td style=\"text-align: right;\">  0.449966</td><td style=\"text-align: right;\">0.099322 </td></tr>\n",
       "<tr><td>train_model_40645985</td><td>TERMINATED</td><td>172.21.168.162:227974</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">    0.00359241 </td><td style=\"text-align: right;\">            2</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        22</td><td style=\"text-align: right;\">        48</td><td style=\"text-align: right;\">        34</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">         5</td><td style=\"text-align: right;\">        47</td><td style=\"text-align: right;\">        12</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         83.3048</td><td style=\"text-align: right;\">  0.441776</td><td style=\"text-align: right;\">0.100801 </td></tr>\n",
       "<tr><td>train_model_1be74f9e</td><td>TERMINATED</td><td>172.21.168.162:229444</td><td style=\"text-align: right;\">        29</td><td style=\"text-align: right;\">    0.00417077 </td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">            3</td><td style=\"text-align: right;\">        15</td><td style=\"text-align: right;\">        24</td><td style=\"text-align: right;\">        35</td><td style=\"text-align: right;\">        25</td><td style=\"text-align: right;\">        42</td><td style=\"text-align: right;\">        10</td><td style=\"text-align: right;\">        13</td><td style=\"text-align: right;\">        45</td><td style=\"text-align: right;\">    20</td><td style=\"text-align: right;\">         85.6131</td><td style=\"text-align: right;\">  0.4305  </td><td style=\"text-align: right;\">0.102837 </td></tr>\n",
       "</tbody>\n",
       "</table>\n",
       "  </div>\n",
       "</div>\n",
       "<style>\n",
       ".tuneStatus {\n",
       "  color: var(--jp-ui-font-color1);\n",
       "}\n",
       ".tuneStatus .systemInfo {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus td {\n",
       "  white-space: nowrap;\n",
       "}\n",
       ".tuneStatus .trialStatus {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       ".tuneStatus h3 {\n",
       "  font-weight: bold;\n",
       "}\n",
       ".tuneStatus .hDivider {\n",
       "  border-bottom-width: var(--jp-border-width);\n",
       "  border-bottom-color: var(--jp-border-color0);\n",
       "  border-bottom-style: solid;\n",
       "}\n",
       ".tuneStatus .vDivider {\n",
       "  border-left-width: var(--jp-border-width);\n",
       "  border-left-color: var(--jp-border-color0);\n",
       "  border-left-style: solid;\n",
       "  margin: 0.5em 1em 0.5em 1em;\n",
       "}\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=202448)\u001b[0m 2025-01-09 21:04:38.374624: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=202448)\u001b[0m 2025-01-09 21:04:38.386109: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=202448)\u001b[0m 2025-01-09 21:04:38.405174: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=202448)\u001b[0m 2025-01-09 21:04:38.405224: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=202448)\u001b[0m 2025-01-09 21:04:38.416788: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=202448)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=202448)\u001b[0m 2025-01-09 21:04:39.309840: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.391092: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.559823: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.559877: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.564128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.564188: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.564205: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.695058: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.695132: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.695138: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.695165: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:41.695228: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1459 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m I0000 00:00:1736436884.529600  202604 service.cc:145] XLA service 0x7f5ab8002c80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m I0000 00:00:1736436884.529683  202604 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:44.564783: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m 2025-01-09 21:04:44.781259: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m I0000 00:00:1736436888.179148  202604 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000000)\n",
      "2025-01-09 21:04:57,133\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.134 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:04:57,135\tWARNING util.py:201 -- The `process_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:04:57,136\tWARNING util.py:201 -- Processing trial results took 1.137 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:04:57,137\tWARNING util.py:201 -- The `process_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000001)\n",
      "2025-01-09 21:05:01,804\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.320 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:01,806\tWARNING util.py:201 -- The `process_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:01,806\tWARNING util.py:201 -- Processing trial results took 1.323 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:01,807\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000002)\n",
      "2025-01-09 21:05:06,569\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:06,570\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:06,571\tWARNING util.py:201 -- Processing trial results took 1.311 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:06,572\tWARNING util.py:201 -- The `process_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000003)\n",
      "2025-01-09 21:05:11,264\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:11,266\tWARNING util.py:201 -- The `process_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:11,266\tWARNING util.py:201 -- Processing trial results took 1.377 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:11,268\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000004)\n",
      "2025-01-09 21:05:16,119\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.467 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:16,121\tWARNING util.py:201 -- The `process_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:16,121\tWARNING util.py:201 -- Processing trial results took 1.469 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:16,122\tWARNING util.py:201 -- The `process_trial_result` operation took 1.469 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000005)\n",
      "2025-01-09 21:05:20,523\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:20,525\tWARNING util.py:201 -- The `process_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:20,526\tWARNING util.py:201 -- Processing trial results took 1.302 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:20,526\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000006)\n",
      "2025-01-09 21:05:24,898\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:24,900\tWARNING util.py:201 -- The `process_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:24,901\tWARNING util.py:201 -- Processing trial results took 1.382 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:24,902\tWARNING util.py:201 -- The `process_trial_result` operation took 1.383 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000007)\n",
      "2025-01-09 21:05:29,262\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:29,263\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:29,264\tWARNING util.py:201 -- Processing trial results took 1.333 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:29,265\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000008)\n",
      "2025-01-09 21:05:33,670\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.242 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:33,671\tWARNING util.py:201 -- The `process_trial_result` operation took 1.244 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:33,672\tWARNING util.py:201 -- Processing trial results took 1.245 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:33,673\tWARNING util.py:201 -- The `process_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000009)\n",
      "2025-01-09 21:05:37,896\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.205 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:37,898\tWARNING util.py:201 -- The `process_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:37,898\tWARNING util.py:201 -- Processing trial results took 1.207 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:37,898\tWARNING util.py:201 -- The `process_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000010)\n",
      "2025-01-09 21:05:42,238\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:42,239\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:42,240\tWARNING util.py:201 -- Processing trial results took 1.317 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:42,241\tWARNING util.py:201 -- The `process_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000011)\n",
      "2025-01-09 21:05:47,045\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.431 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:47,047\tWARNING util.py:201 -- The `process_trial_result` operation took 1.433 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:47,047\tWARNING util.py:201 -- Processing trial results took 1.434 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:47,048\tWARNING util.py:201 -- The `process_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000012)\n",
      "2025-01-09 21:05:51,764\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.386 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:51,766\tWARNING util.py:201 -- The `process_trial_result` operation took 1.388 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:51,766\tWARNING util.py:201 -- Processing trial results took 1.388 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:51,767\tWARNING util.py:201 -- The `process_trial_result` operation took 1.389 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000013)\n",
      "2025-01-09 21:05:56,305\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:56,308\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:05:56,308\tWARNING util.py:201 -- Processing trial results took 1.338 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:05:56,309\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000014)\n",
      "2025-01-09 21:06:01,060\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.465 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:01,061\tWARNING util.py:201 -- The `process_trial_result` operation took 1.467 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:01,062\tWARNING util.py:201 -- Processing trial results took 1.467 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:01,063\tWARNING util.py:201 -- The `process_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000015)\n",
      "2025-01-09 21:06:05,738\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.441 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:05,740\tWARNING util.py:201 -- The `process_trial_result` operation took 1.442 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:05,741\tWARNING util.py:201 -- Processing trial results took 1.443 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:05,742\tWARNING util.py:201 -- The `process_trial_result` operation took 1.444 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000016)\n",
      "2025-01-09 21:06:10,065\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:10,066\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:10,067\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:10,068\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000017)\n",
      "2025-01-09 21:06:14,559\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:14,561\tWARNING util.py:201 -- The `process_trial_result` operation took 1.276 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:14,562\tWARNING util.py:201 -- Processing trial results took 1.277 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:14,563\tWARNING util.py:201 -- The `process_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000018)\n",
      "2025-01-09 21:06:19,019\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.422 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:19,021\tWARNING util.py:201 -- The `process_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:19,021\tWARNING util.py:201 -- Processing trial results took 1.424 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:19,022\tWARNING util.py:201 -- The `process_trial_result` operation took 1.425 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=202448)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_4aa3b002_1_emb_unit=24,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=10,units_u1=4,unitsm_2=27,unitsm_3=44_2025-01-09_21-04-36/checkpoint_000019)\n",
      "2025-01-09 21:06:23,385\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:23,387\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:23,387\tWARNING util.py:201 -- Processing trial results took 1.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:23,387\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:06:23 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_4aa3b002 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/db2077de1c31405f859f1479438587b8.\n",
      "2025/01/09 21:06:23 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=203996)\u001b[0m 2025-01-09 21:06:26.198246: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=203996)\u001b[0m 2025-01-09 21:06:26.209605: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=203996)\u001b[0m 2025-01-09 21:06:26.223634: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=203996)\u001b[0m 2025-01-09 21:06:26.223676: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=203996)\u001b[0m 2025-01-09 21:06:26.232658: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=203996)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=203996)\u001b[0m 2025-01-09 21:06:27.075561: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.123243: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.168615: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.168669: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.171980: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.172048: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.172066: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.262260: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.262334: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.262341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.262368: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:29.262390: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m I0000 00:00:1736436991.464142  204112 service.cc:145] XLA service 0x7f6e0400a1c0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m I0000 00:00:1736436991.464228  204112 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:31.505717: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m 2025-01-09 21:06:31.741334: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m I0000 00:00:1736436995.970358  204112 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000000)\n",
      "2025-01-09 21:06:44,283\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:44,284\tWARNING util.py:201 -- The `process_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:44,285\tWARNING util.py:201 -- Processing trial results took 1.298 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:44,285\tWARNING util.py:201 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000001)\n",
      "2025-01-09 21:06:48,753\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:48,755\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:48,756\tWARNING util.py:201 -- Processing trial results took 1.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:48,756\tWARNING util.py:201 -- The `process_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000002)\n",
      "2025-01-09 21:06:53,427\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:53,429\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:53,429\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:53,429\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000003)\n",
      "2025-01-09 21:06:58,090\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:58,092\tWARNING util.py:201 -- The `process_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:06:58,093\tWARNING util.py:201 -- Processing trial results took 1.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:06:58,094\tWARNING util.py:201 -- The `process_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000004)\n",
      "2025-01-09 21:07:02,624\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:02,626\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:02,627\tWARNING util.py:201 -- Processing trial results took 1.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:02,627\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000005)\n",
      "2025-01-09 21:07:07,217\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.398 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:07,219\tWARNING util.py:201 -- The `process_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:07,219\tWARNING util.py:201 -- Processing trial results took 1.400 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:07,220\tWARNING util.py:201 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000006)\n",
      "2025-01-09 21:07:11,746\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:11,748\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:11,749\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:11,749\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000007)\n",
      "2025-01-09 21:07:16,196\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:16,198\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:16,199\tWARNING util.py:201 -- Processing trial results took 1.306 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:16,200\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000008)\n",
      "2025-01-09 21:07:20,527\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.183 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:20,528\tWARNING util.py:201 -- The `process_trial_result` operation took 1.184 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:20,529\tWARNING util.py:201 -- Processing trial results took 1.185 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:20,530\tWARNING util.py:201 -- The `process_trial_result` operation took 1.186 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000009)\n",
      "2025-01-09 21:07:25,016\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:25,018\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:25,019\tWARNING util.py:201 -- Processing trial results took 1.333 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:25,019\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000010)\n",
      "2025-01-09 21:07:29,492\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:29,494\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:29,495\tWARNING util.py:201 -- Processing trial results took 1.317 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:29,495\tWARNING util.py:201 -- The `process_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000011)\n",
      "2025-01-09 21:07:33,869\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:33,870\tWARNING util.py:201 -- The `process_trial_result` operation took 1.284 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:33,871\tWARNING util.py:201 -- Processing trial results took 1.284 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:33,872\tWARNING util.py:201 -- The `process_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000012)\n",
      "2025-01-09 21:07:38,432\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:38,433\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:38,434\tWARNING util.py:201 -- Processing trial results took 1.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:38,435\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000013)\n",
      "2025-01-09 21:07:43,017\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:43,019\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:43,020\tWARNING util.py:201 -- Processing trial results took 1.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:43,020\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000014)\n",
      "2025-01-09 21:07:47,578\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:47,580\tWARNING util.py:201 -- The `process_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:47,580\tWARNING util.py:201 -- Processing trial results took 1.349 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:47,580\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000015)\n",
      "2025-01-09 21:07:51,955\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.253 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:51,956\tWARNING util.py:201 -- The `process_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:51,957\tWARNING util.py:201 -- Processing trial results took 1.256 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:51,957\tWARNING util.py:201 -- The `process_trial_result` operation took 1.256 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000016)\n",
      "2025-01-09 21:07:56,655\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:56,657\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:07:56,657\tWARNING util.py:201 -- Processing trial results took 1.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:07:56,658\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000017)\n",
      "2025-01-09 21:08:01,098\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:01,100\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:01,100\tWARNING util.py:201 -- Processing trial results took 1.316 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:01,101\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000018)\n",
      "2025-01-09 21:08:05,520\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:05,522\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:05,523\tWARNING util.py:201 -- Processing trial results took 1.348 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:05,523\tWARNING util.py:201 -- The `process_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=203996)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_cd8a920e_2_emb_unit=27,learning_rate=0.0033,num_layersm=3,num_layersu=3,units_m1=46,units_u1=14,unitsm_2=20,unitsm_3=4_2025-01-09_21-04-41/checkpoint_000019)\n",
      "2025-01-09 21:08:09,897\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.276 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:09,898\tWARNING util.py:201 -- The `process_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:09,900\tWARNING util.py:201 -- Processing trial results took 1.278 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:09,900\tWARNING util.py:201 -- The `process_trial_result` operation took 1.278 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:08:10 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_cd8a920e at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/3ad6798db72c4236b63514d402005cd8.\n",
      "2025/01/09 21:08:10 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=205389)\u001b[0m 2025-01-09 21:08:11.677499: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=205389)\u001b[0m 2025-01-09 21:08:11.688235: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=205389)\u001b[0m 2025-01-09 21:08:11.702745: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=205389)\u001b[0m 2025-01-09 21:08:11.702785: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=205389)\u001b[0m 2025-01-09 21:08:11.711806: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=205389)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=205389)\u001b[0m 2025-01-09 21:08:12.305450: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:13.989032: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.015841: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.015900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.019849: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.019897: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.019913: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.119206: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.119308: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.119318: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.119343: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:14.119367: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m I0000 00:00:1736437095.865733  205495 service.cc:145] XLA service 0x7f86c4010ed0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m I0000 00:00:1736437095.865784  205495 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:15.894497: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m 2025-01-09 21:08:16.063529: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m I0000 00:00:1736437099.482621  205495 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_3', 4 bytes spill stores, 4 bytes spill loads\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m \n",
      "\u001b[36m(train_model pid=205389)\u001b[0m I0000 00:00:1736437099.485796  205495 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000000)\n",
      "2025-01-09 21:08:28,277\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:28,278\tWARNING util.py:201 -- The `process_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:28,279\tWARNING util.py:201 -- Processing trial results took 1.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:28,280\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000001)\n",
      "2025-01-09 21:08:32,617\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:32,619\tWARNING util.py:201 -- The `process_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:32,620\tWARNING util.py:201 -- Processing trial results took 1.287 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:32,620\tWARNING util.py:201 -- The `process_trial_result` operation took 1.288 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000002)\n",
      "2025-01-09 21:08:37,005\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:37,007\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:37,007\tWARNING util.py:201 -- Processing trial results took 1.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:37,007\tWARNING util.py:201 -- The `process_trial_result` operation took 1.248 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000003)\n",
      "2025-01-09 21:08:41,197\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:41,198\tWARNING util.py:201 -- The `process_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:41,198\tWARNING util.py:201 -- Processing trial results took 1.304 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:41,199\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000004)\n",
      "2025-01-09 21:08:45,683\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.409 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:45,685\tWARNING util.py:201 -- The `process_trial_result` operation took 1.411 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:45,686\tWARNING util.py:201 -- Processing trial results took 1.412 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:45,686\tWARNING util.py:201 -- The `process_trial_result` operation took 1.413 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000005)\n",
      "2025-01-09 21:08:49,957\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.275 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:49,958\tWARNING util.py:201 -- The `process_trial_result` operation took 1.277 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:49,959\tWARNING util.py:201 -- Processing trial results took 1.277 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:49,959\tWARNING util.py:201 -- The `process_trial_result` operation took 1.278 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000006)\n",
      "2025-01-09 21:08:54,686\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:54,688\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:54,688\tWARNING util.py:201 -- Processing trial results took 1.379 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:54,689\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000007)\n",
      "2025-01-09 21:08:58,884\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:58,886\tWARNING util.py:201 -- The `process_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:08:58,887\tWARNING util.py:201 -- Processing trial results took 1.288 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:08:58,888\tWARNING util.py:201 -- The `process_trial_result` operation took 1.288 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000008)\n",
      "2025-01-09 21:09:03,459\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:03,461\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:03,462\tWARNING util.py:201 -- Processing trial results took 1.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:03,462\tWARNING util.py:201 -- The `process_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000009)\n",
      "2025-01-09 21:09:07,838\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:07,840\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:07,840\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:07,841\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000010)\n",
      "2025-01-09 21:09:12,127\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.243 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:12,129\tWARNING util.py:201 -- The `process_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:12,129\tWARNING util.py:201 -- Processing trial results took 1.246 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:12,130\tWARNING util.py:201 -- The `process_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000011)\n",
      "2025-01-09 21:09:16,235\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.205 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:16,236\tWARNING util.py:201 -- The `process_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:16,237\tWARNING util.py:201 -- Processing trial results took 1.208 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:16,238\tWARNING util.py:201 -- The `process_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000012)\n",
      "2025-01-09 21:09:20,474\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:20,476\tWARNING util.py:201 -- The `process_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:20,477\tWARNING util.py:201 -- Processing trial results took 1.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:20,477\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000013)\n",
      "2025-01-09 21:09:25,274\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:25,276\tWARNING util.py:201 -- The `process_trial_result` operation took 1.296 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:25,276\tWARNING util.py:201 -- Processing trial results took 1.297 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:25,277\tWARNING util.py:201 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000014)\n",
      "2025-01-09 21:09:29,796\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:29,797\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:29,798\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:29,799\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000015)\n",
      "2025-01-09 21:09:34,283\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:34,285\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:34,285\tWARNING util.py:201 -- Processing trial results took 1.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:34,286\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000016)\n",
      "2025-01-09 21:09:38,766\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.388 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:38,768\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:38,769\tWARNING util.py:201 -- Processing trial results took 1.390 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:38,769\tWARNING util.py:201 -- The `process_trial_result` operation took 1.391 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000017)\n",
      "2025-01-09 21:09:43,130\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:43,132\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:43,134\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:43,134\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000018)\n",
      "2025-01-09 21:09:47,803\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:47,805\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:47,806\tWARNING util.py:201 -- Processing trial results took 1.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:47,806\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=205389)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c7e937e_3_emb_unit=28,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=24,units_u1=45,unitsm_2=13,unitsm_3=4_2025-01-09_21-06-29/checkpoint_000019)\n",
      "2025-01-09 21:09:52,420\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:52,421\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:09:52,422\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:09:52,422\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:09:52 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_8c7e937e at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/559be61490d44f429e6babd882d7646e.\n",
      "2025/01/09 21:09:52 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=206849)\u001b[0m 2025-01-09 21:09:55.203385: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=206849)\u001b[0m 2025-01-09 21:09:55.214364: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=206849)\u001b[0m 2025-01-09 21:09:55.228594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=206849)\u001b[0m 2025-01-09 21:09:55.228639: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=206849)\u001b[0m 2025-01-09 21:09:55.237350: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=206849)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=206849)\u001b[0m 2025-01-09 21:09:56.094233: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.140567: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.184197: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.184255: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.187405: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.187461: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.187482: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.280358: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.280453: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.280463: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.280491: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:09:58.280515: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m I0000 00:00:1736437200.356551  206960 service.cc:145] XLA service 0x7f3aa40062e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m I0000 00:00:1736437200.356630  206960 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:10:00.392685: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m 2025-01-09 21:10:00.606603: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m I0000 00:00:1736437203.456120  206960 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000000)\n",
      "2025-01-09 21:10:11,608\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:11,609\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:11,610\tWARNING util.py:201 -- Processing trial results took 1.356 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:11,611\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000001)\n",
      "2025-01-09 21:10:16,188\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:16,189\tWARNING util.py:201 -- The `process_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:16,190\tWARNING util.py:201 -- Processing trial results took 1.394 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:16,190\tWARNING util.py:201 -- The `process_trial_result` operation took 1.395 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000002)\n",
      "2025-01-09 21:10:20,799\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:20,801\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:20,801\tWARNING util.py:201 -- Processing trial results took 1.379 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:20,802\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000003)\n",
      "2025-01-09 21:10:25,436\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:25,437\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:25,438\tWARNING util.py:201 -- Processing trial results took 1.327 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:25,439\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000004)\n",
      "2025-01-09 21:10:29,783\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.256 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:29,785\tWARNING util.py:201 -- The `process_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:29,785\tWARNING util.py:201 -- Processing trial results took 1.258 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:29,787\tWARNING util.py:201 -- The `process_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000005)\n",
      "2025-01-09 21:10:34,252\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:34,254\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:34,255\tWARNING util.py:201 -- Processing trial results took 1.341 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:34,255\tWARNING util.py:201 -- The `process_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000006)\n",
      "2025-01-09 21:10:38,744\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:38,745\tWARNING util.py:201 -- The `process_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:38,747\tWARNING util.py:201 -- Processing trial results took 1.260 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:38,747\tWARNING util.py:201 -- The `process_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000007)\n",
      "2025-01-09 21:10:43,224\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:43,226\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:43,226\tWARNING util.py:201 -- Processing trial results took 1.333 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:43,227\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000008)\n",
      "2025-01-09 21:10:47,723\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:47,724\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:47,725\tWARNING util.py:201 -- Processing trial results took 1.340 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:47,725\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000009)\n",
      "2025-01-09 21:10:52,066\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.176 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:52,068\tWARNING util.py:201 -- The `process_trial_result` operation took 1.178 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:52,068\tWARNING util.py:201 -- Processing trial results took 1.178 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:52,068\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000010)\n",
      "2025-01-09 21:10:56,895\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:56,897\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:10:56,898\tWARNING util.py:201 -- Processing trial results took 1.354 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:10:56,898\tWARNING util.py:201 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000011)\n",
      "2025-01-09 21:11:01,355\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:01,357\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:01,358\tWARNING util.py:201 -- Processing trial results took 1.346 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:01,358\tWARNING util.py:201 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000012)\n",
      "2025-01-09 21:11:05,811\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:05,812\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:05,813\tWARNING util.py:201 -- Processing trial results took 1.341 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:05,814\tWARNING util.py:201 -- The `process_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000013)\n",
      "2025-01-09 21:11:10,204\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.280 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:10,205\tWARNING util.py:201 -- The `process_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:10,206\tWARNING util.py:201 -- Processing trial results took 1.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:10,207\tWARNING util.py:201 -- The `process_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000014)\n",
      "2025-01-09 21:11:14,807\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:14,808\tWARNING util.py:201 -- The `process_trial_result` operation took 1.385 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:14,809\tWARNING util.py:201 -- Processing trial results took 1.387 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:14,810\tWARNING util.py:201 -- The `process_trial_result` operation took 1.387 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000015)\n",
      "2025-01-09 21:11:19,406\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.436 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:19,408\tWARNING util.py:201 -- The `process_trial_result` operation took 1.438 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:19,408\tWARNING util.py:201 -- Processing trial results took 1.439 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:19,409\tWARNING util.py:201 -- The `process_trial_result` operation took 1.439 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000016)\n",
      "2025-01-09 21:11:23,802\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.271 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:23,804\tWARNING util.py:201 -- The `process_trial_result` operation took 1.273 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:23,805\tWARNING util.py:201 -- Processing trial results took 1.274 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:23,805\tWARNING util.py:201 -- The `process_trial_result` operation took 1.275 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000017)\n",
      "2025-01-09 21:11:28,306\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:28,308\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:28,309\tWARNING util.py:201 -- Processing trial results took 1.382 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:28,309\tWARNING util.py:201 -- The `process_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000018)\n",
      "2025-01-09 21:11:32,965\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:32,967\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:32,968\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:32,968\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=206849)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_14b90495_4_emb_unit=26,learning_rate=0.0004,num_layersm=3,num_layersu=3,units_m1=18,units_u1=22,unitsm_2=25,unitsm_3=2_2025-01-09_21-08-13/checkpoint_000019)\n",
      "2025-01-09 21:11:37,381\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:37,383\tWARNING util.py:201 -- The `process_trial_result` operation took 1.285 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:37,384\tWARNING util.py:201 -- Processing trial results took 1.286 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:37,385\tWARNING util.py:201 -- The `process_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:11:37 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_14b90495 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/4be21dab58ba4a14bb86e1194270131d.\n",
      "2025/01/09 21:11:37 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=208191)\u001b[0m 2025-01-09 21:11:38.720661: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=208191)\u001b[0m 2025-01-09 21:11:38.731597: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=208191)\u001b[0m 2025-01-09 21:11:38.746809: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=208191)\u001b[0m 2025-01-09 21:11:38.746875: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=208191)\u001b[0m 2025-01-09 21:11:38.756147: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=208191)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=208191)\u001b[0m 2025-01-09 21:11:39.360308: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.185839: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.217394: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.217456: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.220647: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.220698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.220718: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.323490: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.323558: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.323566: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.323593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:41.323617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m I0000 00:00:1736437303.086372  208302 service.cc:145] XLA service 0x7f87b4001ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m I0000 00:00:1736437303.086427  208302 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:43.118358: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m 2025-01-09 21:11:43.303340: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m I0000 00:00:1736437306.209519  208302 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m I0000 00:00:1736437310.774276  208300 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m \n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000000)\n",
      "2025-01-09 21:11:53,779\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:53,781\tWARNING util.py:201 -- The `process_trial_result` operation took 1.210 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:53,781\tWARNING util.py:201 -- Processing trial results took 1.210 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:53,782\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000001)\n",
      "2025-01-09 21:11:58,206\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:58,207\tWARNING util.py:201 -- The `process_trial_result` operation took 1.209 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:11:58,209\tWARNING util.py:201 -- Processing trial results took 1.210 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:11:58,209\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000002)\n",
      "2025-01-09 21:12:02,800\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.369 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:02,802\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:02,802\tWARNING util.py:201 -- Processing trial results took 1.371 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:02,803\tWARNING util.py:201 -- The `process_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000003)\n",
      "2025-01-09 21:12:07,553\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:07,554\tWARNING util.py:201 -- The `process_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:07,555\tWARNING util.py:201 -- Processing trial results took 1.376 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:07,555\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000004)\n",
      "2025-01-09 21:12:12,072\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:12,073\tWARNING util.py:201 -- The `process_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:12,074\tWARNING util.py:201 -- Processing trial results took 1.304 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:12,075\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000005)\n",
      "2025-01-09 21:12:16,580\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:16,582\tWARNING util.py:201 -- The `process_trial_result` operation took 1.296 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:16,582\tWARNING util.py:201 -- Processing trial results took 1.297 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:16,583\tWARNING util.py:201 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000006)\n",
      "2025-01-09 21:12:21,266\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:21,268\tWARNING util.py:201 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:21,268\tWARNING util.py:201 -- Processing trial results took 1.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:21,269\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000007)\n",
      "2025-01-09 21:12:25,951\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:25,953\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:25,954\tWARNING util.py:201 -- Processing trial results took 1.366 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:25,955\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000008)\n",
      "2025-01-09 21:12:30,480\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:30,482\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:30,483\tWARNING util.py:201 -- Processing trial results took 1.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:30,483\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000009)\n",
      "2025-01-09 21:12:35,060\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.279 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:35,062\tWARNING util.py:201 -- The `process_trial_result` operation took 1.281 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:35,063\tWARNING util.py:201 -- Processing trial results took 1.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:35,063\tWARNING util.py:201 -- The `process_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000010)\n",
      "2025-01-09 21:12:39,386\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.170 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:39,387\tWARNING util.py:201 -- The `process_trial_result` operation took 1.172 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:39,388\tWARNING util.py:201 -- Processing trial results took 1.173 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:39,389\tWARNING util.py:201 -- The `process_trial_result` operation took 1.173 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000011)\n",
      "2025-01-09 21:12:43,907\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:43,909\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:43,910\tWARNING util.py:201 -- Processing trial results took 1.348 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:43,910\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000012)\n",
      "2025-01-09 21:12:48,662\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:48,663\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:48,664\tWARNING util.py:201 -- Processing trial results took 1.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:48,664\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000013)\n",
      "2025-01-09 21:12:53,257\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.366 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:53,259\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:53,260\tWARNING util.py:201 -- Processing trial results took 1.369 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:53,261\tWARNING util.py:201 -- The `process_trial_result` operation took 1.370 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000014)\n",
      "2025-01-09 21:12:57,880\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:57,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:12:57,882\tWARNING util.py:201 -- Processing trial results took 1.356 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:12:57,882\tWARNING util.py:201 -- The `process_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000015)\n",
      "2025-01-09 21:13:02,574\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:02,575\tWARNING util.py:201 -- The `process_trial_result` operation took 1.288 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:02,576\tWARNING util.py:201 -- Processing trial results took 1.289 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:02,577\tWARNING util.py:201 -- The `process_trial_result` operation took 1.290 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000016)\n",
      "2025-01-09 21:13:07,343\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:07,345\tWARNING util.py:201 -- The `process_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:07,345\tWARNING util.py:201 -- Processing trial results took 1.262 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:07,345\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000017)\n",
      "2025-01-09 21:13:11,791\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.266 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:11,792\tWARNING util.py:201 -- The `process_trial_result` operation took 1.268 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:11,795\tWARNING util.py:201 -- Processing trial results took 1.270 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:11,795\tWARNING util.py:201 -- The `process_trial_result` operation took 1.271 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000018)\n",
      "2025-01-09 21:13:16,400\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.205 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:16,402\tWARNING util.py:201 -- The `process_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:16,403\tWARNING util.py:201 -- Processing trial results took 1.208 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:16,403\tWARNING util.py:201 -- The `process_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=208191)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_f1fe9322_5_emb_unit=26,learning_rate=0.0041,num_layersm=2,num_layersu=3,units_m1=43,units_u1=28,unitsm_2=27,unitsm_3=1_2025-01-09_21-09-58/checkpoint_000019)\n",
      "2025-01-09 21:13:20,728\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.114 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:20,730\tWARNING util.py:201 -- The `process_trial_result` operation took 1.116 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:20,730\tWARNING util.py:201 -- Processing trial results took 1.116 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:20,731\tWARNING util.py:201 -- The `process_trial_result` operation took 1.117 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:13:21 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_f1fe9322 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/070598b04468428bb9a5ee2ccf08cc3b.\n",
      "2025/01/09 21:13:21 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=209576)\u001b[0m 2025-01-09 21:13:22.777572: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=209576)\u001b[0m 2025-01-09 21:13:22.804240: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=209576)\u001b[0m 2025-01-09 21:13:22.818724: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=209576)\u001b[0m 2025-01-09 21:13:22.818773: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=209576)\u001b[0m 2025-01-09 21:13:22.831096: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=209576)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=209576)\u001b[0m 2025-01-09 21:13:23.442970: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.166966: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.197711: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.197765: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.200685: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.200770: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.200791: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.294167: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.294235: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.294241: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.294265: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:25.294287: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m I0000 00:00:1736437407.182135  209686 service.cc:145] XLA service 0x7f6174018ab0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m I0000 00:00:1736437407.182207  209686 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:27.223174: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m 2025-01-09 21:13:27.483536: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m I0000 00:00:1736437412.162831  209686 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m I0000 00:00:1736437415.672973  209884 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'triton_gemm_dot_410', 8 bytes spill stores, 8 bytes spill loads\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m \n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000000)\n",
      "2025-01-09 21:13:42,193\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:42,194\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:42,196\tWARNING util.py:201 -- Processing trial results took 1.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:42,196\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000001)\n",
      "2025-01-09 21:13:46,827\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:46,828\tWARNING util.py:201 -- The `process_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:46,829\tWARNING util.py:201 -- Processing trial results took 1.471 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:46,830\tWARNING util.py:201 -- The `process_trial_result` operation took 1.471 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000002)\n",
      "2025-01-09 21:13:51,244\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.213 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:51,245\tWARNING util.py:201 -- The `process_trial_result` operation took 1.215 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:51,246\tWARNING util.py:201 -- Processing trial results took 1.216 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:51,246\tWARNING util.py:201 -- The `process_trial_result` operation took 1.216 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000003)\n",
      "2025-01-09 21:13:56,094\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:56,096\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:13:56,096\tWARNING util.py:201 -- Processing trial results took 1.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:13:56,097\tWARNING util.py:201 -- The `process_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000004)\n",
      "2025-01-09 21:14:00,777\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:00,779\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:00,780\tWARNING util.py:201 -- Processing trial results took 1.350 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:00,781\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000005)\n",
      "2025-01-09 21:14:05,078\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.155 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:05,080\tWARNING util.py:201 -- The `process_trial_result` operation took 1.157 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:05,081\tWARNING util.py:201 -- Processing trial results took 1.158 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:05,082\tWARNING util.py:201 -- The `process_trial_result` operation took 1.160 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000006)\n",
      "2025-01-09 21:14:10,067\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:10,069\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:10,069\tWARNING util.py:201 -- Processing trial results took 1.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:10,070\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000007)\n",
      "2025-01-09 21:14:14,729\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:14,731\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:14,732\tWARNING util.py:201 -- Processing trial results took 1.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:14,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.320 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000008)\n",
      "2025-01-09 21:14:19,730\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.477 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:19,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.479 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:19,732\tWARNING util.py:201 -- Processing trial results took 1.480 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:19,733\tWARNING util.py:201 -- The `process_trial_result` operation took 1.481 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000009)\n",
      "2025-01-09 21:14:24,431\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.446 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:24,433\tWARNING util.py:201 -- The `process_trial_result` operation took 1.448 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:24,433\tWARNING util.py:201 -- Processing trial results took 1.449 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:24,434\tWARNING util.py:201 -- The `process_trial_result` operation took 1.449 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000010)\n",
      "2025-01-09 21:14:29,060\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:29,062\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:29,062\tWARNING util.py:201 -- Processing trial results took 1.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:29,063\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000011)\n",
      "2025-01-09 21:14:33,836\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.189 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:33,838\tWARNING util.py:201 -- The `process_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:33,839\tWARNING util.py:201 -- Processing trial results took 1.192 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:33,840\tWARNING util.py:201 -- The `process_trial_result` operation took 1.194 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000012)\n",
      "2025-01-09 21:14:38,633\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:38,634\tWARNING util.py:201 -- The `process_trial_result` operation took 1.437 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:38,635\tWARNING util.py:201 -- Processing trial results took 1.438 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:38,636\tWARNING util.py:201 -- The `process_trial_result` operation took 1.438 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000013)\n",
      "2025-01-09 21:14:43,130\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:43,132\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:43,133\tWARNING util.py:201 -- Processing trial results took 1.361 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:43,134\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000014)\n",
      "2025-01-09 21:14:47,678\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:47,680\tWARNING util.py:201 -- The `process_trial_result` operation took 1.258 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:47,680\tWARNING util.py:201 -- Processing trial results took 1.258 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:47,681\tWARNING util.py:201 -- The `process_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000015)\n",
      "2025-01-09 21:14:52,204\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:52,207\tWARNING util.py:201 -- The `process_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:52,207\tWARNING util.py:201 -- Processing trial results took 1.330 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:52,208\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000016)\n",
      "2025-01-09 21:14:56,721\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:56,722\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:14:56,723\tWARNING util.py:201 -- Processing trial results took 1.350 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:14:56,724\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000017)\n",
      "2025-01-09 21:15:01,169\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:01,170\tWARNING util.py:201 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:01,171\tWARNING util.py:201 -- Processing trial results took 1.355 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:01,171\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000018)\n",
      "2025-01-09 21:15:05,776\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.397 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:05,777\tWARNING util.py:201 -- The `process_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:05,778\tWARNING util.py:201 -- Processing trial results took 1.399 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:05,778\tWARNING util.py:201 -- The `process_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=209576)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2a680cce_6_emb_unit=19,learning_rate=0.0084,num_layersm=3,num_layersu=3,units_m1=12,units_u1=24,unitsm_2=34,unitsm_3=3_2025-01-09_21-11-41/checkpoint_000019)\n",
      "2025-01-09 21:15:10,196\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:10,198\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:10,198\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:10,199\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:15:10 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_2a680cce at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/8cb1483a4ba44a9da91f119bcf48d3fc.\n",
      "2025/01/09 21:15:10 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=211059)\u001b[0m 2025-01-09 21:15:12.307515: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=211059)\u001b[0m 2025-01-09 21:15:12.324070: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=211059)\u001b[0m 2025-01-09 21:15:12.340167: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=211059)\u001b[0m 2025-01-09 21:15:12.340210: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=211059)\u001b[0m 2025-01-09 21:15:12.349654: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=211059)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=211059)\u001b[0m 2025-01-09 21:15:13.232277: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.367291: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.410783: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.410836: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.413318: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.413367: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.413383: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.507891: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.507966: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.507973: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.508000: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:15.508027: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m I0000 00:00:1736437517.347994  211172 service.cc:145] XLA service 0x7fef2400ec60 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m I0000 00:00:1736437517.348049  211172 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:17.380435: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m 2025-01-09 21:15:17.562147: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m I0000 00:00:1736437519.999541  211172 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000000)\n",
      "2025-01-09 21:15:27,720\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:27,722\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:27,722\tWARNING util.py:201 -- Processing trial results took 1.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:27,722\tWARNING util.py:201 -- The `process_trial_result` operation took 1.320 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000001)\n",
      "2025-01-09 21:15:32,456\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:32,458\tWARNING util.py:201 -- The `process_trial_result` operation took 1.472 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:32,458\tWARNING util.py:201 -- Processing trial results took 1.473 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:32,459\tWARNING util.py:201 -- The `process_trial_result` operation took 1.473 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000002)\n",
      "2025-01-09 21:15:36,949\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.268 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:36,951\tWARNING util.py:201 -- The `process_trial_result` operation took 1.270 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:36,952\tWARNING util.py:201 -- Processing trial results took 1.271 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:36,952\tWARNING util.py:201 -- The `process_trial_result` operation took 1.271 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000003)\n",
      "2025-01-09 21:15:41,572\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.420 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:41,573\tWARNING util.py:201 -- The `process_trial_result` operation took 1.421 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:41,574\tWARNING util.py:201 -- Processing trial results took 1.422 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:41,575\tWARNING util.py:201 -- The `process_trial_result` operation took 1.423 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000004)\n",
      "2025-01-09 21:15:45,878\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:45,880\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:45,880\tWARNING util.py:201 -- Processing trial results took 1.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:45,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000005)\n",
      "2025-01-09 21:15:50,271\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:50,273\tWARNING util.py:201 -- The `process_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:50,274\tWARNING util.py:201 -- Processing trial results took 1.311 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:50,274\tWARNING util.py:201 -- The `process_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000006)\n",
      "2025-01-09 21:15:54,307\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.011 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:54,308\tWARNING util.py:201 -- The `process_trial_result` operation took 1.012 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:54,309\tWARNING util.py:201 -- Processing trial results took 1.013 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:54,309\tWARNING util.py:201 -- The `process_trial_result` operation took 1.014 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000007)\n",
      "2025-01-09 21:15:58,730\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:58,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:15:58,733\tWARNING util.py:201 -- Processing trial results took 1.336 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:15:58,733\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000008)\n",
      "2025-01-09 21:16:03,035\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:03,037\tWARNING util.py:201 -- The `process_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:03,037\tWARNING util.py:201 -- Processing trial results took 1.336 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:03,038\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000009)\n",
      "2025-01-09 21:16:07,427\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:07,429\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:07,429\tWARNING util.py:201 -- Processing trial results took 1.328 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:07,430\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000010)\n",
      "2025-01-09 21:16:11,813\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:11,815\tWARNING util.py:201 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:11,816\tWARNING util.py:201 -- Processing trial results took 1.356 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:11,817\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000011)\n",
      "2025-01-09 21:16:16,267\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:16,270\tWARNING util.py:201 -- The `process_trial_result` operation took 1.366 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:16,270\tWARNING util.py:201 -- Processing trial results took 1.366 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:16,271\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000012)\n",
      "2025-01-09 21:16:20,548\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:20,549\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:20,551\tWARNING util.py:201 -- Processing trial results took 1.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:20,552\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000013)\n",
      "2025-01-09 21:16:24,759\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.081 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:24,761\tWARNING util.py:201 -- The `process_trial_result` operation took 1.083 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:24,761\tWARNING util.py:201 -- Processing trial results took 1.083 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:24,761\tWARNING util.py:201 -- The `process_trial_result` operation took 1.084 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000014)\n",
      "2025-01-09 21:16:29,102\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:29,104\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:29,105\tWARNING util.py:201 -- Processing trial results took 1.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:29,105\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000015)\n",
      "2025-01-09 21:16:33,538\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:33,539\tWARNING util.py:201 -- The `process_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:33,540\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:33,541\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000016)\n",
      "2025-01-09 21:16:37,824\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:37,826\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:37,827\tWARNING util.py:201 -- Processing trial results took 1.325 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:37,827\tWARNING util.py:201 -- The `process_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000017)\n",
      "2025-01-09 21:16:42,285\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.272 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:42,287\tWARNING util.py:201 -- The `process_trial_result` operation took 1.274 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:42,288\tWARNING util.py:201 -- Processing trial results took 1.275 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:42,288\tWARNING util.py:201 -- The `process_trial_result` operation took 1.276 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000018)\n",
      "2025-01-09 21:16:46,517\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.218 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:46,519\tWARNING util.py:201 -- The `process_trial_result` operation took 1.220 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:46,519\tWARNING util.py:201 -- Processing trial results took 1.221 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:46,520\tWARNING util.py:201 -- The `process_trial_result` operation took 1.221 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=211059)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_8c364857_7_emb_unit=27,learning_rate=0.0035,num_layersm=2,num_layersu=2,units_m1=27,units_u1=7,unitsm_2=48,unitsm_3=49_2025-01-09_21-13-25/checkpoint_000019)\n",
      "2025-01-09 21:16:50,865\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:50,867\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:16:50,868\tWARNING util.py:201 -- Processing trial results took 1.307 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:16:50,868\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:16:51 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_8c364857 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/1931d876cc974c9391b26250fd11ad6d.\n",
      "2025/01/09 21:16:51 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=212437)\u001b[0m 2025-01-09 21:16:52.752985: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=212437)\u001b[0m 2025-01-09 21:16:52.762770: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=212437)\u001b[0m 2025-01-09 21:16:52.778278: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=212437)\u001b[0m 2025-01-09 21:16:52.778331: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=212437)\u001b[0m 2025-01-09 21:16:52.787064: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=212437)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=212437)\u001b[0m 2025-01-09 21:16:53.332949: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.805597: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.836903: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.836963: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.840003: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.840070: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.840089: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.940678: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.940742: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.940749: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.940775: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:54.940795: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m I0000 00:00:1736437616.732915  212549 service.cc:145] XLA service 0x7fcf1c022580 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m I0000 00:00:1736437616.732970  212549 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:56.765696: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m 2025-01-09 21:16:56.950247: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m I0000 00:00:1736437618.824677  212549 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000000)\n",
      "2025-01-09 21:17:06,042\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:06,043\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:06,044\tWARNING util.py:201 -- Processing trial results took 1.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:06,044\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000001)\n",
      "2025-01-09 21:17:10,727\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.218 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:10,728\tWARNING util.py:201 -- The `process_trial_result` operation took 1.220 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:10,729\tWARNING util.py:201 -- Processing trial results took 1.220 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:10,730\tWARNING util.py:201 -- The `process_trial_result` operation took 1.221 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000002)\n",
      "2025-01-09 21:17:15,322\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.210 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:15,323\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:15,324\tWARNING util.py:201 -- Processing trial results took 1.212 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:15,325\tWARNING util.py:201 -- The `process_trial_result` operation took 1.213 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000003)\n",
      "2025-01-09 21:17:20,197\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:20,198\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:20,199\tWARNING util.py:201 -- Processing trial results took 1.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:20,199\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000004)\n",
      "2025-01-09 21:17:25,138\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.410 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:25,140\tWARNING util.py:201 -- The `process_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:25,141\tWARNING util.py:201 -- Processing trial results took 1.413 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:25,141\tWARNING util.py:201 -- The `process_trial_result` operation took 1.413 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000005)\n",
      "2025-01-09 21:17:29,975\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:29,977\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:29,978\tWARNING util.py:201 -- Processing trial results took 1.365 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:29,978\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000006)\n",
      "2025-01-09 21:17:34,803\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:34,805\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:34,805\tWARNING util.py:201 -- Processing trial results took 1.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:34,806\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000007)\n",
      "2025-01-09 21:17:39,583\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:39,584\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:39,585\tWARNING util.py:201 -- Processing trial results took 1.328 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:39,585\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000008)\n",
      "2025-01-09 21:17:44,311\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:44,313\tWARNING util.py:201 -- The `process_trial_result` operation took 1.383 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:44,314\tWARNING util.py:201 -- Processing trial results took 1.384 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:44,314\tWARNING util.py:201 -- The `process_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000009)\n",
      "2025-01-09 21:17:49,171\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:49,172\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:49,174\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:49,174\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000010)\n",
      "2025-01-09 21:17:53,963\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:53,965\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:53,966\tWARNING util.py:201 -- Processing trial results took 1.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:53,966\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000011)\n",
      "2025-01-09 21:17:58,772\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:58,773\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:17:58,774\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:17:58,775\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000012)\n",
      "2025-01-09 21:18:03,487\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:03,489\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:03,489\tWARNING util.py:201 -- Processing trial results took 1.322 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:03,490\tWARNING util.py:201 -- The `process_trial_result` operation took 1.322 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000013)\n",
      "2025-01-09 21:18:08,272\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:08,273\tWARNING util.py:201 -- The `process_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:08,274\tWARNING util.py:201 -- Processing trial results took 1.374 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:08,274\tWARNING util.py:201 -- The `process_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000014)\n",
      "2025-01-09 21:18:12,797\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.253 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:12,798\tWARNING util.py:201 -- The `process_trial_result` operation took 1.254 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:12,798\tWARNING util.py:201 -- Processing trial results took 1.254 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:12,799\tWARNING util.py:201 -- The `process_trial_result` operation took 1.255 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000015)\n",
      "2025-01-09 21:18:17,381\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.058 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:17,382\tWARNING util.py:201 -- The `process_trial_result` operation took 1.059 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:17,383\tWARNING util.py:201 -- Processing trial results took 1.060 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:17,383\tWARNING util.py:201 -- The `process_trial_result` operation took 1.060 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000016)\n",
      "2025-01-09 21:18:22,084\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.322 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:22,086\tWARNING util.py:201 -- The `process_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:22,086\tWARNING util.py:201 -- Processing trial results took 1.324 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:22,087\tWARNING util.py:201 -- The `process_trial_result` operation took 1.324 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000017)\n",
      "2025-01-09 21:18:26,870\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:26,871\tWARNING util.py:201 -- The `process_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:26,872\tWARNING util.py:201 -- Processing trial results took 1.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:26,872\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000018)\n",
      "2025-01-09 21:18:31,428\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.244 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:31,429\tWARNING util.py:201 -- The `process_trial_result` operation took 1.246 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:31,430\tWARNING util.py:201 -- Processing trial results took 1.246 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:31,430\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=212437)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_395fc625_8_emb_unit=24,learning_rate=0.0060,num_layersm=3,num_layersu=2,units_m1=16,units_u1=11,unitsm_2=4,unitsm_3=10_2025-01-09_21-15-15/checkpoint_000019)\n",
      "2025-01-09 21:18:36,115\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.288 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:36,116\tWARNING util.py:201 -- The `process_trial_result` operation took 1.290 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:36,117\tWARNING util.py:201 -- Processing trial results took 1.290 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:36,118\tWARNING util.py:201 -- The `process_trial_result` operation took 1.291 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:18:36 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_395fc625 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/5db0f94d25724be7bf2e147b4741d243.\n",
      "2025/01/09 21:18:36 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=213792)\u001b[0m 2025-01-09 21:18:38.217086: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=213792)\u001b[0m 2025-01-09 21:18:38.227780: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=213792)\u001b[0m 2025-01-09 21:18:38.241368: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=213792)\u001b[0m 2025-01-09 21:18:38.241410: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=213792)\u001b[0m 2025-01-09 21:18:38.250105: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=213792)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=213792)\u001b[0m 2025-01-09 21:18:39.127422: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.210008: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.243428: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.243484: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.246715: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.246771: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.246789: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.345410: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.345513: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.345523: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.345551: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:41.345572: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m I0000 00:00:1736437723.243235  213905 service.cc:145] XLA service 0x7f4c14001640 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m I0000 00:00:1736437723.243281  213905 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:43.275043: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m 2025-01-09 21:18:43.466028: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m I0000 00:00:1736437727.756003  213905 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000000)\n",
      "2025-01-09 21:18:56,171\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.423 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:56,172\tWARNING util.py:201 -- The `process_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:18:56,173\tWARNING util.py:201 -- Processing trial results took 1.425 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:18:56,173\tWARNING util.py:201 -- The `process_trial_result` operation took 1.425 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000001)\n",
      "2025-01-09 21:19:00,656\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:00,657\tWARNING util.py:201 -- The `process_trial_result` operation took 1.383 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:00,658\tWARNING util.py:201 -- Processing trial results took 1.384 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:00,658\tWARNING util.py:201 -- The `process_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000002)\n",
      "2025-01-09 21:19:06,146\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.398 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:06,148\tWARNING util.py:201 -- The `process_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:06,149\tWARNING util.py:201 -- Processing trial results took 1.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:06,150\tWARNING util.py:201 -- The `process_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000003)\n",
      "2025-01-09 21:19:10,605\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.323 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:10,607\tWARNING util.py:201 -- The `process_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:10,608\tWARNING util.py:201 -- Processing trial results took 1.326 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:10,609\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000004)\n",
      "2025-01-09 21:19:15,257\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.369 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:15,259\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:15,260\tWARNING util.py:201 -- Processing trial results took 1.372 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:15,260\tWARNING util.py:201 -- The `process_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000005)\n",
      "2025-01-09 21:19:19,827\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:19,829\tWARNING util.py:201 -- The `process_trial_result` operation took 1.267 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:19,829\tWARNING util.py:201 -- Processing trial results took 1.268 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:19,830\tWARNING util.py:201 -- The `process_trial_result` operation took 1.268 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000006)\n",
      "2025-01-09 21:19:24,867\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:24,869\tWARNING util.py:201 -- The `process_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:24,869\tWARNING util.py:201 -- Processing trial results took 1.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:24,870\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000007)\n",
      "2025-01-09 21:19:30,599\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.471 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:30,601\tWARNING util.py:201 -- The `process_trial_result` operation took 1.473 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:30,601\tWARNING util.py:201 -- Processing trial results took 1.474 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:30,602\tWARNING util.py:201 -- The `process_trial_result` operation took 1.474 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000008)\n",
      "2025-01-09 21:19:35,396\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:35,397\tWARNING util.py:201 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:35,398\tWARNING util.py:201 -- Processing trial results took 1.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:35,398\tWARNING util.py:201 -- The `process_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000009)\n",
      "2025-01-09 21:19:40,081\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:40,083\tWARNING util.py:201 -- The `process_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:40,084\tWARNING util.py:201 -- Processing trial results took 1.376 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:40,084\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000010)\n",
      "2025-01-09 21:19:44,622\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.370 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:44,624\tWARNING util.py:201 -- The `process_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:44,624\tWARNING util.py:201 -- Processing trial results took 1.372 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:44,625\tWARNING util.py:201 -- The `process_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000011)\n",
      "2025-01-09 21:19:49,121\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:49,123\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:49,123\tWARNING util.py:201 -- Processing trial results took 1.376 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:49,124\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000012)\n",
      "2025-01-09 21:19:53,787\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:53,789\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:53,789\tWARNING util.py:201 -- Processing trial results took 1.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:53,790\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000013)\n",
      "2025-01-09 21:19:58,431\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.455 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:58,432\tWARNING util.py:201 -- The `process_trial_result` operation took 1.456 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:19:58,433\tWARNING util.py:201 -- Processing trial results took 1.457 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:19:58,434\tWARNING util.py:201 -- The `process_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000014)\n",
      "2025-01-09 21:20:03,054\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.369 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:03,055\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:03,056\tWARNING util.py:201 -- Processing trial results took 1.371 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:03,057\tWARNING util.py:201 -- The `process_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000015)\n",
      "2025-01-09 21:20:07,784\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.420 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:07,786\tWARNING util.py:201 -- The `process_trial_result` operation took 1.422 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:07,787\tWARNING util.py:201 -- Processing trial results took 1.423 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:07,787\tWARNING util.py:201 -- The `process_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000016)\n",
      "2025-01-09 21:20:12,683\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.453 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:12,684\tWARNING util.py:201 -- The `process_trial_result` operation took 1.454 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:12,686\tWARNING util.py:201 -- Processing trial results took 1.455 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:12,686\tWARNING util.py:201 -- The `process_trial_result` operation took 1.455 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000017)\n",
      "2025-01-09 21:20:17,449\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:17,451\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:17,451\tWARNING util.py:201 -- Processing trial results took 1.344 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:17,452\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000018)\n",
      "2025-01-09 21:20:22,102\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:22,104\tWARNING util.py:201 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:22,104\tWARNING util.py:201 -- Processing trial results took 1.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:22,105\tWARNING util.py:201 -- The `process_trial_result` operation took 1.348 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=213792)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_3250a14a_9_emb_unit=19,learning_rate=0.0005,num_layersm=2,num_layersu=3,units_m1=38,units_u1=38,unitsm_2=15,unitsm_3=4_2025-01-09_21-16-54/checkpoint_000019)\n",
      "2025-01-09 21:20:26,621\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.406 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:26,623\tWARNING util.py:201 -- The `process_trial_result` operation took 1.408 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:26,624\tWARNING util.py:201 -- Processing trial results took 1.408 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:26,624\tWARNING util.py:201 -- The `process_trial_result` operation took 1.409 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:20:27 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_3250a14a at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/fc419fe5591b498fa5428f1aeae2f611.\n",
      "2025/01/09 21:20:27 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=215253)\u001b[0m 2025-01-09 21:20:28.808222: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=215253)\u001b[0m 2025-01-09 21:20:28.820188: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=215253)\u001b[0m 2025-01-09 21:20:28.835093: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=215253)\u001b[0m 2025-01-09 21:20:28.835134: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=215253)\u001b[0m 2025-01-09 21:20:28.844257: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=215253)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=215253)\u001b[0m 2025-01-09 21:20:29.439832: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.115697: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.146577: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.146636: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.149467: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.149527: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.149546: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.249516: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.249609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.249617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.249643: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:31.249667: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m I0000 00:00:1736437833.221846  215359 service.cc:145] XLA service 0x7efcdc007ec0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m I0000 00:00:1736437833.221895  215359 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:33.256999: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m 2025-01-09 21:20:33.468732: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m I0000 00:00:1736437837.153186  215359 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000000)\n",
      "2025-01-09 21:20:46,146\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:46,148\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:46,149\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:46,149\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000001)\n",
      "2025-01-09 21:20:51,019\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:51,021\tWARNING util.py:201 -- The `process_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:51,022\tWARNING util.py:201 -- Processing trial results took 1.404 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:51,022\tWARNING util.py:201 -- The `process_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000002)\n",
      "2025-01-09 21:20:55,923\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:55,924\tWARNING util.py:201 -- The `process_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:20:55,925\tWARNING util.py:201 -- Processing trial results took 1.468 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:20:55,925\tWARNING util.py:201 -- The `process_trial_result` operation took 1.469 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000003)\n",
      "2025-01-09 21:21:00,639\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:00,640\tWARNING util.py:201 -- The `process_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:00,641\tWARNING util.py:201 -- Processing trial results took 1.392 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:00,642\tWARNING util.py:201 -- The `process_trial_result` operation took 1.393 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000004)\n",
      "2025-01-09 21:21:05,448\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:05,450\tWARNING util.py:201 -- The `process_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:05,451\tWARNING util.py:201 -- Processing trial results took 1.383 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:05,451\tWARNING util.py:201 -- The `process_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000005)\n",
      "2025-01-09 21:21:10,431\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.563 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:10,433\tWARNING util.py:201 -- The `process_trial_result` operation took 1.566 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:10,434\tWARNING util.py:201 -- Processing trial results took 1.566 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:10,434\tWARNING util.py:201 -- The `process_trial_result` operation took 1.567 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000006)\n",
      "2025-01-09 21:21:15,396\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:15,398\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:15,398\tWARNING util.py:201 -- Processing trial results took 1.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:15,399\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000007)\n",
      "2025-01-09 21:21:20,509\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:20,510\tWARNING util.py:201 -- The `process_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:20,511\tWARNING util.py:201 -- Processing trial results took 1.395 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:20,511\tWARNING util.py:201 -- The `process_trial_result` operation took 1.395 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000008)\n",
      "2025-01-09 21:21:25,305\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.476 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:25,306\tWARNING util.py:201 -- The `process_trial_result` operation took 1.478 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:25,307\tWARNING util.py:201 -- Processing trial results took 1.478 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:25,307\tWARNING util.py:201 -- The `process_trial_result` operation took 1.479 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000009)\n",
      "2025-01-09 21:21:30,124\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.409 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:30,126\tWARNING util.py:201 -- The `process_trial_result` operation took 1.411 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:30,127\tWARNING util.py:201 -- Processing trial results took 1.412 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:30,127\tWARNING util.py:201 -- The `process_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000010)\n",
      "2025-01-09 21:21:34,984\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:34,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:34,986\tWARNING util.py:201 -- Processing trial results took 1.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:34,987\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000011)\n",
      "2025-01-09 21:21:39,990\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.444 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:39,992\tWARNING util.py:201 -- The `process_trial_result` operation took 1.446 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:39,993\tWARNING util.py:201 -- Processing trial results took 1.447 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:39,994\tWARNING util.py:201 -- The `process_trial_result` operation took 1.448 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000012)\n",
      "2025-01-09 21:21:46,249\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:46,251\tWARNING util.py:201 -- The `process_trial_result` operation took 1.944 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:46,252\tWARNING util.py:201 -- Processing trial results took 1.945 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:46,253\tWARNING util.py:201 -- The `process_trial_result` operation took 1.946 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000013)\n",
      "2025-01-09 21:21:51,113\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:51,115\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:51,116\tWARNING util.py:201 -- Processing trial results took 1.304 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:51,117\tWARNING util.py:201 -- The `process_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000014)\n",
      "2025-01-09 21:21:56,150\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:56,152\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:21:56,152\tWARNING util.py:201 -- Processing trial results took 1.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:21:56,153\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000015)\n",
      "2025-01-09 21:22:01,174\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.436 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:01,176\tWARNING util.py:201 -- The `process_trial_result` operation took 1.439 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:01,177\tWARNING util.py:201 -- Processing trial results took 1.440 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:01,177\tWARNING util.py:201 -- The `process_trial_result` operation took 1.440 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000016)\n",
      "2025-01-09 21:22:06,022\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.495 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:06,024\tWARNING util.py:201 -- The `process_trial_result` operation took 1.497 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:06,025\tWARNING util.py:201 -- Processing trial results took 1.498 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:06,025\tWARNING util.py:201 -- The `process_trial_result` operation took 1.498 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000017)\n",
      "2025-01-09 21:22:10,736\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:10,737\tWARNING util.py:201 -- The `process_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:10,737\tWARNING util.py:201 -- Processing trial results took 1.374 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:10,738\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000018)\n",
      "2025-01-09 21:22:15,737\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.432 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:15,739\tWARNING util.py:201 -- The `process_trial_result` operation took 1.434 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:15,740\tWARNING util.py:201 -- Processing trial results took 1.435 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:15,740\tWARNING util.py:201 -- The `process_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=215253)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_b673c549_10_emb_unit=21,learning_rate=0.0003,num_layersm=3,num_layersu=3,units_m1=48,units_u1=34,unitsm_2=28,unitsm_3=_2025-01-09_21-18-41/checkpoint_000019)\n",
      "2025-01-09 21:22:20,934\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.399 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:20,935\tWARNING util.py:201 -- The `process_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:20,936\tWARNING util.py:201 -- Processing trial results took 1.401 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:20,936\tWARNING util.py:201 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:22:21 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_b673c549 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/1328bfadb4e54d6db896902d09f89735.\n",
      "2025/01/09 21:22:21 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=216718)\u001b[0m 2025-01-09 21:22:22.859518: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=216718)\u001b[0m 2025-01-09 21:22:22.869595: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=216718)\u001b[0m 2025-01-09 21:22:22.883108: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=216718)\u001b[0m 2025-01-09 21:22:22.883150: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=216718)\u001b[0m 2025-01-09 21:22:22.891267: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=216718)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=216718)\u001b[0m 2025-01-09 21:22:23.564134: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.397751: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.427787: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.427844: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.431583: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.431698: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.431721: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.520024: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.520092: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.520098: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.520123: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:25.520145: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m I0000 00:00:1736437947.301848  216828 service.cc:145] XLA service 0x7f21840027e0 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m I0000 00:00:1736437947.301903  216828 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:27.334682: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m 2025-01-09 21:22:27.530466: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m I0000 00:00:1736437951.752728  216828 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_4', 16 bytes spill stores, 16 bytes spill loads\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m \n",
      "\u001b[36m(train_model pid=216718)\u001b[0m I0000 00:00:1736437951.759875  216828 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000000)\n",
      "2025-01-09 21:22:41,581\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:41,583\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:41,583\tWARNING util.py:201 -- Processing trial results took 1.375 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:41,584\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000001)\n",
      "2025-01-09 21:22:46,517\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.659 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:46,519\tWARNING util.py:201 -- The `process_trial_result` operation took 1.661 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:46,519\tWARNING util.py:201 -- Processing trial results took 1.661 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:46,520\tWARNING util.py:201 -- The `process_trial_result` operation took 1.662 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000002)\n",
      "2025-01-09 21:22:51,343\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:51,344\tWARNING util.py:201 -- The `process_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:51,345\tWARNING util.py:201 -- Processing trial results took 1.468 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:51,345\tWARNING util.py:201 -- The `process_trial_result` operation took 1.469 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000003)\n",
      "2025-01-09 21:22:55,877\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:55,879\tWARNING util.py:201 -- The `process_trial_result` operation took 1.317 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:22:55,880\tWARNING util.py:201 -- Processing trial results took 1.318 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:22:55,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000004)\n",
      "2025-01-09 21:23:00,579\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.478 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:00,581\tWARNING util.py:201 -- The `process_trial_result` operation took 1.480 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:00,582\tWARNING util.py:201 -- Processing trial results took 1.481 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:00,582\tWARNING util.py:201 -- The `process_trial_result` operation took 1.481 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000005)\n",
      "2025-01-09 21:23:05,388\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.388 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:05,390\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:05,391\tWARNING util.py:201 -- Processing trial results took 1.390 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:05,391\tWARNING util.py:201 -- The `process_trial_result` operation took 1.391 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000006)\n",
      "2025-01-09 21:23:10,176\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.609 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:10,177\tWARNING util.py:201 -- The `process_trial_result` operation took 1.611 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:10,178\tWARNING util.py:201 -- Processing trial results took 1.611 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:10,179\tWARNING util.py:201 -- The `process_trial_result` operation took 1.612 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000007)\n",
      "2025-01-09 21:23:14,702\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:14,704\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:14,706\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:14,706\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000008)\n",
      "2025-01-09 21:23:19,671\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.200 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:19,673\tWARNING util.py:201 -- The `process_trial_result` operation took 1.202 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:19,674\tWARNING util.py:201 -- Processing trial results took 1.203 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:19,674\tWARNING util.py:201 -- The `process_trial_result` operation took 1.203 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000009)\n",
      "2025-01-09 21:23:24,901\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:24,903\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:24,903\tWARNING util.py:201 -- Processing trial results took 1.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:24,904\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000010)\n",
      "2025-01-09 21:23:29,512\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:29,514\tWARNING util.py:201 -- The `process_trial_result` operation took 1.459 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:29,515\tWARNING util.py:201 -- Processing trial results took 1.460 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:29,515\tWARNING util.py:201 -- The `process_trial_result` operation took 1.460 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000011)\n",
      "2025-01-09 21:23:34,159\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.387 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:34,160\tWARNING util.py:201 -- The `process_trial_result` operation took 1.389 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:34,161\tWARNING util.py:201 -- Processing trial results took 1.390 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:34,162\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000012)\n",
      "2025-01-09 21:23:38,797\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:38,799\tWARNING util.py:201 -- The `process_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:38,799\tWARNING util.py:201 -- Processing trial results took 1.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:38,800\tWARNING util.py:201 -- The `process_trial_result` operation took 1.266 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000013)\n",
      "2025-01-09 21:23:43,243\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.245 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:43,245\tWARNING util.py:201 -- The `process_trial_result` operation took 1.247 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:43,246\tWARNING util.py:201 -- Processing trial results took 1.248 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:43,246\tWARNING util.py:201 -- The `process_trial_result` operation took 1.248 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000014)\n",
      "2025-01-09 21:23:47,688\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.241 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:47,690\tWARNING util.py:201 -- The `process_trial_result` operation took 1.243 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:47,690\tWARNING util.py:201 -- Processing trial results took 1.243 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:47,690\tWARNING util.py:201 -- The `process_trial_result` operation took 1.244 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000015)\n",
      "2025-01-09 21:23:52,213\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:52,215\tWARNING util.py:201 -- The `process_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:52,215\tWARNING util.py:201 -- Processing trial results took 1.305 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:52,216\tWARNING util.py:201 -- The `process_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000016)\n",
      "2025-01-09 21:23:57,256\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.511 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:57,260\tWARNING util.py:201 -- The `process_trial_result` operation took 1.515 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:23:57,261\tWARNING util.py:201 -- Processing trial results took 1.515 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:23:57,261\tWARNING util.py:201 -- The `process_trial_result` operation took 1.516 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000017)\n",
      "2025-01-09 21:24:01,836\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:01,837\tWARNING util.py:201 -- The `process_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:01,838\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:01,838\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000018)\n",
      "2025-01-09 21:24:06,263\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.201 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:06,265\tWARNING util.py:201 -- The `process_trial_result` operation took 1.203 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:06,265\tWARNING util.py:201 -- Processing trial results took 1.203 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:06,266\tWARNING util.py:201 -- The `process_trial_result` operation took 1.204 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=216718)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fe897ff4_11_emb_unit=19,learning_rate=0.0002,num_layersm=2,num_layersu=3,units_m1=18,units_u1=46,unitsm_2=42,unitsm_3=_2025-01-09_21-20-31/checkpoint_000019)\n",
      "2025-01-09 21:24:10,958\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.536 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:10,959\tWARNING util.py:201 -- The `process_trial_result` operation took 1.537 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:10,960\tWARNING util.py:201 -- Processing trial results took 1.538 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:10,961\tWARNING util.py:201 -- The `process_trial_result` operation took 1.539 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:24:11 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_fe897ff4 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/f62e975dce3342e697885ae95c2cd2e8.\n",
      "2025/01/09 21:24:11 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=218161)\u001b[0m 2025-01-09 21:24:13.341415: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=218161)\u001b[0m 2025-01-09 21:24:13.355370: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=218161)\u001b[0m 2025-01-09 21:24:13.373951: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=218161)\u001b[0m 2025-01-09 21:24:13.374005: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=218161)\u001b[0m 2025-01-09 21:24:13.386552: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=218161)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=218161)\u001b[0m 2025-01-09 21:24:14.270788: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.442981: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.496495: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.496563: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.498906: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.498973: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.498990: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.591176: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.591418: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.591432: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.591477: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:16.591571: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m I0000 00:00:1736438058.680868  218276 service.cc:145] XLA service 0x7f2b24006f00 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m I0000 00:00:1736438058.680940  218276 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:18.719496: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m 2025-01-09 21:24:18.958289: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m I0000 00:00:1736438062.093726  218276 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000000)\n",
      "2025-01-09 21:24:29,984\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:29,985\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:29,986\tWARNING util.py:201 -- Processing trial results took 1.352 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:29,987\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000001)\n",
      "2025-01-09 21:24:34,828\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:34,829\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:34,830\tWARNING util.py:201 -- Processing trial results took 1.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:34,832\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000002)\n",
      "2025-01-09 21:24:39,501\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:39,503\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:39,503\tWARNING util.py:201 -- Processing trial results took 1.331 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:39,504\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000003)\n",
      "2025-01-09 21:24:44,203\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.203 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:44,204\tWARNING util.py:201 -- The `process_trial_result` operation took 1.205 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:44,205\tWARNING util.py:201 -- Processing trial results took 1.206 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:44,206\tWARNING util.py:201 -- The `process_trial_result` operation took 1.206 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000004)\n",
      "2025-01-09 21:24:48,957\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:48,960\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:48,960\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:48,960\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000005)\n",
      "2025-01-09 21:24:54,310\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.520 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:54,312\tWARNING util.py:201 -- The `process_trial_result` operation took 1.522 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:54,313\tWARNING util.py:201 -- Processing trial results took 1.523 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:54,314\tWARNING util.py:201 -- The `process_trial_result` operation took 1.523 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000006)\n",
      "2025-01-09 21:24:59,173\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.125 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:59,175\tWARNING util.py:201 -- The `process_trial_result` operation took 1.127 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:24:59,175\tWARNING util.py:201 -- Processing trial results took 1.128 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:24:59,176\tWARNING util.py:201 -- The `process_trial_result` operation took 1.128 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000007)\n",
      "2025-01-09 21:25:04,055\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.370 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:04,056\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:04,057\tWARNING util.py:201 -- Processing trial results took 1.373 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:04,058\tWARNING util.py:201 -- The `process_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000008)\n",
      "2025-01-09 21:25:09,031\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.462 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:09,033\tWARNING util.py:201 -- The `process_trial_result` operation took 1.464 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:09,034\tWARNING util.py:201 -- Processing trial results took 1.465 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:09,034\tWARNING util.py:201 -- The `process_trial_result` operation took 1.465 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000009)\n",
      "2025-01-09 21:25:13,842\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:13,843\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:13,844\tWARNING util.py:201 -- Processing trial results took 1.345 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:13,844\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000010)\n",
      "2025-01-09 21:25:18,748\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.415 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:18,750\tWARNING util.py:201 -- The `process_trial_result` operation took 1.416 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:18,751\tWARNING util.py:201 -- Processing trial results took 1.417 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:18,751\tWARNING util.py:201 -- The `process_trial_result` operation took 1.418 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000011)\n",
      "2025-01-09 21:25:23,515\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.411 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:23,516\tWARNING util.py:201 -- The `process_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:23,517\tWARNING util.py:201 -- Processing trial results took 1.413 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:23,517\tWARNING util.py:201 -- The `process_trial_result` operation took 1.414 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000012)\n",
      "2025-01-09 21:25:28,607\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:28,608\tWARNING util.py:201 -- The `process_trial_result` operation took 1.459 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:28,609\tWARNING util.py:201 -- Processing trial results took 1.460 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:28,609\tWARNING util.py:201 -- The `process_trial_result` operation took 1.460 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000013)\n",
      "2025-01-09 21:25:33,613\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:33,615\tWARNING util.py:201 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:33,615\tWARNING util.py:201 -- Processing trial results took 1.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:33,615\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000014)\n",
      "2025-01-09 21:25:38,461\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:38,463\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:38,464\tWARNING util.py:201 -- Processing trial results took 1.379 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:38,464\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000015)\n",
      "2025-01-09 21:25:43,300\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:43,302\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:43,303\tWARNING util.py:201 -- Processing trial results took 1.365 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:43,303\tWARNING util.py:201 -- The `process_trial_result` operation took 1.366 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000016)\n",
      "2025-01-09 21:25:48,439\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.432 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:48,442\tWARNING util.py:201 -- The `process_trial_result` operation took 1.435 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:48,442\tWARNING util.py:201 -- Processing trial results took 1.435 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:48,443\tWARNING util.py:201 -- The `process_trial_result` operation took 1.436 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000017)\n",
      "2025-01-09 21:25:53,504\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:53,505\tWARNING util.py:201 -- The `process_trial_result` operation took 1.351 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:53,505\tWARNING util.py:201 -- Processing trial results took 1.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:53,506\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000018)\n",
      "2025-01-09 21:25:58,329\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:58,331\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:25:58,331\tWARNING util.py:201 -- Processing trial results took 1.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:25:58,332\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=218161)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_da84f091_12_emb_unit=22,learning_rate=0.0030,num_layersm=3,num_layersu=3,units_m1=13,units_u1=27,unitsm_2=19,unitsm_3=_2025-01-09_21-22-25/checkpoint_000019)\n",
      "2025-01-09 21:26:03,003\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.287 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:03,005\tWARNING util.py:201 -- The `process_trial_result` operation took 1.288 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:03,006\tWARNING util.py:201 -- Processing trial results took 1.289 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:03,007\tWARNING util.py:201 -- The `process_trial_result` operation took 1.290 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:26:03 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_da84f091 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/1200ba5348de4267b678f6f002cbb3d1.\n",
      "2025/01/09 21:26:03 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=219591)\u001b[0m 2025-01-09 21:26:04.801393: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=219591)\u001b[0m 2025-01-09 21:26:04.810693: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=219591)\u001b[0m 2025-01-09 21:26:04.823665: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=219591)\u001b[0m 2025-01-09 21:26:04.823706: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=219591)\u001b[0m 2025-01-09 21:26:04.831524: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=219591)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=219591)\u001b[0m 2025-01-09 21:26:05.387457: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:06.921824: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:06.945290: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:06.945354: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:06.947825: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:06.947888: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:06.947909: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:07.032992: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:07.033067: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:07.033075: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:07.033104: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:07.033129: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m I0000 00:00:1736438168.777989  219699 service.cc:145] XLA service 0x7fb078014f40 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m I0000 00:00:1736438168.778046  219699 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:08.807348: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m 2025-01-09 21:26:08.979478: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m I0000 00:00:1736438170.728731  219699 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000000)\n",
      "2025-01-09 21:26:17,479\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:17,480\tWARNING util.py:201 -- The `process_trial_result` operation took 1.382 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:17,481\tWARNING util.py:201 -- Processing trial results took 1.383 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:17,482\tWARNING util.py:201 -- The `process_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000001)\n",
      "2025-01-09 21:26:21,744\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.990 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:21,746\tWARNING util.py:201 -- The `process_trial_result` operation took 0.992 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:21,746\tWARNING util.py:201 -- Processing trial results took 0.992 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:21,747\tWARNING util.py:201 -- The `process_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000002)\n",
      "2025-01-09 21:26:26,190\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:26,192\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:26,192\tWARNING util.py:201 -- Processing trial results took 1.300 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:26,193\tWARNING util.py:201 -- The `process_trial_result` operation took 1.301 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000003)\n",
      "2025-01-09 21:26:30,417\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.176 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:30,419\tWARNING util.py:201 -- The `process_trial_result` operation took 1.177 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:30,420\tWARNING util.py:201 -- Processing trial results took 1.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:30,421\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000004)\n",
      "2025-01-09 21:26:34,729\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.249 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:34,731\tWARNING util.py:201 -- The `process_trial_result` operation took 1.251 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:34,732\tWARNING util.py:201 -- Processing trial results took 1.252 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:34,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.252 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000005)\n",
      "2025-01-09 21:26:39,265\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:39,266\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:39,267\tWARNING util.py:201 -- Processing trial results took 1.360 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:39,268\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000006)\n",
      "2025-01-09 21:26:43,665\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:43,667\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:43,668\tWARNING util.py:201 -- Processing trial results took 1.379 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:43,668\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000007)\n",
      "2025-01-09 21:26:48,151\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:48,154\tWARNING util.py:201 -- The `process_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:48,154\tWARNING util.py:201 -- Processing trial results took 1.298 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:48,155\tWARNING util.py:201 -- The `process_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000008)\n",
      "2025-01-09 21:26:52,395\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.084 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:52,396\tWARNING util.py:201 -- The `process_trial_result` operation took 1.085 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:52,397\tWARNING util.py:201 -- Processing trial results took 1.086 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:52,398\tWARNING util.py:201 -- The `process_trial_result` operation took 1.087 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000009)\n",
      "2025-01-09 21:26:56,938\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.215 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:56,940\tWARNING util.py:201 -- The `process_trial_result` operation took 1.217 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:26:56,941\tWARNING util.py:201 -- Processing trial results took 1.218 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:26:56,942\tWARNING util.py:201 -- The `process_trial_result` operation took 1.219 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000010)\n",
      "2025-01-09 21:27:01,340\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:01,342\tWARNING util.py:201 -- The `process_trial_result` operation took 1.233 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:01,342\tWARNING util.py:201 -- Processing trial results took 1.233 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:01,343\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000011)\n",
      "2025-01-09 21:27:05,805\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:05,807\tWARNING util.py:201 -- The `process_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:05,807\tWARNING util.py:201 -- Processing trial results took 1.374 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:05,808\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000012)\n",
      "2025-01-09 21:27:10,252\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:10,254\tWARNING util.py:201 -- The `process_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:10,255\tWARNING util.py:201 -- Processing trial results took 1.377 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:10,255\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000013)\n",
      "2025-01-09 21:27:15,587\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.486 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:15,589\tWARNING util.py:201 -- The `process_trial_result` operation took 1.489 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:15,589\tWARNING util.py:201 -- Processing trial results took 1.489 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:15,591\tWARNING util.py:201 -- The `process_trial_result` operation took 1.490 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000014)\n",
      "2025-01-09 21:27:20,280\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.489 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:20,282\tWARNING util.py:201 -- The `process_trial_result` operation took 1.491 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:20,282\tWARNING util.py:201 -- Processing trial results took 1.492 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:20,283\tWARNING util.py:201 -- The `process_trial_result` operation took 1.492 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000015)\n",
      "2025-01-09 21:27:24,869\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.455 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:24,871\tWARNING util.py:201 -- The `process_trial_result` operation took 1.457 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:24,871\tWARNING util.py:201 -- Processing trial results took 1.457 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:24,871\tWARNING util.py:201 -- The `process_trial_result` operation took 1.458 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000016)\n",
      "2025-01-09 21:27:29,973\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:29,974\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:29,975\tWARNING util.py:201 -- Processing trial results took 1.329 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:29,976\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000017)\n",
      "2025-01-09 21:27:34,553\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.429 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:34,555\tWARNING util.py:201 -- The `process_trial_result` operation took 1.431 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:34,556\tWARNING util.py:201 -- Processing trial results took 1.432 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:34,556\tWARNING util.py:201 -- The `process_trial_result` operation took 1.432 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000018)\n",
      "2025-01-09 21:27:39,257\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.557 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:39,258\tWARNING util.py:201 -- The `process_trial_result` operation took 1.559 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:39,259\tWARNING util.py:201 -- Processing trial results took 1.559 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:39,260\tWARNING util.py:201 -- The `process_trial_result` operation took 1.560 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=219591)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_6aede8f1_13_emb_unit=27,learning_rate=0.0064,num_layersm=2,num_layersu=2,units_m1=20,units_u1=49,unitsm_2=6,unitsm_3=2_2025-01-09_21-24-16/checkpoint_000019)\n",
      "2025-01-09 21:27:43,383\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.103 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:43,384\tWARNING util.py:201 -- The `process_trial_result` operation took 1.104 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:27:43,385\tWARNING util.py:201 -- Processing trial results took 1.105 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:27:43,386\tWARNING util.py:201 -- The `process_trial_result` operation took 1.106 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:27:43 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_6aede8f1 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/a9490a4add6b40ef85ae61a9a0b39468.\n",
      "2025/01/09 21:27:43 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=220913)\u001b[0m 2025-01-09 21:27:44.934471: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=220913)\u001b[0m 2025-01-09 21:27:44.946257: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=220913)\u001b[0m 2025-01-09 21:27:44.961594: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=220913)\u001b[0m 2025-01-09 21:27:44.961643: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=220913)\u001b[0m 2025-01-09 21:27:44.971809: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=220913)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=220913)\u001b[0m 2025-01-09 21:27:45.563475: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.287786: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.323480: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.323540: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.326560: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.326646: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.326668: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.418289: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.418365: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.418373: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.418400: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:47.418425: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m I0000 00:00:1736438269.101327  221022 service.cc:145] XLA service 0x7f9f48001a80 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m I0000 00:00:1736438269.101389  221022 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:49.132850: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m 2025-01-09 21:27:49.307774: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m I0000 00:00:1736438272.342480  221022 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_3', 4 bytes spill stores, 4 bytes spill loads\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m \n",
      "\u001b[36m(train_model pid=220913)\u001b[0m I0000 00:00:1736438272.347523  221022 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000000)\n",
      "2025-01-09 21:28:00,363\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:00,364\tWARNING util.py:201 -- The `process_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:00,365\tWARNING util.py:201 -- Processing trial results took 1.265 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:00,365\tWARNING util.py:201 -- The `process_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000001)\n",
      "2025-01-09 21:28:05,375\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:05,377\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:05,378\tWARNING util.py:201 -- Processing trial results took 1.364 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:05,379\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000002)\n",
      "2025-01-09 21:28:09,946\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:09,948\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:09,949\tWARNING util.py:201 -- Processing trial results took 1.317 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:09,949\tWARNING util.py:201 -- The `process_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000003)\n",
      "2025-01-09 21:28:15,041\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.484 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:15,042\tWARNING util.py:201 -- The `process_trial_result` operation took 1.485 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:15,043\tWARNING util.py:201 -- Processing trial results took 1.486 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:15,044\tWARNING util.py:201 -- The `process_trial_result` operation took 1.487 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000004)\n",
      "2025-01-09 21:28:19,705\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.080 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:19,707\tWARNING util.py:201 -- The `process_trial_result` operation took 1.082 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:19,707\tWARNING util.py:201 -- Processing trial results took 1.083 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:19,708\tWARNING util.py:201 -- The `process_trial_result` operation took 1.083 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000005)\n",
      "2025-01-09 21:28:24,850\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.403 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:24,852\tWARNING util.py:201 -- The `process_trial_result` operation took 1.405 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:24,853\tWARNING util.py:201 -- Processing trial results took 1.406 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:24,854\tWARNING util.py:201 -- The `process_trial_result` operation took 1.407 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000006)\n",
      "2025-01-09 21:28:29,896\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.266 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:29,898\tWARNING util.py:201 -- The `process_trial_result` operation took 1.268 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:29,899\tWARNING util.py:201 -- Processing trial results took 1.269 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:29,899\tWARNING util.py:201 -- The `process_trial_result` operation took 1.270 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000007)\n",
      "2025-01-09 21:28:34,737\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:34,739\tWARNING util.py:201 -- The `process_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:34,739\tWARNING util.py:201 -- Processing trial results took 1.373 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:34,740\tWARNING util.py:201 -- The `process_trial_result` operation took 1.374 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000008)\n",
      "2025-01-09 21:28:40,173\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.941 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:40,174\tWARNING util.py:201 -- The `process_trial_result` operation took 1.942 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:40,175\tWARNING util.py:201 -- Processing trial results took 1.943 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:40,175\tWARNING util.py:201 -- The `process_trial_result` operation took 1.943 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000009)\n",
      "2025-01-09 21:28:44,601\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.213 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:44,603\tWARNING util.py:201 -- The `process_trial_result` operation took 1.215 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:44,604\tWARNING util.py:201 -- Processing trial results took 1.216 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:44,604\tWARNING util.py:201 -- The `process_trial_result` operation took 1.216 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000010)\n",
      "2025-01-09 21:28:49,065\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.177 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:49,067\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:49,068\tWARNING util.py:201 -- Processing trial results took 1.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:49,068\tWARNING util.py:201 -- The `process_trial_result` operation took 1.180 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000011)\n",
      "2025-01-09 21:28:53,584\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.190 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:53,586\tWARNING util.py:201 -- The `process_trial_result` operation took 1.191 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:53,586\tWARNING util.py:201 -- Processing trial results took 1.192 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:53,587\tWARNING util.py:201 -- The `process_trial_result` operation took 1.192 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000012)\n",
      "2025-01-09 21:28:58,282\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:58,283\tWARNING util.py:201 -- The `process_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:28:58,284\tWARNING util.py:201 -- Processing trial results took 1.394 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:28:58,285\tWARNING util.py:201 -- The `process_trial_result` operation took 1.395 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000013)\n",
      "2025-01-09 21:29:02,928\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.426 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:02,929\tWARNING util.py:201 -- The `process_trial_result` operation took 1.428 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:02,930\tWARNING util.py:201 -- Processing trial results took 1.429 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:02,931\tWARNING util.py:201 -- The `process_trial_result` operation took 1.430 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000014)\n",
      "2025-01-09 21:29:07,589\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:07,590\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:07,591\tWARNING util.py:201 -- Processing trial results took 1.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:07,591\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000015)\n",
      "2025-01-09 21:29:12,188\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.299 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:12,190\tWARNING util.py:201 -- The `process_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:12,190\tWARNING util.py:201 -- Processing trial results took 1.302 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:12,191\tWARNING util.py:201 -- The `process_trial_result` operation took 1.303 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000016)\n",
      "2025-01-09 21:29:16,805\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:16,807\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:16,807\tWARNING util.py:201 -- Processing trial results took 1.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:16,808\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000017)\n",
      "2025-01-09 21:29:21,548\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:21,550\tWARNING util.py:201 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:21,550\tWARNING util.py:201 -- Processing trial results took 1.346 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:21,551\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000018)\n",
      "2025-01-09 21:29:26,054\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.176 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:26,056\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:26,056\tWARNING util.py:201 -- Processing trial results took 1.179 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:26,057\tWARNING util.py:201 -- The `process_trial_result` operation took 1.180 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=220913)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a0675423_14_emb_unit=27,learning_rate=0.0004,num_layersm=2,num_layersu=2,units_m1=5,units_u1=44,unitsm_2=29,unitsm_3=2_2025-01-09_21-26-06/checkpoint_000019)\n",
      "2025-01-09 21:29:30,585\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.251 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:30,586\tWARNING util.py:201 -- The `process_trial_result` operation took 1.253 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:30,587\tWARNING util.py:201 -- Processing trial results took 1.253 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:30,587\tWARNING util.py:201 -- The `process_trial_result` operation took 1.254 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:29:30 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_a0675423 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/5c6bbc4b5c5642ff87f3c8097e71c5b3.\n",
      "2025/01/09 21:29:30 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=222319)\u001b[0m 2025-01-09 21:29:33.351506: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=222319)\u001b[0m 2025-01-09 21:29:33.362757: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=222319)\u001b[0m 2025-01-09 21:29:33.378545: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=222319)\u001b[0m 2025-01-09 21:29:33.378596: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=222319)\u001b[0m 2025-01-09 21:29:33.387983: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=222319)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=222319)\u001b[0m 2025-01-09 21:29:34.360384: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.362209: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.402189: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.402257: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.405277: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.405339: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.405357: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.489250: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.489333: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.489341: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.489371: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:36.489397: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m I0000 00:00:1736438378.651804  222431 service.cc:145] XLA service 0x7faf18005d70 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m I0000 00:00:1736438378.651866  222431 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:38.686062: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m 2025-01-09 21:29:38.896445: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m I0000 00:00:1736438381.290519  222431 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000000)\n",
      "2025-01-09 21:29:48,730\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.326 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:48,731\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:48,732\tWARNING util.py:201 -- Processing trial results took 1.328 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:48,732\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000001)\n",
      "2025-01-09 21:29:53,250\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:53,252\tWARNING util.py:201 -- The `process_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:53,253\tWARNING util.py:201 -- Processing trial results took 1.310 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:53,253\tWARNING util.py:201 -- The `process_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000002)\n",
      "2025-01-09 21:29:57,377\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.023 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:57,379\tWARNING util.py:201 -- The `process_trial_result` operation took 1.024 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:29:57,380\tWARNING util.py:201 -- Processing trial results took 1.026 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:29:57,381\tWARNING util.py:201 -- The `process_trial_result` operation took 1.026 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000003)\n",
      "2025-01-09 21:30:01,904\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:01,906\tWARNING util.py:201 -- The `process_trial_result` operation took 1.329 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:01,907\tWARNING util.py:201 -- Processing trial results took 1.330 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:01,907\tWARNING util.py:201 -- The `process_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000004)\n",
      "2025-01-09 21:30:06,386\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:06,388\tWARNING util.py:201 -- The `process_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:06,388\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:06,389\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000005)\n",
      "2025-01-09 21:30:10,760\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.206 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:10,762\tWARNING util.py:201 -- The `process_trial_result` operation took 1.208 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:10,762\tWARNING util.py:201 -- Processing trial results took 1.208 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:10,763\tWARNING util.py:201 -- The `process_trial_result` operation took 1.209 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000006)\n",
      "2025-01-09 21:30:15,334\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:15,336\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:15,337\tWARNING util.py:201 -- Processing trial results took 1.345 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:15,337\tWARNING util.py:201 -- The `process_trial_result` operation took 1.345 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000007)\n",
      "2025-01-09 21:30:19,886\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:19,887\tWARNING util.py:201 -- The `process_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:19,888\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:19,889\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000008)\n",
      "2025-01-09 21:30:25,007\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.923 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:25,009\tWARNING util.py:201 -- The `process_trial_result` operation took 1.924 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:25,010\tWARNING util.py:201 -- Processing trial results took 1.925 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:25,011\tWARNING util.py:201 -- The `process_trial_result` operation took 1.926 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000009)\n",
      "2025-01-09 21:30:29,650\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:29,652\tWARNING util.py:201 -- The `process_trial_result` operation took 1.313 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:29,653\tWARNING util.py:201 -- Processing trial results took 1.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:29,654\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000010)\n",
      "2025-01-09 21:30:34,080\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:34,082\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:34,082\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:34,083\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000011)\n",
      "2025-01-09 21:30:38,550\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:38,552\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:38,553\tWARNING util.py:201 -- Processing trial results took 1.360 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:38,553\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000012)\n",
      "2025-01-09 21:30:42,983\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.130 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:42,984\tWARNING util.py:201 -- The `process_trial_result` operation took 1.132 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:42,985\tWARNING util.py:201 -- Processing trial results took 1.133 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:42,986\tWARNING util.py:201 -- The `process_trial_result` operation took 1.133 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000013)\n",
      "2025-01-09 21:30:47,553\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:47,554\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:47,555\tWARNING util.py:201 -- Processing trial results took 1.360 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:47,556\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000014)\n",
      "2025-01-09 21:30:52,109\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:52,111\tWARNING util.py:201 -- The `process_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:52,111\tWARNING util.py:201 -- Processing trial results took 1.357 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:52,112\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000015)\n",
      "2025-01-09 21:30:56,626\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:56,628\tWARNING util.py:201 -- The `process_trial_result` operation took 1.320 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:30:56,629\tWARNING util.py:201 -- Processing trial results took 1.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:30:56,629\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000016)\n",
      "2025-01-09 21:31:01,004\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.306 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:01,006\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:01,007\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:01,007\tWARNING util.py:201 -- The `process_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000017)\n",
      "2025-01-09 21:31:05,521\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:05,523\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:05,523\tWARNING util.py:201 -- Processing trial results took 1.350 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:05,524\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000018)\n",
      "2025-01-09 21:31:10,010\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.306 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:10,011\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:10,012\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:10,013\tWARNING util.py:201 -- The `process_trial_result` operation took 1.309 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=222319)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_33d1ecd6_15_emb_unit=24,learning_rate=0.0039,num_layersm=2,num_layersu=2,units_m1=5,units_u1=41,unitsm_2=8,unitsm_3=9,_2025-01-09_21-27-47/checkpoint_000019)\n",
      "2025-01-09 21:31:14,438\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.249 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:14,440\tWARNING util.py:201 -- The `process_trial_result` operation took 1.250 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:14,441\tWARNING util.py:201 -- Processing trial results took 1.251 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:14,441\tWARNING util.py:201 -- The `process_trial_result` operation took 1.251 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:31:14 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_33d1ecd6 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/2965d285cec54151a8d2bae02347404e.\n",
      "2025/01/09 21:31:14 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=223657)\u001b[0m 2025-01-09 21:31:16.885704: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=223657)\u001b[0m 2025-01-09 21:31:16.896246: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=223657)\u001b[0m 2025-01-09 21:31:16.910891: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=223657)\u001b[0m 2025-01-09 21:31:16.910940: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=223657)\u001b[0m 2025-01-09 21:31:16.919999: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=223657)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=223657)\u001b[0m 2025-01-09 21:31:17.508125: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.058872: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.089211: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.089275: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.092537: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.092632: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.092655: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.184654: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.184739: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.184748: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.184781: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:19.184806: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m I0000 00:00:1736438481.147491  223772 service.cc:145] XLA service 0x7f5d38012390 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m I0000 00:00:1736438481.147550  223772 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:21.179700: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m 2025-01-09 21:31:21.369449: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m I0000 00:00:1736438484.510574  223772 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000000)\n",
      "2025-01-09 21:31:33,188\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.330 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:33,189\tWARNING util.py:201 -- The `process_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:33,190\tWARNING util.py:201 -- Processing trial results took 1.332 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:33,190\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000001)\n",
      "2025-01-09 21:31:37,936\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.370 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:37,937\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:37,938\tWARNING util.py:201 -- Processing trial results took 1.372 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:37,939\tWARNING util.py:201 -- The `process_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000002)\n",
      "2025-01-09 21:31:42,426\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 0.993 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:42,427\tWARNING util.py:201 -- The `process_trial_result` operation took 0.994 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:42,428\tWARNING util.py:201 -- Processing trial results took 0.995 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:42,429\tWARNING util.py:201 -- The `process_trial_result` operation took 0.996 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000003)\n",
      "2025-01-09 21:31:47,112\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.212 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:47,114\tWARNING util.py:201 -- The `process_trial_result` operation took 1.214 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:47,115\tWARNING util.py:201 -- Processing trial results took 1.214 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:47,115\tWARNING util.py:201 -- The `process_trial_result` operation took 1.215 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000004)\n",
      "2025-01-09 21:31:51,836\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.305 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:51,837\tWARNING util.py:201 -- The `process_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:51,838\tWARNING util.py:201 -- Processing trial results took 1.307 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:51,839\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000005)\n",
      "2025-01-09 21:31:56,602\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.207 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:56,604\tWARNING util.py:201 -- The `process_trial_result` operation took 1.209 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:31:56,604\tWARNING util.py:201 -- Processing trial results took 1.210 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:31:56,605\tWARNING util.py:201 -- The `process_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000006)\n",
      "2025-01-09 21:32:01,338\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:01,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:01,340\tWARNING util.py:201 -- Processing trial results took 1.337 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:01,341\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000007)\n",
      "2025-01-09 21:32:06,510\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.424 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:06,512\tWARNING util.py:201 -- The `process_trial_result` operation took 1.425 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:06,513\tWARNING util.py:201 -- Processing trial results took 1.426 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:06,514\tWARNING util.py:201 -- The `process_trial_result` operation took 1.428 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000008)\n",
      "2025-01-09 21:32:11,293\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:11,294\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:11,296\tWARNING util.py:201 -- Processing trial results took 1.339 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:11,297\tWARNING util.py:201 -- The `process_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000009)\n",
      "2025-01-09 21:32:16,239\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:16,241\tWARNING util.py:201 -- The `process_trial_result` operation took 1.467 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:16,242\tWARNING util.py:201 -- Processing trial results took 1.468 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:16,242\tWARNING util.py:201 -- The `process_trial_result` operation took 1.468 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000010)\n",
      "2025-01-09 21:32:21,245\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.383 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:21,246\tWARNING util.py:201 -- The `process_trial_result` operation took 1.384 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:21,247\tWARNING util.py:201 -- Processing trial results took 1.385 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:21,248\tWARNING util.py:201 -- The `process_trial_result` operation took 1.386 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000011)\n",
      "2025-01-09 21:32:25,993\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:25,995\tWARNING util.py:201 -- The `process_trial_result` operation took 1.406 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:25,996\tWARNING util.py:201 -- Processing trial results took 1.407 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:25,997\tWARNING util.py:201 -- The `process_trial_result` operation took 1.407 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000012)\n",
      "2025-01-09 21:32:30,848\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:30,850\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:30,851\tWARNING util.py:201 -- Processing trial results took 1.361 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:30,851\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000013)\n",
      "2025-01-09 21:32:35,719\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.400 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:35,720\tWARNING util.py:201 -- The `process_trial_result` operation took 1.401 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:35,721\tWARNING util.py:201 -- Processing trial results took 1.402 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:35,721\tWARNING util.py:201 -- The `process_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000014)\n",
      "2025-01-09 21:32:40,519\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.304 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:40,520\tWARNING util.py:201 -- The `process_trial_result` operation took 1.306 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:40,521\tWARNING util.py:201 -- Processing trial results took 1.307 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:40,522\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000015)\n",
      "2025-01-09 21:32:45,572\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.448 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:45,573\tWARNING util.py:201 -- The `process_trial_result` operation took 1.450 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:45,574\tWARNING util.py:201 -- Processing trial results took 1.451 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:45,575\tWARNING util.py:201 -- The `process_trial_result` operation took 1.451 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000016)\n",
      "2025-01-09 21:32:50,252\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.341 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:50,254\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:50,255\tWARNING util.py:201 -- Processing trial results took 1.343 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:50,255\tWARNING util.py:201 -- The `process_trial_result` operation took 1.344 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000017)\n",
      "2025-01-09 21:32:55,123\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.394 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:55,125\tWARNING util.py:201 -- The `process_trial_result` operation took 1.396 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:55,126\tWARNING util.py:201 -- Processing trial results took 1.397 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:55,127\tWARNING util.py:201 -- The `process_trial_result` operation took 1.398 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000018)\n",
      "2025-01-09 21:32:59,883\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.290 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:59,885\tWARNING util.py:201 -- The `process_trial_result` operation took 1.292 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:32:59,885\tWARNING util.py:201 -- Processing trial results took 1.292 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:32:59,886\tWARNING util.py:201 -- The `process_trial_result` operation took 1.293 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=223657)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_a64c6e17_16_emb_unit=28,learning_rate=0.0001,num_layersm=3,num_layersu=2,units_m1=5,units_u1=9,unitsm_2=27,unitsm_3=46_2025-01-09_21-29-36/checkpoint_000019)\n",
      "2025-01-09 21:33:04,826\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.463 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:04,827\tWARNING util.py:201 -- The `process_trial_result` operation took 1.465 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:04,828\tWARNING util.py:201 -- Processing trial results took 1.466 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:04,829\tWARNING util.py:201 -- The `process_trial_result` operation took 1.466 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:33:05 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_a64c6e17 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/efec2e6231e94e228d8d368df64e94b8.\n",
      "2025/01/09 21:33:05 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=225116)\u001b[0m 2025-01-09 21:33:07.489445: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=225116)\u001b[0m 2025-01-09 21:33:07.500999: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=225116)\u001b[0m 2025-01-09 21:33:07.516666: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=225116)\u001b[0m 2025-01-09 21:33:07.516712: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=225116)\u001b[0m 2025-01-09 21:33:07.526173: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=225116)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=225116)\u001b[0m 2025-01-09 21:33:08.425909: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.500049: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.538929: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.538988: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.542650: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.542735: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.542756: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.638999: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.639082: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.639089: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.639113: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:10.639139: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m I0000 00:00:1736438592.679268  225231 service.cc:145] XLA service 0x7fce68005620 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m I0000 00:00:1736438592.679317  225231 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:12.715375: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m 2025-01-09 21:33:12.926867: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m I0000 00:00:1736438596.114703  225231 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m I0000 00:00:1736438601.186120  225231 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_1', 8 bytes spill stores, 8 bytes spill loads\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m \n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000000)\n",
      "2025-01-09 21:33:24,459\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:24,461\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:24,462\tWARNING util.py:201 -- Processing trial results took 1.378 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:24,462\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000001)\n",
      "2025-01-09 21:33:29,144\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.336 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:29,146\tWARNING util.py:201 -- The `process_trial_result` operation took 1.338 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:29,146\tWARNING util.py:201 -- Processing trial results took 1.338 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:29,146\tWARNING util.py:201 -- The `process_trial_result` operation took 1.339 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000002)\n",
      "2025-01-09 21:33:33,896\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.420 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:33,898\tWARNING util.py:201 -- The `process_trial_result` operation took 1.422 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:33,899\tWARNING util.py:201 -- Processing trial results took 1.423 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:33,899\tWARNING util.py:201 -- The `process_trial_result` operation took 1.423 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000003)\n",
      "2025-01-09 21:33:39,144\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.412 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:39,146\tWARNING util.py:201 -- The `process_trial_result` operation took 1.414 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:39,146\tWARNING util.py:201 -- Processing trial results took 1.415 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:39,147\tWARNING util.py:201 -- The `process_trial_result` operation took 1.415 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000004)\n",
      "2025-01-09 21:33:44,082\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.532 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:44,084\tWARNING util.py:201 -- The `process_trial_result` operation took 1.535 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:44,085\tWARNING util.py:201 -- Processing trial results took 1.536 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:44,085\tWARNING util.py:201 -- The `process_trial_result` operation took 1.536 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000005)\n",
      "2025-01-09 21:33:49,093\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.280 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:49,095\tWARNING util.py:201 -- The `process_trial_result` operation took 1.282 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:49,096\tWARNING util.py:201 -- Processing trial results took 1.282 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:49,096\tWARNING util.py:201 -- The `process_trial_result` operation took 1.283 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000006)\n",
      "2025-01-09 21:33:53,879\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.259 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:53,881\tWARNING util.py:201 -- The `process_trial_result` operation took 1.261 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:53,881\tWARNING util.py:201 -- Processing trial results took 1.261 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:53,882\tWARNING util.py:201 -- The `process_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000007)\n",
      "2025-01-09 21:33:58,893\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:58,895\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:33:58,896\tWARNING util.py:201 -- Processing trial results took 1.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:33:58,897\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000008)\n",
      "2025-01-09 21:34:03,578\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:03,580\tWARNING util.py:201 -- The `process_trial_result` operation took 1.380 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:03,580\tWARNING util.py:201 -- Processing trial results took 1.381 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:03,581\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000009)\n",
      "2025-01-09 21:34:08,223\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:08,224\tWARNING util.py:201 -- The `process_trial_result` operation took 1.370 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:08,225\tWARNING util.py:201 -- Processing trial results took 1.371 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:08,226\tWARNING util.py:201 -- The `process_trial_result` operation took 1.371 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000010)\n",
      "2025-01-09 21:34:13,198\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:13,199\tWARNING util.py:201 -- The `process_trial_result` operation took 1.378 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:13,200\tWARNING util.py:201 -- Processing trial results took 1.379 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:13,201\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000011)\n",
      "2025-01-09 21:34:18,036\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:18,037\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:18,038\tWARNING util.py:201 -- Processing trial results took 1.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:18,039\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000012)\n",
      "2025-01-09 21:34:22,506\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:22,508\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:22,508\tWARNING util.py:201 -- Processing trial results took 1.335 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:22,509\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000013)\n",
      "2025-01-09 21:34:27,326\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:27,327\tWARNING util.py:201 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:27,328\tWARNING util.py:201 -- Processing trial results took 1.356 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:27,328\tWARNING util.py:201 -- The `process_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000014)\n",
      "2025-01-09 21:34:31,942\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.307 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:31,944\tWARNING util.py:201 -- The `process_trial_result` operation took 1.308 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:31,945\tWARNING util.py:201 -- Processing trial results took 1.309 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:31,945\tWARNING util.py:201 -- The `process_trial_result` operation took 1.310 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000015)\n",
      "2025-01-09 21:34:36,539\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:36,540\tWARNING util.py:201 -- The `process_trial_result` operation took 1.335 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:36,541\tWARNING util.py:201 -- Processing trial results took 1.336 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:36,542\tWARNING util.py:201 -- The `process_trial_result` operation took 1.337 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000016)\n",
      "2025-01-09 21:34:41,361\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.211 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:41,362\tWARNING util.py:201 -- The `process_trial_result` operation took 1.212 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:41,364\tWARNING util.py:201 -- Processing trial results took 1.214 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:41,364\tWARNING util.py:201 -- The `process_trial_result` operation took 1.214 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000017)\n",
      "2025-01-09 21:34:46,104\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:46,105\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:46,106\tWARNING util.py:201 -- Processing trial results took 1.348 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:46,107\tWARNING util.py:201 -- The `process_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000018)\n",
      "2025-01-09 21:34:50,691\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.229 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:50,692\tWARNING util.py:201 -- The `process_trial_result` operation took 1.231 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:50,692\tWARNING util.py:201 -- Processing trial results took 1.231 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:50,693\tWARNING util.py:201 -- The `process_trial_result` operation took 1.232 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=225116)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_2bd8f9f3_17_emb_unit=17,learning_rate=0.0007,num_layersm=3,num_layersu=3,units_m1=25,units_u1=8,unitsm_2=35,unitsm_3=8_2025-01-09_21-31-19/checkpoint_000019)\n",
      "2025-01-09 21:34:55,182\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.009 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:55,183\tWARNING util.py:201 -- The `process_trial_result` operation took 1.011 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:34:55,184\tWARNING util.py:201 -- Processing trial results took 1.012 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:34:55,184\tWARNING util.py:201 -- The `process_trial_result` operation took 1.012 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:34:55 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_2bd8f9f3 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/cfd2724327fa4782b78b75c439459350.\n",
      "2025/01/09 21:34:55 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=226560)\u001b[0m 2025-01-09 21:34:57.414636: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=226560)\u001b[0m 2025-01-09 21:34:57.426130: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=226560)\u001b[0m 2025-01-09 21:34:57.442347: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=226560)\u001b[0m 2025-01-09 21:34:57.442403: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=226560)\u001b[0m 2025-01-09 21:34:57.451981: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=226560)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=226560)\u001b[0m 2025-01-09 21:34:58.397664: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.437525: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.480315: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.480367: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.484541: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.484680: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.484700: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.567128: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.567196: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.567203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.567227: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:00.567250: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m I0000 00:00:1736438702.485410  226675 service.cc:145] XLA service 0x7f5988002250 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m I0000 00:00:1736438702.485462  226675 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:02.516432: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m 2025-01-09 21:35:02.694069: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m I0000 00:00:1736438705.587527  226675 asm_compiler.cc:369] ptxas warning : Registers are spilled to local memory in function 'input_slice_fusion_3', 4 bytes spill stores, 4 bytes spill loads\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m \n",
      "\u001b[36m(train_model pid=226560)\u001b[0m I0000 00:00:1736438705.590802  226675 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000000)\n",
      "2025-01-09 21:35:14,009\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.494 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:14,011\tWARNING util.py:201 -- The `process_trial_result` operation took 1.496 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:14,012\tWARNING util.py:201 -- Processing trial results took 1.496 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:14,012\tWARNING util.py:201 -- The `process_trial_result` operation took 1.497 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000001)\n",
      "2025-01-09 21:35:18,929\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:18,931\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:18,932\tWARNING util.py:201 -- Processing trial results took 1.361 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:18,932\tWARNING util.py:201 -- The `process_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000002)\n",
      "2025-01-09 21:35:23,582\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.332 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:23,584\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:23,584\tWARNING util.py:201 -- Processing trial results took 1.334 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:23,585\tWARNING util.py:201 -- The `process_trial_result` operation took 1.334 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000003)\n",
      "2025-01-09 21:35:28,343\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:28,344\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:28,345\tWARNING util.py:201 -- Processing trial results took 1.353 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:28,347\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000004)\n",
      "2025-01-09 21:35:32,819\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:32,822\tWARNING util.py:201 -- The `process_trial_result` operation took 1.346 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:32,823\tWARNING util.py:201 -- Processing trial results took 1.347 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:32,823\tWARNING util.py:201 -- The `process_trial_result` operation took 1.347 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000005)\n",
      "2025-01-09 21:35:37,499\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.340 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:37,501\tWARNING util.py:201 -- The `process_trial_result` operation took 1.342 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:37,501\tWARNING util.py:201 -- Processing trial results took 1.342 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:37,502\tWARNING util.py:201 -- The `process_trial_result` operation took 1.343 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000006)\n",
      "2025-01-09 21:35:42,140\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.331 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:42,142\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:42,142\tWARNING util.py:201 -- Processing trial results took 1.333 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:42,143\tWARNING util.py:201 -- The `process_trial_result` operation took 1.333 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000007)\n",
      "2025-01-09 21:35:46,905\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.497 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:46,907\tWARNING util.py:201 -- The `process_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:46,908\tWARNING util.py:201 -- Processing trial results took 1.499 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:46,908\tWARNING util.py:201 -- The `process_trial_result` operation took 1.500 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000008)\n",
      "2025-01-09 21:35:51,434\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.262 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:51,435\tWARNING util.py:201 -- The `process_trial_result` operation took 1.263 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:51,436\tWARNING util.py:201 -- Processing trial results took 1.264 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:51,436\tWARNING util.py:201 -- The `process_trial_result` operation took 1.265 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000009)\n",
      "2025-01-09 21:35:56,319\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.372 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:56,321\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:35:56,322\tWARNING util.py:201 -- Processing trial results took 1.376 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:35:56,323\tWARNING util.py:201 -- The `process_trial_result` operation took 1.376 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000010)\n",
      "2025-01-09 21:36:01,134\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.318 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:01,135\tWARNING util.py:201 -- The `process_trial_result` operation took 1.319 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:01,136\tWARNING util.py:201 -- Processing trial results took 1.320 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:01,136\tWARNING util.py:201 -- The `process_trial_result` operation took 1.321 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000011)\n",
      "2025-01-09 21:36:06,150\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.405 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:06,151\tWARNING util.py:201 -- The `process_trial_result` operation took 1.407 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:06,153\tWARNING util.py:201 -- Processing trial results took 1.409 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:06,153\tWARNING util.py:201 -- The `process_trial_result` operation took 1.409 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000012)\n",
      "2025-01-09 21:36:11,099\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:11,100\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:11,101\tWARNING util.py:201 -- Processing trial results took 1.355 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:11,102\tWARNING util.py:201 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000013)\n",
      "2025-01-09 21:36:15,532\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.325 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:15,533\tWARNING util.py:201 -- The `process_trial_result` operation took 1.327 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:15,534\tWARNING util.py:201 -- Processing trial results took 1.328 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:15,534\tWARNING util.py:201 -- The `process_trial_result` operation took 1.328 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000014)\n",
      "2025-01-09 21:36:20,428\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.311 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:20,430\tWARNING util.py:201 -- The `process_trial_result` operation took 1.313 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:20,431\tWARNING util.py:201 -- Processing trial results took 1.314 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:20,432\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000015)\n",
      "2025-01-09 21:36:25,384\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.092 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:25,386\tWARNING util.py:201 -- The `process_trial_result` operation took 1.095 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:25,387\tWARNING util.py:201 -- Processing trial results took 1.095 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:25,388\tWARNING util.py:201 -- The `process_trial_result` operation took 1.097 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000016)\n",
      "2025-01-09 21:36:30,338\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.298 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:30,340\tWARNING util.py:201 -- The `process_trial_result` operation took 1.300 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:30,341\tWARNING util.py:201 -- Processing trial results took 1.301 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:30,342\tWARNING util.py:201 -- The `process_trial_result` operation took 1.302 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000017)\n",
      "2025-01-09 21:36:35,069\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.446 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:35,070\tWARNING util.py:201 -- The `process_trial_result` operation took 1.447 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:35,072\tWARNING util.py:201 -- Processing trial results took 1.448 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:35,072\tWARNING util.py:201 -- The `process_trial_result` operation took 1.449 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000018)\n",
      "2025-01-09 21:36:39,685\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:39,687\tWARNING util.py:201 -- The `process_trial_result` operation took 1.395 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:39,688\tWARNING util.py:201 -- Processing trial results took 1.395 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:39,688\tWARNING util.py:201 -- The `process_trial_result` operation took 1.396 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=226560)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_fb38c247_18_emb_unit=24,learning_rate=0.0078,num_layersm=2,num_layersu=2,units_m1=25,units_u1=32,unitsm_2=22,unitsm_3=_2025-01-09_21-33-10/checkpoint_000019)\n",
      "2025-01-09 21:36:44,236\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:44,237\tWARNING util.py:201 -- The `process_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:36:44,238\tWARNING util.py:201 -- Processing trial results took 1.367 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:36:44,239\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:36:44 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_fb38c247 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/11ac333a3c1a4861837667bfdbf57597.\n",
      "2025/01/09 21:36:44 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=227974)\u001b[0m 2025-01-09 21:36:45.999530: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=227974)\u001b[0m 2025-01-09 21:36:46.012804: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=227974)\u001b[0m 2025-01-09 21:36:46.033964: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=227974)\u001b[0m 2025-01-09 21:36:46.034016: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=227974)\u001b[0m 2025-01-09 21:36:46.045740: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=227974)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=227974)\u001b[0m 2025-01-09 21:36:46.836355: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.616774: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.650301: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.650369: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.653828: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.653900: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.653921: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.755526: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.755609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.755617: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.755651: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:48.755679: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m I0000 00:00:1736438810.612141  228088 service.cc:145] XLA service 0x7f3c40005460 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m I0000 00:00:1736438810.612196  228088 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:50.643651: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m 2025-01-09 21:36:50.827430: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m I0000 00:00:1736438814.279732  228088 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000000)\n",
      "2025-01-09 21:37:03,356\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.136 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:03,357\tWARNING util.py:201 -- The `process_trial_result` operation took 1.138 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:03,358\tWARNING util.py:201 -- Processing trial results took 1.138 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:03,358\tWARNING util.py:201 -- The `process_trial_result` operation took 1.139 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000001)\n",
      "2025-01-09 21:37:08,204\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.396 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:08,206\tWARNING util.py:201 -- The `process_trial_result` operation took 1.397 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:08,206\tWARNING util.py:201 -- Processing trial results took 1.398 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:08,206\tWARNING util.py:201 -- The `process_trial_result` operation took 1.398 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000002)\n",
      "2025-01-09 21:37:13,273\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.467 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:13,275\tWARNING util.py:201 -- The `process_trial_result` operation took 1.469 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:13,275\tWARNING util.py:201 -- Processing trial results took 1.469 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:13,276\tWARNING util.py:201 -- The `process_trial_result` operation took 1.470 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000003)\n",
      "2025-01-09 21:37:18,822\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:18,824\tWARNING util.py:201 -- The `process_trial_result` operation took 1.314 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:18,826\tWARNING util.py:201 -- Processing trial results took 1.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:18,827\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000004)\n",
      "2025-01-09 21:37:23,721\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.500 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:23,724\tWARNING util.py:201 -- The `process_trial_result` operation took 1.502 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:23,724\tWARNING util.py:201 -- Processing trial results took 1.503 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:23,725\tWARNING util.py:201 -- The `process_trial_result` operation took 1.503 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000005)\n",
      "2025-01-09 21:37:28,964\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.166 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:28,966\tWARNING util.py:201 -- The `process_trial_result` operation took 1.169 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:28,966\tWARNING util.py:201 -- Processing trial results took 1.169 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:28,967\tWARNING util.py:201 -- The `process_trial_result` operation took 1.170 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000006)\n",
      "2025-01-09 21:37:34,086\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.410 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:34,088\tWARNING util.py:201 -- The `process_trial_result` operation took 1.411 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:34,089\tWARNING util.py:201 -- Processing trial results took 1.413 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:34,089\tWARNING util.py:201 -- The `process_trial_result` operation took 1.413 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000007)\n",
      "2025-01-09 21:37:39,259\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.543 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:39,260\tWARNING util.py:201 -- The `process_trial_result` operation took 1.545 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:39,261\tWARNING util.py:201 -- Processing trial results took 1.545 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:39,262\tWARNING util.py:201 -- The `process_trial_result` operation took 1.547 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000008)\n",
      "2025-01-09 21:37:44,179\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.432 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:44,181\tWARNING util.py:201 -- The `process_trial_result` operation took 1.434 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:44,182\tWARNING util.py:201 -- Processing trial results took 1.435 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:44,183\tWARNING util.py:201 -- The `process_trial_result` operation took 1.436 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000009)\n",
      "2025-01-09 21:37:49,268\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.275 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:49,269\tWARNING util.py:201 -- The `process_trial_result` operation took 1.276 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:49,270\tWARNING util.py:201 -- Processing trial results took 1.277 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:49,272\tWARNING util.py:201 -- The `process_trial_result` operation took 1.279 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000010)\n",
      "2025-01-09 21:37:54,836\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.616 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:54,838\tWARNING util.py:201 -- The `process_trial_result` operation took 1.618 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:54,838\tWARNING util.py:201 -- Processing trial results took 1.619 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:54,839\tWARNING util.py:201 -- The `process_trial_result` operation took 1.619 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000011)\n",
      "2025-01-09 21:37:59,610\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.388 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:59,611\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:37:59,612\tWARNING util.py:201 -- Processing trial results took 1.391 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:37:59,613\tWARNING util.py:201 -- The `process_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000012)\n",
      "2025-01-09 21:38:04,334\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:04,335\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:04,337\tWARNING util.py:201 -- Processing trial results took 1.359 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:04,338\tWARNING util.py:201 -- The `process_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000013)\n",
      "2025-01-09 21:38:09,234\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.230 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:09,236\tWARNING util.py:201 -- The `process_trial_result` operation took 1.233 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:09,237\tWARNING util.py:201 -- Processing trial results took 1.233 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:09,237\tWARNING util.py:201 -- The `process_trial_result` operation took 1.234 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000014)\n",
      "2025-01-09 21:38:14,291\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.504 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:14,293\tWARNING util.py:201 -- The `process_trial_result` operation took 1.506 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:14,293\tWARNING util.py:201 -- Processing trial results took 1.506 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:14,294\tWARNING util.py:201 -- The `process_trial_result` operation took 1.507 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000015)\n",
      "2025-01-09 21:38:19,422\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.499 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:19,424\tWARNING util.py:201 -- The `process_trial_result` operation took 1.501 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:19,424\tWARNING util.py:201 -- Processing trial results took 1.502 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:19,425\tWARNING util.py:201 -- The `process_trial_result` operation took 1.502 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000016)\n",
      "2025-01-09 21:38:24,704\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.293 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:24,705\tWARNING util.py:201 -- The `process_trial_result` operation took 1.295 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:24,707\tWARNING util.py:201 -- Processing trial results took 1.296 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:24,708\tWARNING util.py:201 -- The `process_trial_result` operation took 1.297 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000017)\n",
      "2025-01-09 21:38:30,033\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.441 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:30,034\tWARNING util.py:201 -- The `process_trial_result` operation took 1.442 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:30,035\tWARNING util.py:201 -- Processing trial results took 1.443 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:30,036\tWARNING util.py:201 -- The `process_trial_result` operation took 1.443 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000018)\n",
      "2025-01-09 21:38:34,813\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.389 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:34,814\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:34,815\tWARNING util.py:201 -- Processing trial results took 1.391 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:34,816\tWARNING util.py:201 -- The `process_trial_result` operation took 1.392 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=227974)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_40645985_19_emb_unit=25,learning_rate=0.0036,num_layersm=2,num_layersu=3,units_m1=22,units_u1=48,unitsm_2=34,unitsm_3=_2025-01-09_21-35-00/checkpoint_000019)\n",
      "2025-01-09 21:38:39,572\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.215 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:39,573\tWARNING util.py:201 -- The `process_trial_result` operation took 1.216 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:38:39,574\tWARNING util.py:201 -- Processing trial results took 1.217 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:38:39,574\tWARNING util.py:201 -- The `process_trial_result` operation took 1.218 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:38:39 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_40645985 at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/e6e27ec725db4a0a92a75d855f0a22f1.\n",
      "2025/01/09 21:38:39 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(pid=229444)\u001b[0m 2025-01-09 21:38:42.481248: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "\u001b[36m(pid=229444)\u001b[0m 2025-01-09 21:38:42.491739: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:479] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "\u001b[36m(pid=229444)\u001b[0m 2025-01-09 21:38:42.505287: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:10575] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "\u001b[36m(pid=229444)\u001b[0m 2025-01-09 21:38:42.505326: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1442] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "\u001b[36m(pid=229444)\u001b[0m 2025-01-09 21:38:42.513768: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "\u001b[36m(pid=229444)\u001b[0m To enable the following instructions: AVX2 AVX_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "\u001b[36m(pid=229444)\u001b[0m 2025-01-09 21:38:43.477219: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.712131: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.750809: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.750880: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.753496: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.753593: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.753609: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.851319: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.851392: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.851399: I tensorflow/core/common_runtime/gpu/gpu_device.cc:2019] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.851426: I external/local_xla/xla/stream_executor/cuda/cuda_executor.cc:984] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Your kernel may have been built without NUMA support.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:45.851453: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1928] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1767 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 3050 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m I0000 00:00:1736438928.129083  229558 service.cc:145] XLA service 0x7fead0005580 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m I0000 00:00:1736438928.129154  229558 service.cc:153]   StreamExecutor device (0): NVIDIA GeForce RTX 3050 Laptop GPU, Compute Capability 8.6\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:48.167178: I tensorflow/compiler/mlir/tensorflow/utils/dump_mlir_util.cc:268] disabling MLIR crash reproducer, set env var `MLIR_CRASH_REPRODUCER_DIRECTORY` to enable.\n",
      "\u001b[33m(raylet)\u001b[0m /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
      "\u001b[33m(raylet)\u001b[0m   warnings.warn(\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m 2025-01-09 21:38:48.399019: I external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:465] Loaded cuDNN version 8907\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m I0000 00:00:1736438933.323009  229558 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000000)\n",
      "2025-01-09 21:39:03,621\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.387 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:03,622\tWARNING util.py:201 -- The `process_trial_result` operation took 1.389 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:03,623\tWARNING util.py:201 -- Processing trial results took 1.389 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:03,623\tWARNING util.py:201 -- The `process_trial_result` operation took 1.390 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000001)\n",
      "2025-01-09 21:39:08,280\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.038 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:08,282\tWARNING util.py:201 -- The `process_trial_result` operation took 1.040 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:08,282\tWARNING util.py:201 -- Processing trial results took 1.040 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:08,283\tWARNING util.py:201 -- The `process_trial_result` operation took 1.041 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000002)\n",
      "2025-01-09 21:39:13,367\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:13,368\tWARNING util.py:201 -- The `process_trial_result` operation took 1.355 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:13,369\tWARNING util.py:201 -- Processing trial results took 1.356 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:13,370\tWARNING util.py:201 -- The `process_trial_result` operation took 1.357 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000003)\n",
      "2025-01-09 21:39:18,249\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.264 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:18,251\tWARNING util.py:201 -- The `process_trial_result` operation took 1.266 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:18,251\tWARNING util.py:201 -- Processing trial results took 1.267 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:18,252\tWARNING util.py:201 -- The `process_trial_result` operation took 1.268 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000004)\n",
      "2025-01-09 21:39:23,026\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.147 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:23,028\tWARNING util.py:201 -- The `process_trial_result` operation took 1.149 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:23,029\tWARNING util.py:201 -- Processing trial results took 1.150 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:23,029\tWARNING util.py:201 -- The `process_trial_result` operation took 1.150 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000005)\n",
      "2025-01-09 21:39:28,193\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:28,194\tWARNING util.py:201 -- The `process_trial_result` operation took 1.379 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:28,195\tWARNING util.py:201 -- Processing trial results took 1.380 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:28,197\tWARNING util.py:201 -- The `process_trial_result` operation took 1.381 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000006)\n",
      "2025-01-09 21:39:33,035\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.356 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:33,037\tWARNING util.py:201 -- The `process_trial_result` operation took 1.358 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:33,038\tWARNING util.py:201 -- Processing trial results took 1.358 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:33,038\tWARNING util.py:201 -- The `process_trial_result` operation took 1.359 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000007)\n",
      "2025-01-09 21:39:37,799\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.177 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:37,801\tWARNING util.py:201 -- The `process_trial_result` operation took 1.179 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:37,802\tWARNING util.py:201 -- Processing trial results took 1.180 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:37,803\tWARNING util.py:201 -- The `process_trial_result` operation took 1.180 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000008)\n",
      "2025-01-09 21:39:42,910\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.383 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:42,912\tWARNING util.py:201 -- The `process_trial_result` operation took 1.385 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:42,913\tWARNING util.py:201 -- Processing trial results took 1.386 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:42,913\tWARNING util.py:201 -- The `process_trial_result` operation took 1.386 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000009)\n",
      "2025-01-09 21:39:47,862\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.360 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:47,864\tWARNING util.py:201 -- The `process_trial_result` operation took 1.362 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:47,865\tWARNING util.py:201 -- Processing trial results took 1.362 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:47,865\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000010)\n",
      "2025-01-09 21:39:52,793\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.361 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:52,795\tWARNING util.py:201 -- The `process_trial_result` operation took 1.363 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:52,796\tWARNING util.py:201 -- Processing trial results took 1.363 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:52,796\tWARNING util.py:201 -- The `process_trial_result` operation took 1.364 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000011)\n",
      "2025-01-09 21:39:57,487\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.203 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:57,489\tWARNING util.py:201 -- The `process_trial_result` operation took 1.205 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:39:57,490\tWARNING util.py:201 -- Processing trial results took 1.206 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:39:57,491\tWARNING util.py:201 -- The `process_trial_result` operation took 1.206 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000012)\n",
      "2025-01-09 21:40:02,489\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.349 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:02,490\tWARNING util.py:201 -- The `process_trial_result` operation took 1.350 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:02,492\tWARNING util.py:201 -- Processing trial results took 1.351 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:02,492\tWARNING util.py:201 -- The `process_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000013)\n",
      "2025-01-09 21:40:07,399\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.352 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:07,400\tWARNING util.py:201 -- The `process_trial_result` operation took 1.353 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:07,401\tWARNING util.py:201 -- Processing trial results took 1.354 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:07,401\tWARNING util.py:201 -- The `process_trial_result` operation took 1.354 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000014)\n",
      "2025-01-09 21:40:12,495\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.447 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:12,496\tWARNING util.py:201 -- The `process_trial_result` operation took 1.448 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:12,497\tWARNING util.py:201 -- Processing trial results took 1.449 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:12,498\tWARNING util.py:201 -- The `process_trial_result` operation took 1.450 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000015)\n",
      "2025-01-09 21:40:17,406\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.365 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:17,407\tWARNING util.py:201 -- The `process_trial_result` operation took 1.367 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:17,408\tWARNING util.py:201 -- Processing trial results took 1.368 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:17,408\tWARNING util.py:201 -- The `process_trial_result` operation took 1.368 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000016)\n",
      "2025-01-09 21:40:23,139\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.402 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:23,141\tWARNING util.py:201 -- The `process_trial_result` operation took 1.404 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:23,141\tWARNING util.py:201 -- Processing trial results took 1.404 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:23,142\tWARNING util.py:201 -- The `process_trial_result` operation took 1.405 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000017)\n",
      "2025-01-09 21:40:27,968\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.312 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:27,970\tWARNING util.py:201 -- The `process_trial_result` operation took 1.315 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:27,971\tWARNING util.py:201 -- Processing trial results took 1.315 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:27,971\tWARNING util.py:201 -- The `process_trial_result` operation took 1.316 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000018)\n",
      "2025-01-09 21:40:32,766\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.257 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:32,768\tWARNING util.py:201 -- The `process_trial_result` operation took 1.258 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:32,768\tWARNING util.py:201 -- Processing trial results took 1.259 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:32,770\tWARNING util.py:201 -- The `process_trial_result` operation took 1.260 s, which may be a performance bottleneck.\n",
      "\u001b[36m(train_model pid=229444)\u001b[0m Checkpoint successfully created at: Checkpoint(filesystem=local, path=/root/ray_results/agah_bohb/train_model_1be74f9e_20_emb_unit=29,learning_rate=0.0042,num_layersm=3,num_layersu=3,units_m1=15,units_u1=24,unitsm_2=35,unitsm_3=_2025-01-09_21-36-48/checkpoint_000019)\n",
      "2025-01-09 21:40:37,651\tWARNING util.py:201 -- The `callbacks.on_trial_result` operation took 1.373 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:37,653\tWARNING util.py:201 -- The `process_trial_result` operation took 1.375 s, which may be a performance bottleneck.\n",
      "2025-01-09 21:40:37,654\tWARNING util.py:201 -- Processing trial results took 1.376 s, which may be a performance bottleneck. Please consider reporting results less frequently to Ray Tune.\n",
      "2025-01-09 21:40:37,654\tWARNING util.py:201 -- The `process_trial_result` operation took 1.377 s, which may be a performance bottleneck.\n",
      "2025/01/09 21:40:37 INFO mlflow.tracking._tracking_service.client: 🏃 View run train_model_1be74f9e at: http://127.0.0.1:5000/#/experiments/494741450204073734/runs/7e8d72e4cc5d4258ae4dc048b7c26e61.\n",
      "2025/01/09 21:40:37 INFO mlflow.tracking._tracking_service.client: 🧪 View experiment at: http://127.0.0.1:5000/#/experiments/494741450204073734.\n",
      "2025-01-09 21:40:38,041\tINFO tune.py:1009 -- Wrote the latest version of all result files and experiment state to '/root/ray_results/agah_bohb' in 0.0098s.\n",
      "2025-01-09 21:40:38,050\tINFO tune.py:1041 -- Total run time: 2161.44 seconds (2161.07 seconds for the tuning loop).\n"
     ]
    }
   ],
   "source": [
    "# from ray.tune.search.bohb import TuneBOHB\n",
    "# from ray.tune.schedulers import HyperBandForBOHB\n",
    "from ray import tune\n",
    "from ray.tune.search.bayesopt import BayesOptSearch\n",
    "from ray.tune.search.hyperopt import HyperOptSearch\n",
    "\n",
    "# Define the search space for hyperparameters\n",
    "search_space = {\n",
    "    \"units_u1\": tune.randint(1, 50),\n",
    "    \"units_m1\": tune.randint(1, 50),\n",
    "    \"num_layersu\": tune.randint(2, 4),\n",
    "    \"num_layersm\": tune.randint(2, 4),\n",
    "    \"learning_rate\": tune.loguniform(1e-4, 1e-2),\n",
    "    \"emb_unit\": tune.randint(16, 32)\n",
    "}\n",
    "for i in range(2, 5):  # Add units for layers dynamically based on max num_layers\n",
    "    search_space[f'unitsu_{i}'] = tune.randint(1, 50)\n",
    "    search_space[f'unitsm_{i}'] = tune.randint(1, 50)\n",
    "\n",
    "# # Set up the scheduler for BOHB\n",
    "# scheduler = HyperBandForBOHB(\n",
    "#     time_attr=\"training_iteration\", \n",
    "#     metric=\"mse\", \n",
    "#     mode=\"min\",\n",
    "#     max_t=50,  # Maximum iterations\n",
    "#     reduction_factor=2  # Successive halving reduction factor\n",
    "# )\n",
    "\n",
    "# Set up BOHB search algorithm\n",
    "# search_algo = TuneBOHB(metric=\"mse\", mode=\"min\")\n",
    "hyperopt_search = HyperOptSearch(\n",
    "    metric=\"mse\", mode=\"min\")\n",
    "\n",
    "# Configure the tuner\n",
    "tuner = tune.Tuner(\n",
    "    tune.with_resources(train_model, {\"cpu\": 10, \"gpu\": 1}),\n",
    "    param_space=search_space,\n",
    "    tune_config=tune.TuneConfig(\n",
    "        num_samples=20,\n",
    "        search_alg=hyperopt_search,\n",
    "    ),\n",
    "    run_config=train.RunConfig(\n",
    "        name=\"agah_bohb\",\n",
    "        callbacks=[\n",
    "            MLflowLoggerCallback(\n",
    "                tracking_uri=\"http://127.0.0.1:5000\",\n",
    "                experiment_name=\"movie_recommender_bayes\"  # MLflow experiment name\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    ")\n",
    "\n",
    "# Start tuning and get results\n",
    "analysis = tuner.fit()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d3ce645d-b41c-4c78-a682-d2c90239437e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting hyperopt\n",
      "  Downloading hyperopt-0.2.7-py2.py3-none-any.whl.metadata (1.7 kB)\n",
      "Requirement already satisfied: numpy in /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages (from hyperopt) (1.26.4)\n",
      "Requirement already satisfied: scipy in /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages (from hyperopt) (1.14.0)\n",
      "Requirement already satisfied: six in /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages (from hyperopt) (1.16.0)\n",
      "Requirement already satisfied: networkx>=2.2 in /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages (from hyperopt) (3.3)\n",
      "Collecting future (from hyperopt)\n",
      "  Downloading future-1.0.0-py3-none-any.whl.metadata (4.0 kB)\n",
      "Requirement already satisfied: tqdm in /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages (from hyperopt) (4.67.1)\n",
      "Requirement already satisfied: cloudpickle in /home/tensor/anaconda3/envs/tensor_check/lib/python3.11/site-packages (from hyperopt) (3.1.0)\n",
      "Collecting py4j (from hyperopt)\n",
      "  Downloading py4j-0.10.9.8-py2.py3-none-any.whl.metadata (1.3 kB)\n",
      "Downloading hyperopt-0.2.7-py2.py3-none-any.whl (1.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.6/1.6 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading future-1.0.0-py3-none-any.whl (491 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m491.3/491.3 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hDownloading py4j-0.10.9.8-py2.py3-none-any.whl (202 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m202.8/202.8 kB\u001b[0m \u001b[31m11.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: py4j, future, hyperopt\n",
      "Successfully installed future-1.0.0 hyperopt-0.2.7 py4j-0.10.9.8\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager, possibly rendering your system unusable.It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv. Use the --root-user-action option if you know what you are doing and want to suppress this warning.\u001b[0m\u001b[33m\n",
      "\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.1\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.3.1\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -U hyperopt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7bbad769-2a02-497c-9bc8-bdaadc6207a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best hyperparameters found were:  {'units_u1': 45, 'units_m1': 24, 'num_layersu': 2, 'num_layersm': 2, 'learning_rate': 0.0003754957799830956, 'emb_unit': 28, 'unitsu_2': 33, 'unitsm_2': 13, 'unitsu_3': 45, 'unitsm_3': 48, 'unitsu_4': 33, 'unitsm_4': 10}\n"
     ]
    }
   ],
   "source": [
    "print(\"Best hyperparameters found were: \", analysis.get_best_result(metric=\"r2_score\", mode=\"max\").config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
